{"entries":[{"caption":"9  Model Optimizations","key":"sec-model_optimizations","order":{"number":1,"section":[9,0,0,0,0,0,0]}},{"caption":"Interoperability of ONNX. Source: TowardsDataScience.","key":"fig-interop","order":{"number":42,"section":[9,5,5,0,0,0,0]}},{"caption":"Quantization (a)symmetry. Source: @gholami2021survey.","key":"fig-quantization-symmetry","order":{"number":23,"section":[9,3,6,2,0,0,0]}},{"caption":"Pruning","key":"exr-p","order":{"number":1,"section":[9,2,1,6,0,0,0]}},{"caption":"Energy use by quantized operations. Source: Mark Horowitz, Stanford University.","key":"fig-quantized-energy","order":{"number":14,"section":[9,3,3,2,0,0,0]}},{"caption":"Quantization-Aware Training. Source: @gholami2021survey.","key":"fig-QAT-diagram","order":{"number":26,"section":[9,3,7,0,0,0,0]}},{"caption":"Iterative pruning.","key":"fig-iterative-pruning","order":{"number":3,"section":[9,2,1,2,3,0,0]}},{"caption":"Comparison of structured versus unstructured pruning.","key":"tbl-pruning_methods","order":{"number":1,"section":[9,2,1,4,0,0,0]}},{"caption":"Quantization granularity: variable ranges. Source: @gholami2021survey.","key":"fig-quantization-granularity","order":{"number":24,"section":[9,3,6,3,0,0,0]}},{"caption":"Energy use by quantized operations. Source: @isscc2014computings.","key":"fig-operations-energy-comparison","order":{"number":16,"section":[9,3,3,2,0,0,0]}},{"caption":"Three layers to be covered.","key":"fig-3-sections","order":{"number":1,"section":[9,1,0,0,0,0,0]}},{"caption":"Channel vs layer pruning.","key":"fig-channel-layer-pruning","order":{"number":2,"section":[9,2,1,2,1,0,0]}},{"caption":"9.3 Efficient Numerics Representation","key":"sec-model_ops_numerics","order":{"number":4,"section":[9,3,0,0,0,0,0]}},{"caption":"Quantized Sine Wave.","key":"fig-quantized-sine-wave","order":{"number":18,"section":[9,3,4,1,0,0,0]}},{"caption":"Quantization uniformity. Source: @gholami2021survey.","key":"fig-quantization-uniformity","order":{"number":20,"section":[9,3,5,2,0,0,0]}},{"caption":"Benefits of lower precision data types. Source: @wu2020integer.","key":"fig-nvidia-turing","order":{"number":31,"section":[9,3,9,0,0,0,0]}},{"caption":"Color mapping of activations. Source: @alexnet2012.","key":"fig-color-mapping","order":{"number":41,"section":[9,5,4,0,2,0,0]}},{"caption":"Scalable Model Compression with TensorFlow","key":"exr-mc","order":{"number":2,"section":[9,2,2,3,0,0,0]}},{"caption":"Model size vs. accelerator memory. Source: @xiao2022smoothquant.","key":"fig-model-size-pace","order":{"number":30,"section":[9,3,9,0,0,0,0]}},{"caption":"Lottery ticket hypothesis experiments.","key":"fig-lottery-ticket-hypothesis","order":{"number":6,"section":[9,2,1,5,0,0,0]}},{"caption":"Input activations to layer 3 in ResNet50. Source: @@wu2020integer.","key":"fig-resnet-activations-histogram","order":{"number":22,"section":[9,3,6,0,0,0,0]}},{"caption":"PTQ and QAT. Source: @ultimate.","key":"fig-QAT-PTQ-relation","order":{"number":27,"section":[9,3,7,0,0,0,0]}},{"caption":"One-shot pruning.","key":"fig-oneshot-pruning","order":{"number":4,"section":[9,2,1,2,3,0,0]}},{"caption":"Quantization errors. Source: @kuzmin2022fp8.","key":"fig-quantization-error","order":{"number":40,"section":[9,5,4,0,2,0,0]}},{"caption":"Sparse network heat map. Source: Numenta.","key":"fig-sprase-heat-map","order":{"number":39,"section":[9,5,4,0,1,0,0]}},{"caption":"Finding the winning ticket subnetwork.","key":"fig-winning-ticket","order":{"number":7,"section":[9,2,1,5,0,0,0]}},{"caption":"GraphOptimizer. Source: @annette2020.","key":"fig-graph-optimizer","order":{"number":38,"section":[9,5,2,0,0,0,0]}},{"caption":"Low matrix factorization. Source: The Clever Machine.","key":"fig-matrix-factorization","order":{"number":10,"section":[9,2,2,2,1,0,0]}},{"caption":"SplitNets vs other approaches. Source: @dong2022splitnets.","key":"fig-splitnet-performance","order":{"number":37,"section":[9,4,5,4,0,0,0]}},{"caption":"Delegating data processing to an FPGA. Source: @kwon2021hardwaresoftware.","key":"fig-fpga-preprocessing","order":{"number":36,"section":[9,4,5,3,0,0,0]}},{"caption":"9.2.1 Pruning","key":"sec-pruning","order":{"number":2,"section":[9,2,1,0,0,0,0]}},{"caption":"hls4ml framework workflow. Source: @fahim2021hls4ml.","key":"fig-hls4ml-workflow","order":{"number":35,"section":[9,4,5,2,0,0,0]}},{"caption":"CiM for keyword spotting. Source: @zhou2021analognets.","key":"fig-computing-memory","order":{"number":34,"section":[9,4,4,0,0,0,0]}},{"caption":"Search spaces accuracy. Source: @lin2020mcunet.","key":"fig-search-space-flops","order":{"number":33,"section":[9,4,1,4,1,0,0]}},{"caption":"Depthwise separable convolutions. Source: @hegde2023introduction.","key":"fig-depthwise-convolution","order":{"number":12,"section":[9,2,3,1,0,0,0]}},{"caption":"Sparse weight matrix.","key":"fig-sparse-matrix","order":{"number":8,"section":[9,2,1,6,0,0,0]}},{"caption":"Quantization","key":"vid-quant","order":{"number":1,"section":[9,3,11,0,0,0,0]}},{"caption":"Accuracy vs. compression rate under different compression methods. Source: @han2015deep.","key":"fig-compression-methods","order":{"number":32,"section":[9,3,10,0,0,0,0]}},{"caption":"9.7 Resources","key":"sec-model-optimizations-resource","order":{"number":7,"section":[9,7,0,0,0,0,0]}},{"caption":"Relative accuracies of PTQ and QAT. Source: @wu2020integer.","key":"fig-quantization-methods-summary","order":{"number":28,"section":[9,3,7,0,0,0,0]}},{"caption":"Post-Training Quantization and calibration. Source: @gholami2021survey.","key":"fig-PTQ-diagram","order":{"number":25,"section":[9,3,7,0,0,0,0]}},{"caption":"Weight and activation quantization. Source: HarvardX.","key":"fig-weight-activations-quantization","order":{"number":29,"section":[9,3,8,0,0,0,0]}},{"caption":"9.4 Efficient Hardware Implementation","key":"sec-model_ops_hw","order":{"number":6,"section":[9,4,0,0,0,0,0]}},{"caption":"Knowledge Distillation","key":"sec-kd","order":{"number":3,"section":[9,2,2,1,0,0,0]}},{"caption":"Sine Wave.","key":"fig-sine-wave","order":{"number":17,"section":[9,3,4,1,0,0,0]}},{"caption":"Tensor decomposition. Source: @xinyu.","key":"fig-tensor-decomposition","order":{"number":11,"section":[9,2,2,3,0,0,0]}},{"caption":"9.3.4 Quantization","key":"sec-quant","order":{"number":5,"section":[9,3,4,0,0,0,0]}},{"caption":"Edge-Aware Model Design","key":"exr-md","order":{"number":3,"section":[9,2,3,3,0,0,0]}},{"caption":"Integer vs Binary quantization functions.","key":"fig-integer-binary-quantization","order":{"number":21,"section":[9,3,5,3,0,0,0]}},{"caption":"Knowledge distillation training process. Source: @intellabs2023knowledge.","key":"fig-knowledge-distillation","order":{"number":9,"section":[9,2,2,1,1,0,0]}},{"caption":"9.2 Efficient Model Representation","key":"sec-model_ops_representation","order":{"number":1,"section":[9,2,0,0,0,0,0]}},{"caption":"Speed of three different models in normal and quantized form.","key":"fig-models-speeds","order":{"number":15,"section":[9,3,3,2,0,0,0]}},{"caption":"Three floating-point formats.","key":"fig-3float","order":{"number":13,"section":[9,3,1,2,0,0,0]}},{"caption":"Effect of quantization on model sizes. Source: HarvardX.","key":"fig-quantized-models-size","order":{"number":19,"section":[9,3,4,1,0,0,0]}},{"caption":"Unstructured vs structured pruning. Source: @qi2021efficient.","key":"fig-structured-unstructured","order":{"number":5,"section":[9,2,1,4,0,0,0]}}],"headings":["introduction","sec-model_ops_representation","sec-pruning","overview","structured-pruning","structures-to-target-for-pruning","establishing-a-criteria-for-pruning","selecting-a-pruning-strategy","advantages-of-structured-pruning","unstructured-pruning","lottery-ticket-hypothesis","challenges-limitations","model-compression","sec-kd","overview-and-benefits","challenges","low-rank-matrix-factorization","background-and-benefits","challenges-1","tensor-decomposition","edge-aware-model-design","model-design-techniques","example-model-architectures","streamlining-model-architecture-search","sec-model_ops_numerics","motivation","the-basics","types","precision","numeric-encoding-and-storage","efficiency-benefits","numeric-representation-nuances","memory-usage","computational-complexity","hardware-compatibility","precision-and-accuracy-trade-offs","trade-off-examples","autonomous-vehicles","mobile-health-applications","high-frequency-trading-hft-systems","edge-based-surveillance-systems","scientific-simulations","sec-quant","initial-breakdown","types-1","uniform-quantization","non-uniform-quantization","stochastic-quantization","zero-shot-quantization","calibration","symmetric-quantization","asymmetric-quantization","granularity","static-and-dynamic-quantization","techniques","weights-vs.-activations","trade-offs","quantization-and-pruning","edge-aware-quantization","sec-model_ops_hw","hardware-aware-neural-architecture-search","single-target-fixed-platform-configuration","hardware-aware-search-strategy","hardware-aware-search-space","single-target-multiple-platform-configurations","multiple-targets","examples-of-hardware-aware-neural-architecture-search","tinynas","topology-aware-nas","challenges-of-hardware-aware-neural-architecture-search","kernel-optimizations","general-kernel-optimizations","loop-unrolling","blocking","tiling","optimized-kernel-libraries","compute-in-memory-cim","memory-access-optimization","leveraging-sparsity","optimization-frameworks","hardware-built-around-software","splitnets","hardware-specific-data-augmentation","software-and-framework-support","built-in-optimization-apis","automated-optimization-tools","hardware-optimization-libraries","visualizing-optimizations","sparsity","quantization-1","model-conversion-and-deployment","conclusion","sec-model-optimizations-resource","sec-model_optimizations"],"options":{"appendix-delim":":","appendix-title":"Appendix","chapter-id":"sec-model_optimizations","chapters":true,"custom":["labqfloatlabLab","exrfloatexrExercise","vidfloatvidVideo"]}}