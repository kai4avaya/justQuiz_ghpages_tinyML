{"entries":[{"caption":"7Â  AI Training","key":"sec-ai_training","order":{"number":1,"section":[7,0,0,0,0,0,0]}},{"caption":"Computational Graph. Source: TensorFlow.","key":"fig-computational-graph","order":{"number":3,"section":[7,3,0,0,0,0,0]}},{"caption":"Regularization","key":"vid-regularization","order":{"number":4,"section":[7,7,1,0,0,0,0]}},{"caption":"Bias/Variance","key":"vid-bias","order":{"number":2,"section":[7,4,2,1,0,0,0]}},{"caption":"Neural network diagram. Source: astroML.","key":"fig-neural-net-diagram","order":{"number":1,"section":[7,2,1,0,0,0,0]}},{"caption":"Activation Functions","key":"exr-af","order":{"number":4,"section":[7,8,5,0,0,0,0]}},{"caption":"Why Regularization Reduces Overfitting","key":"vid-whyreg","order":{"number":5,"section":[7,7,1,0,0,0,0]}},{"caption":"Dropout","key":"vid-dropout","order":{"number":6,"section":[7,7,2,0,0,0,0]}},{"caption":"Weight Initialization","key":"vid-weightinit","order":{"number":8,"section":[7,9,3,0,0,0,0]}},{"caption":"7.13 Resources","key":"sec-ai-training-resource","order":{"number":1,"section":[7,13,0,0,0,0,0]}},{"caption":"Fitting the data overtime. Source: IBM.","key":"fig-fitting-time","order":{"number":5,"section":[7,4,2,1,0,0,0]}},{"caption":"Other Regularization Methods","key":"vid-otherregs","order":{"number":7,"section":[7,7,3,0,0,0,0]}},{"caption":"Comparing the pros and cons of different optimization algorithms.","key":"tbl-optim-algos","order":{"number":2,"section":[7,5,2,0,0,0,0]}},{"caption":"Comparing data parallelism and model parallelism.","key":"tbl-parallelism","order":{"number":5,"section":[7,11,3,0,0,0,0]}},{"caption":"Comparison of optimization platforms for different machine learning use cases.","key":"tbl-platform-comparison","order":{"number":3,"section":[7,6,3,0,0,0,0]}},{"caption":"Weight Initialization","key":"exr-wi","order":{"number":5,"section":[7,9,3,0,0,0,0]}},{"caption":"Data parallelism versus model parallelism.","key":"fig-training-parallelism","order":{"number":8,"section":[7,11,0,0,0,0,0]}},{"caption":"Comparing the pros and cons of different optimization algorithms.","key":"tbl-af","order":{"number":4,"section":[7,8,5,0,0,0,0]}},{"caption":"Train/Dev/Test Sets","key":"vid-train-dev-test","order":{"number":1,"section":[7,4,1,3,0,0,0]}},{"caption":"Regularization","key":"exr-r","order":{"number":3,"section":[7,7,3,0,0,0,0]}},{"caption":"AI training roofline model.","key":"fig-roofline","order":{"number":7,"section":[7,10,2,2,0,0,0]}},{"caption":"Common activation functions. Source: AI Wiki.","key":"fig-activation-functions","order":{"number":6,"section":[7,8,3,0,0,0,0]}},{"caption":"Data fitting: overfitting, right fit, and underfitting. Source: MathWorks.","key":"fig-over-under-fitting","order":{"number":4,"section":[7,4,2,1,0,0,0]}},{"caption":"Hyperparameter Tuning","key":"exr-hpt","order":{"number":2,"section":[7,6,3,2,0,0,0]}},{"caption":"Gradient descent. Source: Towards Data Science.","key":"fig-gradient-descent","order":{"number":2,"section":[7,2,3,0,0,0,0]}},{"caption":"Comparing training, validation, and test data splits.","key":"tbl-training_splits","order":{"number":1,"section":[7,4,0,0,0,0,0]}},{"caption":"Neural Networks with Backpropagation and Gradient Descent","key":"exr-nn","order":{"number":1,"section":[7,2,4,0,0,0,0]}},{"caption":"Hyperparameter","key":"vid-hyperparameter","order":{"number":3,"section":[7,6,3,2,0,0,0]}}],"headings":["introduction","mathematics-of-neural-networks","neural-network-notation","loss-function-as-a-measure-of-goodness-of-fit-against-training-data","training-neural-networks-with-gradient-descent","backpropagation","differentiable-computation-graphs","training-data","dataset-splits","training-set","validation-set","test-set","common-pitfalls-and-mistakes","insufficient-training-data","data-leakage-between-sets","small-or-unrepresentative-validation-set","reusing-the-test-set-multiple-times","same-data-splits-across-experiments","failing-to-stratify-splits","ignoring-time-series-dependencies","no-unseen-data-for-final-evaluation","overoptimizing-on-the-validation-set","optimization-algorithms","optimizations","tradeoffs","benchmarking-algorithms","hyperparameter-tuning","search-algorithms","system-implications","auto-tuners","bigml","tinyml","regularization","l1-and-l2","dropout","early-stopping","activation-functions","sigmoid","tanh","relu","softmax","pros-and-cons","weight-initialization","uniform-and-normal-initialization","xavier-initialization","he-initialization","system-bottlenecks","runtime-complexity-of-matrix-multiplication","layer-multiplications-vs.-activations","mini-batch","optimizing-matrix-multiplication","compute-vs.-memory-bottleneck","training-versus-inference","batch-size","hardware-characteristics","model-architectures","training-parallelization","data-parallel","model-parallelism","comparison","conclusion","sec-ai-training-resource","sec-ai_training"],"options":{"appendix-delim":":","appendix-title":"Appendix","chapter-id":"sec-ai_training","chapters":true,"custom":["labqfloatlabLab","exrfloatexrExercise","vidfloatvidVideo"]}}