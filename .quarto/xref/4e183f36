{"entries":[{"caption":"11  Benchmarking AI","key":"sec-benchmarking_ai","order":{"number":1,"section":[11,0,0,0,0,0,0]}},{"caption":"System Benchmarking - Tensor Operations","key":"exr-cuda","order":{"number":1,"section":[11,4,1,1,0,0,0]}},{"caption":"11.1 Introduction","key":"sec-benchmarking-ai","order":{"number":1,"section":[11,1,0,0,0,0,0]}},{"caption":"MNIST handwritten digits. Source: Suvanjanprasai.","key":"fig-mnist","order":{"number":4,"section":[11,5,1,1,0,0,0]}},{"caption":"Inference Benchmarks - MLPerf","key":"exr-perf","order":{"number":2,"section":[11,4,5,5,0,0,0]}},{"caption":"11.10 Resources","key":"sec-benchmarking-ai-resource","order":{"number":2,"section":[11,10,0,0,0,0,0]}},{"caption":"Benchmarking trifecta.","key":"fig-benchmarking-trifecta","order":{"number":8,"section":[11,7,0,0,0,0,0]}},{"caption":"Model-centric vs. Data-centric ML development. Source: NVIDIA","key":"fig-data-vs-model","order":{"number":7,"section":[11,6,2,0,0,0,0]}},{"caption":"Hardware Lottery.","key":"fig-hardware-lottery","order":{"number":2,"section":[11,4,7,1,0,0,0]}},{"caption":"ML system granularity.","key":"fig-granularity","order":{"number":1,"section":[11,4,1,0,0,0,0]}},{"caption":"MLPerf Tiny modular design. Source: @mattson2020mlperf.","key":"fig-ml-perf","order":{"number":3,"section":[11,4,7,5,0,0,0]}},{"caption":"AI vs human performane. Source: @kiela2021dynabench.","key":"fig-superhuman-perf","order":{"number":6,"section":[11,6,0,0,0,0,0]}},{"caption":"A graph that depicts the top-1 imagenet accuracy vs. the FLOP count of a model along with the model’s parameter count. The figure shows a overall tradeoff between model complexity and accuracy, although some model architectures are more efficiency than others. Source: @bianco2018benchmark.","key":"fig-flops","order":{"number":5,"section":[11,5,2,3,2,0,0]}}],"headings":["sec-benchmarking-ai","historical-context","standard-benchmarks","custom-benchmarks","community-consensus","ai-benchmarks-system-model-and-data","system-benchmarks","model-benchmarks","data-benchmarks","system-benchmarking","granularity","micro-benchmarks","macro-benchmarks","end-to-end-benchmarks","understanding-the-trade-offs","benchmark-components","standardized-datasets","pre-defined-tasks","evaluation-metrics","baselines-and-baseline-models","hardware-and-software-specifications","environmental-conditions","reproducibility-rules","result-interpretation-guidelines","training-vs.-inference","training-benchmarks","purpose","metrics","tasks","benchmarks","example-use-case","inference-benchmarks","purpose-1","metrics-1","tasks-1","benchmarks-1","example-use-case-1","benchmark-example","task","dataset","model","metrics-2","benchmark-harness","baseline-submission","challenges-and-limitations","hardware-lottery","benchmark-engineering","problem","issues","mitigation","model-benchmarking","historical-context-1","mnist-1998","imagenet-2009","coco-2014","gpt-3-2020","present-and-future","model-metrics","accuracy","fairness","complexity","parameters","flops","efficiency","lessons-learned","emerging-trends","limitations-and-challenges","data-benchmarking","limitations-of-model-centric-ai","the-shift-toward-data-centric-ai","benchmarking-data","data-efficiency","the-trifecta","benchmarks-for-emerging-technologies","conclusion","sec-benchmarking-ai-resource","sec-benchmarking_ai"],"options":{"appendix-delim":":","appendix-title":"Appendix","chapter-id":"sec-benchmarking_ai","chapters":true,"custom":["labqfloatlabLab","exrfloatexrExercise","vidfloatvidVideo"]}}