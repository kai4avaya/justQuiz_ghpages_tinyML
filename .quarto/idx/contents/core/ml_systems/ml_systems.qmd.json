{"title":"ML Systems","markdown":{"yaml":{"bibliography":"ml_systems.bib"},"headingText":"ML Systems","headingAttr":{"id":"sec-ml_systems","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n::: {.content-visible when-format=\"html\"}\nResources: [Slides](#sec-ml-systems-resource), [Videos](#sec-ml-systems-resource), [Exercises](#sec-ml-systems-resource)\n:::\n\n![_DALL·E 3 Prompt: Illustration in a rectangular format depicting the merger of embedded systems with Embedded AI. The left half of the image portrays traditional embedded systems, including microcontrollers and processors, detailed and precise. The right half showcases the world of artificial intelligence, with abstract representations of machine learning models, neurons, and data flow. The two halves are distinctly separated, emphasizing the individual significance of embedded tech and AI, but they come together in harmony at the center._](images/png/cover_ml_systems.png)\n\nMachine learning (ML) systems, built on the foundation of computing systems, hold the potential to transform our world. These systems, with their specialized roles and real-time computational capabilities, represent a critical junction where data and computation meet on a micro-scale. They are specifically tailored to optimize performance, energy usage, and spatial efficiency—key factors essential for the successful implementation of ML systems.\n\nAs this chapter progresses, we will explore ML systems' complex and fascinating world. We'll gain insights into their structural design and operational features and understand their key role in powering ML applications. Starting with the basics of microcontroller units, we will examine the interfaces and peripherals that improve their functionalities. This chapter is designed to be a comprehensive guide that explains the nuanced aspects of different ML systems.\n\n::: {.callout-tip}\n\n## Learning Objectives\n\n* Understand the key characteristics and differences between Cloud ML, Edge ML, and TinyML systems.\n\n* Analyze the benefits and challenges associated with each ML paradigm.\n\n* Explore real-world applications and use cases for Cloud ML, Edge ML, and TinyML.\n\n* Compare the performance aspects of each ML approach, including latency, privacy, and resource utilization.\n\n* Examine the evolving landscape of ML systems and potential future developments.\n\n:::\n\n## Overview\n\nML is rapidly evolving, with new paradigms reshaping how models are developed, trained, and deployed. The field is experiencing significant innovation driven by advancements in hardware, software, and algorithmic techniques. These developments are enabling machine learning to be applied in diverse settings, from large-scale cloud infrastructures to edge devices and even tiny, resource-constrained environments.\n\nModern machine learning systems span a spectrum of deployment options, each with its own set of characteristics and use cases. At one end, we have cloud-based ML, which leverages powerful centralized computing resources for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML, which brings computation closer to the data source for reduced latency and improved privacy. At the far end, we find TinyML, which enables machine learning on extremely low-power devices with severe memory and processing constraints.\n\nThis chapter explores the landscape of contemporary machine learning systems, covering three key approaches: Cloud ML, Edge ML, and TinyML. @fig-cloud-edge-tinyml-comparison illustrates the spectrum of distributed intelligence across these approaches, providing a visual comparison of their characteristics. We will examine the unique characteristics, advantages, and challenges of each approach, as depicted in the figure. Additionally, we will discuss the emerging trends and technologies that are shaping the future of machine learning deployment, considering how they might influence the balance between these three paradigms.\n\n![Cloud vs. Edge vs. TinyML: The Spectrum of Distributed Intelligence. Source: ABI Research -- TinyML.](images/png/cloud-edge-tiny.png){#fig-cloud-edge-tinyml-comparison}\n\nThe evolution of machine learning systems can be seen as a progression from centralized to distributed computing paradigms:\n\n1. **Cloud ML:** Initially, ML was predominantly cloud-based. Powerful servers in data centers were used to train and run large ML models. This approach leverages vast computational resources and storage capacities, enabling the development of complex models trained on massive datasets. Cloud ML excels at tasks requiring extensive processing power and is ideal for applications where real-time responsiveness isn't critical.\n\n2. **Edge ML:** As the need for real-time, low-latency processing grew, Edge ML emerged. This paradigm brings inference capabilities closer to the data source, typically on edge devices such as smartphones, smart cameras, or IoT gateways. Edge ML reduces latency, enhances privacy by keeping data local, and can operate with intermittent cloud connectivity. It's particularly useful for applications requiring quick responses or handling sensitive data.\n\n3. **TinyML:** The latest development in this progression is TinyML, which enables ML models to run on extremely resource-constrained microcontrollers and small embedded systems. TinyML allows for on-device inference without relying on connectivity to the cloud or edge, opening up new possibilities for intelligent, battery-operated devices. This approach is crucial for applications where size, power consumption, and cost are critical factors.\n\nEach of these paradigms has its own strengths and is suited to different use cases:\n\n- Cloud ML remains essential for tasks requiring massive computational power or large-scale data analysis.\n- Edge ML is ideal for applications needing low-latency responses or local data processing.\n- TinyML enables AI capabilities in small, power-efficient devices, expanding the reach of ML to new domains.\n\nThe progression from Cloud to Edge to TinyML reflects a broader trend in computing towards more distributed, localized processing. This evolution is driven by the need for faster response times, improved privacy, reduced bandwidth usage, and the ability to operate in environments with limited or no connectivity.\n\n@fig-vMLsizes illustrates the key differences between Cloud ML, Edge ML, and TinyML in terms of hardware, latency, connectivity, power requirements, and model complexity. As we move from Cloud to Edge to TinyML, we see a dramatic reduction in available resources, which presents significant challenges for deploying sophisticated machine learning models. This resource disparity becomes particularly apparent when attempting to deploy deep learning models on microcontrollers, the primary hardware platform for TinyML. These tiny devices have severely constrained memory and storage capacities, which are often insufficient for conventional deep learning models. We will learn to put these things into perspective in this chapter. \n\n![From cloud GPUs to microcontrollers: Navigating the memory and storage landscape across computing devices. Source: [@lin2023tiny]](./images/jpg/cloud_mobile_tiny_sizes.jpg){#fig-vMLsizes}\n\n## Cloud ML\n\nCloud ML leverages powerful servers in the cloud for training and running large, complex ML models and relies on internet connectivity. @fig-cloud-ml provides an overview of Cloud ML's capabilities which we will discuss in greater detail throughout this section.\n\n![Section overview for Cloud ML.](images/png/cloudml.png){#fig-cloud-ml}\n\n### Characteristics\n\n**Definition of Cloud ML**\n\nCloud Machine Learning (Cloud ML) is a subfield of machine learning that leverages the power and scalability of cloud computing infrastructure to develop, train, and deploy machine learning models. By utilizing the vast computational resources available in the cloud, Cloud ML enables the efficient handling of large-scale datasets and complex machine learning algorithms.\n\n**Centralized Infrastructure**\n\nOne of the key characteristics of Cloud ML is its centralized infrastructure. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU data center. Cloud service providers offer a virtual platform that consists of high-capacity servers, expansive storage solutions, and robust networking architectures, all housed in data centers distributed across the globe. As shown in the figure, these centralized facilities can be massive in scale, housing rows upon rows of specialized hardware. This centralized setup allows for the pooling and efficient management of computational resources, making it easier to scale machine learning projects as needed.\n\n![Cloud TPU data center at Google. Source: [Google.](https://blog.google/technology/ai/google-gemini-ai/#scalable-efficient)](images/png/cloud_ml_tpu.png){#fig-cloudml-example}\n\n**Scalable Data Processing and Model Training**\n\nCloud ML excels in its ability to process and analyze massive volumes of data. The centralized infrastructure is designed to handle complex computations and [model training](../training/training.qmd) tasks that require significant computational power. By leveraging the scalability of the cloud, machine learning models can be trained on vast amounts of data, leading to improved learning capabilities and predictive performance.\n\n**Flexible Deployment and Accessibility**\n\nAnother advantage of Cloud ML is the flexibility it offers in terms of deployment and accessibility. Once a machine learning model is trained and validated, it can be easily deployed and made accessible to users through cloud-based services. This allows for seamless integration of machine learning capabilities into various applications and services, regardless of the user's location or device.\n\n**Collaboration and Resource Sharing**\n\nCloud ML promotes collaboration and resource sharing among teams and organizations. The centralized nature of the cloud infrastructure enables multiple users to access and work on the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, accelerates the development process, and optimizes resource utilization.\n\n**Cost-Effectiveness and Scalability**\n\nBy leveraging the pay-as-you-go pricing model offered by cloud service providers, Cloud ML allows organizations to avoid the upfront costs associated with building and maintaining their own machine learning infrastructure. The ability to scale resources up or down based on demand ensures cost-effectiveness and flexibility in managing machine learning projects.\n\nCloud ML has revolutionized the way machine learning is approached, making it more accessible, scalable, and efficient. It has opened up new possibilities for organizations to harness the power of machine learning without the need for significant investments in hardware and infrastructure.\n\n### Benefits\n\nCloud ML offers several significant benefits that make it a powerful choice for machine learning projects:\n\n**Immense Computational Power**\n\nOne of the key advantages of Cloud ML is its ability to provide vast computational resources. The cloud infrastructure is designed to handle complex algorithms and process large datasets efficiently. This is particularly beneficial for machine learning models that require significant computational power, such as deep learning networks or models trained on massive datasets. By leveraging the cloud's computational capabilities, organizations can overcome the limitations of local hardware setups and scale their machine learning projects to meet demanding requirements.\n\n**Dynamic Scalability**\n\nCloud ML offers dynamic scalability, allowing organizations to easily adapt to changing computational needs. As the volume of data grows or the complexity of machine learning models increases, the cloud infrastructure can seamlessly scale up or down to accommodate these changes. This flexibility ensures consistent performance and enables organizations to handle varying workloads without the need for extensive hardware investments. With Cloud ML, resources can be allocated on-demand, providing a cost-effective and efficient solution for managing machine learning projects.\n\n**Access to Advanced Tools and Algorithms**\n\nCloud ML platforms provide access to a wide range of advanced tools and algorithms specifically designed for machine learning. These tools often include pre-built libraries, frameworks, and APIs that simplify the development and deployment of machine learning models. Developers can leverage these resources to accelerate the building, training, and optimization of sophisticated models. By utilizing the latest advancements in machine learning algorithms and techniques, organizations can stay at the forefront of innovation and achieve better results in their machine learning projects.\n\n**Collaborative Environment**\n\nCloud ML fosters a collaborative environment that enables teams to work together seamlessly. The centralized nature of the cloud infrastructure allows multiple users to access and contribute to the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, promotes cross-functional collaboration, and accelerates the development and iteration of machine learning models. Teams can easily share code, datasets, and results, enabling efficient collaboration and driving innovation across the organization.\n\n**Cost-Effectiveness**\n\nAdopting Cloud ML can be a cost-effective solution for organizations, especially compared to building and maintaining an on-premises machine learning infrastructure. Cloud service providers offer flexible pricing models, such as pay-as-you-go or subscription-based plans, allowing organizations to pay only for the resources they consume. This eliminates the need for upfront capital investments in hardware and infrastructure, reducing the overall cost of implementing machine learning projects. Additionally, the scalability of Cloud ML ensures that organizations can optimize their resource usage and avoid over provisioning, further enhancing cost-efficiency.\n\nThe benefits of Cloud ML, including its immense computational power, dynamic scalability, access to advanced tools and algorithms, collaborative environment, and cost-effectiveness, make it a compelling choice for organizations looking to harness the potential of machine learning. By leveraging the capabilities of the cloud, organizations can accelerate their machine learning initiatives, drive innovation, and gain a competitive edge in today's data-driven landscape.\n\n### Challenges\n\nWhile Cloud ML offers numerous benefits, it also comes with certain challenges that organizations need to consider:\n\n**Latency Issues**\n\nOne of the main challenges of Cloud ML is the potential for latency issues, especially in applications that require real-time responses. Since data needs to be sent from the data source to centralized cloud servers for processing and then back to the application, there can be delays introduced by network transmission. This latency can be a significant drawback in time-sensitive scenarios, such as autonomous vehicles, real-time fraud detection, or industrial control systems, where immediate decision-making is critical. Developers need to carefully design their systems to minimize latency and ensure acceptable response times.\n\n**Data Privacy and Security Concerns**\n\nCentralizing data processing and storage in the cloud can raise concerns about data privacy and security. When sensitive data is transmitted and stored in remote data centers, it becomes vulnerable to potential cyber-attacks and unauthorized access. Cloud data centers can become attractive targets for hackers seeking to exploit vulnerabilities and gain access to valuable information. Organizations need to invest in robust security measures, such as encryption, access controls, and continuous monitoring, to protect their data in the cloud. Compliance with data privacy regulations, such as GDPR or HIPAA, also becomes a critical consideration when handling sensitive data in the cloud.\n\n**Cost Considerations**\n\nAs data processing needs grow, the costs associated with using cloud services can escalate. While Cloud ML offers scalability and flexibility, organizations dealing with large data volumes may face increasing costs as they consume more cloud resources. The pay-as-you-go pricing model of cloud services means that costs can quickly add up, especially for compute-intensive tasks like model training and inference. Organizations need to carefully monitor and optimize their cloud usage to ensure cost-effectiveness. They may need to consider strategies such as data compression, efficient algorithm design, and resource allocation optimization to minimize costs while still achieving desired performance.\n\n**Dependency on Internet Connectivity**\n\nCloud ML relies on stable and reliable internet connectivity to function effectively. Since data needs to be transmitted to and from the cloud, any disruptions or limitations in network connectivity can impact the performance and availability of the machine learning system. This dependency on internet connectivity can be a challenge in scenarios where network access is limited, unreliable, or expensive. Organizations need to ensure robust network infrastructure and consider failover mechanisms or offline capabilities to mitigate the impact of connectivity issues.\n\n**Vendor Lock-In**\n\nWhen adopting Cloud ML, organizations often become dependent on the specific tools, APIs, and services provided by their chosen cloud vendor. This vendor lock-in can make it difficult to switch providers or migrate to different platforms in the future. Organizations may face challenges in terms of portability, interoperability, and cost when considering a change in their cloud ML provider. It is important to carefully evaluate vendor offerings, consider long-term strategic goals, and plan for potential migration scenarios to minimize the risks associated with vendor lock-in.\n\nAddressing these challenges requires careful planning, architectural design, and risk mitigation strategies. Organizations need to weigh the benefits of Cloud ML against the potential challenges and make informed decisions based on their specific requirements, data sensitivity, and business objectives. By proactively addressing these challenges, organizations can effectively leverage the power of Cloud ML while ensuring data privacy, security, cost-effectiveness, and overall system reliability.\n\n### Example Use Cases\n\nCloud ML has found widespread adoption across various domains, revolutionizing the way businesses operate and users interact with technology. Let's explore some notable examples of Cloud ML in action:\n\n**Virtual Assistants**\n\nCloud ML plays a crucial role in powering virtual assistants like Siri and Alexa. These systems leverage the immense computational capabilities of the cloud to process and analyze voice inputs in real-time. By harnessing the power of natural language processing and machine learning algorithms, virtual assistants can understand user queries, extract relevant information, and generate intelligent and personalized responses. The cloud's scalability and processing power enable these assistants to handle a vast number of user interactions simultaneously, providing a seamless and responsive user experience.\n\n**Recommendation Systems**\n\nCloud ML forms the backbone of advanced recommendation systems used by platforms like Netflix and Amazon. These systems use the cloud's ability to process and analyze massive datasets to uncover patterns, preferences, and user behavior. By leveraging collaborative filtering and other machine learning techniques, recommendation systems can offer personalized content or product suggestions tailored to each user's interests. The cloud's scalability allows these systems to continuously update and refine their recommendations based on the ever-growing amount of user data, enhancing user engagement and satisfaction.\n\n**Fraud Detection**\n\nIn the financial industry, Cloud ML has revolutionized fraud detection systems. By leveraging the cloud's computational power, these systems can analyze vast amounts of transactional data in real-time to identify potential fraudulent activities. Machine learning algorithms trained on historical fraud patterns can detect anomalies and suspicious behavior, enabling financial institutions to take proactive measures to prevent fraud and minimize financial losses. The cloud's ability to process and store large volumes of data makes it an ideal platform for implementing robust and scalable fraud detection systems.\n\n**Personalized User Experiences**\n\nCloud ML is deeply integrated into our online experiences, shaping the way we interact with digital platforms. From personalized ads on social media feeds to predictive text features in email services, Cloud ML powers smart algorithms that enhance user engagement and convenience. It enables e-commerce sites to recommend products based on a user's browsing and purchase history, fine-tunes search engines to deliver accurate and relevant results, and automates the tagging and categorization of photos on platforms like Facebook. By leveraging the cloud's computational resources, these systems can continuously learn and adapt to user preferences, providing a more intuitive and personalized user experience.\n\n**Security and Anomaly Detection**\n\nCloud ML plays a role in bolstering user security by powering anomaly detection systems. These systems continuously monitor user activities and system logs to identify unusual patterns or suspicious behavior. By analyzing vast amounts of data in real-time, Cloud ML algorithms can detect potential cyber threats, such as unauthorized access attempts, malware infections, or data breaches. The cloud’s scalability and processing power enable these systems to handle the increasing complexity and volume of security data, providing a proactive approach to protecting users and systems from potential threats. \n\n## Edge ML\n\n### Characteristics\n\n**Definition of Edge ML**\n\nEdge Machine Learning (Edge ML) runs machine learning algorithms directly on endpoint devices or closer to where the data is generated rather than relying on centralized cloud servers. This approach brings computation closer to the data source, reducing the need to send large volumes of data over networks, often resulting in lower latency and improved data privacy.  @fig-edge-ml provides an overview of this section.\n\n![Section overview for Edge ML.](images/png/edgeml.png){#fig-edge-ml}\n\n**Decentralized Data Processing**\n\nIn Edge ML, data processing happens in a decentralized fashion, as illustrated in @fig-edgeml-example. Instead of sending data to remote servers, the data is processed locally on devices like smartphones, tablets, or Internet of Things (IoT) devices. The figure showcases various examples of these edge devices, including wearables, industrial sensors, and smart home appliances. This local processing allows devices to make quick decisions based on the data they collect without relying heavily on a central server's resources. \n\n![Edge ML Examples. Source: Edge Impulse.](images/jpg/edge_ml_iot.jpg){#fig-edgeml-example}\n\n**Local Data Storage and Computation**\n\nLocal data storage and computation are key features of Edge ML. This setup ensures that data can be stored and analyzed directly on the devices, thereby maintaining the privacy of the data and reducing the need for constant internet connectivity. Moreover, this often leads to more efficient computation, as data doesn't have to travel long distances, and computations are performed with a more nuanced understanding of the local context, which can sometimes result in more insightful analyses.\n\n### Benefits\n\n**Reduced Latency**\n\nOne of Edge ML's main advantages is the significant latency reduction compared to Cloud ML. This reduced latency can be a critical benefit in situations where milliseconds count, such as in autonomous vehicles, where quick decision-making can mean the difference between safety and an accident.\n\n**Enhanced Data Privacy**\n\nEdge ML also offers improved data privacy, as data is primarily stored and processed locally. This minimizes the risk of data breaches that are more common in centralized data storage solutions. Sensitive information can be kept more secure, as it's not sent over networks that could be intercepted.\n\n**Lower Bandwidth Usage**\n\nOperating closer to the data source means less data must be sent over networks, reducing bandwidth usage. This can result in cost savings and efficiency gains, especially in environments where bandwidth is limited or costly.\n\n### Challenges\n\n**Limited Computational Resources Compared to Cloud ML**\n\nHowever, Edge ML has its challenges. One of the main concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices may have a different processing power or storage capacity than cloud servers, limiting the complexity of the machine learning models that can be deployed.\n\n**Complexity in Managing Edge Nodes**\n\nManaging a network of edge nodes can introduce complexity, especially regarding coordination, updates, and maintenance. Ensuring all nodes operate seamlessly and are up-to-date with the latest algorithms and security protocols can be a logistical challenge.\n\n**Security Concerns at the Edge Nodes**\n\nWhile Edge ML offers enhanced data privacy, edge nodes can sometimes be more vulnerable to physical and cyber-attacks. Developing robust security protocols that protect data at each node without compromising the system's efficiency remains a significant challenge in deploying Edge ML solutions.\n\n### Example Use Cases\n\nEdge ML has many applications, from autonomous vehicles and smart homes to industrial Internet of Things (IoT). These examples were chosen to highlight scenarios where real-time data processing, reduced latency, and enhanced privacy are not just beneficial but often critical to the operation and success of these technologies. They demonstrate the role that Edge ML can play in driving advancements in various sectors, fostering innovation, and paving the way for more intelligent, responsive, and adaptive systems.\n\n**Autonomous Vehicles**\n\nAutonomous vehicles stand as a prime example of Edge ML's potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. Localized machine learning models assist in quickly analyzing data from various sensors to make immediate driving decisions, ensuring safety and smooth operation.\n\n**Smart Homes and Buildings**\n\nEdge ML plays a crucial role in efficiently managing various systems in smart homes and buildings, from lighting and heating to security. By processing data locally, these systems can operate more responsively and harmoniously with the occupants' habits and preferences, creating a more comfortable living environment.\n\n**Industrial IoT**\n\nThe Industrial IoT leverages Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance, optimizing operations, and enhancing safety measures. This revolution in industrial automation and efficiency is transforming manufacturing and production across various sectors.\n\nThe applicability of Edge ML is vast and not limited to these examples. Various other sectors, including healthcare, agriculture, and urban planning, are exploring and integrating Edge ML to develop innovative solutions responsive to real-world needs and challenges, heralding a new era of smart, interconnected systems.\n\n## Tiny ML\n\n### Characteristics\n\n**Definition of TinyML**\n\nTinyML sits at the crossroads of embedded systems and machine learning, representing a burgeoning field that brings smart algorithms directly to tiny microcontrollers and sensors. These microcontrollers operate under severe resource constraints, particularly regarding memory, storage, and computational power. @fig-tiny-ml encapsulates the key aspects of TinyML discussed in this section.\n\n![Section overview for Tiny ML.](images/png/tinyml.png){#fig-tiny-ml}\n\n**On-Device Machine Learning**\n\nIn TinyML, the focus is on on-device machine learning. This means that machine learning models are deployed and trained on the device, eliminating the need for external servers or cloud infrastructures. This allows TinyML to enable intelligent decision-making right where the data is generated, making real-time insights and actions possible, even in settings where connectivity is limited or unavailable.\n\n**Low Power and Resource-Constrained Environments**\n\nTinyML excels in low-power and resource-constrained settings. These environments require highly optimized solutions that function within the available resources. @fig-tinyml-example showcases an example TinyML device kit, illustrating the compact nature of these systems. These devices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail. TinyML meets the need for efficiency through specialized algorithms and models designed to deliver decent performance while consuming minimal energy, thus ensuring extended operational periods, even in battery-powered devices like those shown.\n\n![Examples of TinyML device kits. Source: [Widening Access to Applied Machine Learning with TinyML.](https://arxiv.org/pdf/2106.04008.pdf)](images/jpg/tiny_ml.jpg){#fig-tinyml-example}\n\n:::{#exr-tinyml .callout-caution collapse=\"true\"}\n\n### TinyML with Arduino\n\nGet ready to bring machine learning to the smallest of devices! In the embedded machine learning world, TinyML is where resource constraints meet ingenuity. This Colab notebook will walk you through building a gesture recognition model designed on an Arduino board. You'll learn how to train a small but effective neural network, optimize it for minimal memory usage, and deploy it to your microcontroller. If you're excited about making everyday objects smarter, this is where it begins!  \n  \n[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/arduino/ArduinoTensorFlowLiteTutorials/blob/master/GestureToEmoji/arduino_tinyml_workshop.ipynb)\n\n:::\n\n### Benefits\n\n**Extremely Low Latency**\n\nOne of the standout benefits of TinyML is its ability to offer ultra-low latency. Since computation occurs directly on the device, the time required to send data to external servers and receive a response is eliminated. This is crucial in applications requiring immediate decision-making, enabling quick responses to changing conditions.\n\n**High Data Security**\n\nTinyML inherently enhances data security. Because data processing and analysis happen on the device, the risk of data interception during transmission is virtually eliminated. This localized approach to data management ensures that sensitive information stays on the device, strengthening user data security.\n\n**Energy Efficiency**\n\nTinyML operates within an energy-efficient framework, a necessity given its resource-constrained environments. By employing lean algorithms and optimized computational methods, TinyML ensures that devices can execute complex tasks without rapidly depleting battery life, making it a sustainable option for long-term deployments.\n\n### Challenges\n\n**Limited Computational Capabilities**\n\nHowever, the shift to TinyML comes with its set of hurdles. The primary limitation is the devices' constrained computational capabilities. The need to operate within such limits means that deployed models must be simplified, which could affect the accuracy and sophistication of the solutions.\n\n**Complex Development Cycle**\n\nTinyML also introduces a complicated development cycle. Crafting lightweight and effective models demands a deep understanding of machine learning principles and expertise in embedded systems. This complexity calls for a collaborative development approach, where multi-domain expertise is essential for success.\n\n**Model Optimization and Compression**\n\nA central challenge in TinyML is model optimization and compression. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance and optimizing models to maintain effectiveness while fitting within stringent resource constraints.\n\n### Example Use Cases\n\n**Wearable Devices**\n\nIn wearables, TinyML opens the door to smarter, more responsive gadgets. From fitness trackers offering real-time workout feedback to smart glasses processing visual data on the fly, TinyML transforms how we engage with wearable tech, delivering personalized experiences directly from the device.\n\n**Predictive Maintenance**\n\nIn industrial settings, TinyML plays a significant role in predictive maintenance. By deploying TinyML algorithms on sensors that monitor equipment health, companies can preemptively identify potential issues, reducing downtime and preventing costly breakdowns. On-site data analysis ensures quick responses, potentially stopping minor issues from becoming major problems.\n\n**Anomaly Detection**\n\nTinyML can be employed to create anomaly detection models that identify unusual data patterns. For instance, a smart factory could use TinyML to monitor industrial processes and spot anomalies, helping prevent accidents and improve product quality. Similarly, a security company could use TinyML to monitor network traffic for unusual patterns, aiding in detecting and preventing cyber-attacks. TinyML could monitor patient data for anomalies in healthcare, aiding early disease detection and better patient treatment.\n\n**Environmental Monitoring**\n\nIn environmental monitoring, TinyML enables real-time data analysis from various field-deployed sensors. These could range from city air quality monitoring to wildlife tracking in protected areas. Through TinyML, data can be processed locally, allowing for quick responses to changing conditions and providing a nuanced understanding of environmental patterns, crucial for informed decision-making.\n\nIn summary, TinyML serves as a trailblazer in the evolution of machine learning, fostering innovation across various fields by bringing intelligence directly to the edge. Its potential to transform our interaction with technology and the world is immense, promising a future where devices are connected, intelligent, and capable of making real-time decisions and responses. \n\n## Comparison\n\nLet's bring together the different ML variants we've explored individually for a comprehensive view. @fig-venn-diagram illustrates the relationships and overlaps between Cloud ML, Edge ML, and TinyML using a Venn diagram. This visual representation effectively highlights the unique characteristics of each approach while also showing areas of commonality. Each ML paradigm has its own distinct features, but there are also intersections where these approaches share certain attributes or capabilities. This diagram helps us understand how these variants relate to each other in the broader landscape of machine learning implementations.\n\n![ML Venn diagram. Source: [arXiv](https://arxiv.org/html/2403.19076v1)](images/png/venndiagram.png){#fig-venn-diagram}\n\nFor a more detailed comparison of these ML variants, we can refer to @tbl-big_vs_tiny. This table offers a comprehensive analysis of Cloud ML, Edge ML, and TinyML based on various features and aspects. By examining these different characteristics side by side, we gain a clearer perspective on the unique advantages and distinguishing factors of each approach. This detailed comparison, combined with the visual overview provided by the Venn diagram, aids in making informed decisions based on the specific needs and constraints of a given application or project.\n\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Aspect                   | Cloud ML                                                | Edge ML                                                 | TinyML                                                   |\n+:=========================+:========================================================+:========================================================+:=========================================================+\n| Processing Location      | Centralized servers (Data Centers)                      | Local devices (closer to data sources)                  | On-device (microcontrollers, embedded systems)           |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Latency                  | High (Depends on internet connectivity)                 | Moderate (Reduced latency compared to Cloud ML)         | Low (Immediate processing without network delay)         |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Data Privacy             | Moderate (Data transmitted over networks)               | High (Data remains on local networks)                   | Very High (Data processed on-device, not transmitted)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Computational Power      | High (Utilizes powerful data center infrastructure)     | Moderate (Utilizes local device capabilities)           | Low (Limited to the power of the embedded system)        |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Energy Consumption       | High (Data centers consume significant energy)          | Moderate (Less than data centers, more than TinyML)     | Low (Highly energy-efficient, designed for low power)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Scalability              | High (Easy to scale with additional server resources)   | Moderate (Depends on local device capabilities)         | Low (Limited by the hardware resources of the device)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Cost                     | High (Recurring costs for server usage, maintenance)    | Variable (Depends on the complexity of local setup)     | Low (Primarily upfront costs for hardware components)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Connectivity             | High (Requires stable internet connectivity)            | Low (Can operate with intermittent connectivity)        | Very Low (Can operate without any network connectivity)  |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Real-time Processing     | Moderate (Can be affected by network latency)           | High (Capable of real-time processing locally)          | Very High (Immediate processing with minimal latency)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Application Examples     | Big Data Analysis, Virtual Assistants                   | Autonomous Vehicles, Smart Homes                        | Wearables, Sensor Networks                               |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Complexity               | Moderate to High (Requires knowledge in cloud computing)| Moderate (Requires knowledge in local network setup)    | Moderate to High (Requires expertise in embedded systems)|\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n\n: Comparison of feature aspects across Cloud ML, Edge ML, and TinyML. {#tbl-big_vs_tiny .hover .striped}\n\n## Conclusion\n\nIn this chapter, we've offered a panoramic view of the evolving landscape of machine learning, covering cloud, edge, and tiny ML paradigms. Cloud-based machine learning leverages the immense computational resources of cloud platforms to enable powerful and accurate models but comes with limitations, including latency and privacy concerns. Edge ML mitigates these limitations by bringing inference directly to edge devices, offering lower latency and reduced connectivity needs. TinyML takes this further by miniaturizing ML models to run directly on highly resource-constrained devices, opening up a new category of intelligent applications.\n\nEach approach has its tradeoffs, including model complexity, latency, privacy, and hardware costs. Over time, we anticipate converging these embedded ML approaches, with cloud pre-training facilitating more sophisticated edge and tiny ML implementations. Advances like federated learning and on-device learning will enable embedded devices to refine their models by learning from real-world data.\n\nThe embedded ML landscape is rapidly evolving and poised to enable intelligent applications across a broad spectrum of devices and use cases. This chapter serves as a snapshot of the current state of embedded ML. As algorithms, hardware, and connectivity continue to improve, we can expect embedded devices of all sizes to become increasingly capable, unlocking transformative new applications for artificial intelligence.\n\n## Resources {#sec-ml-systems-resource}\n\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n:::{.callout-note collapse=\"false\"}\n\n#### Slides\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\n* [Embedded Systems Overview.](https://docs.google.com/presentation/d/1Lgrn7bddHYxyrOmk0JfSVmEBimRePqI7WSliUKRPK9E/edit?resourcekey=0-c5JvfDeqHIdV9A5RMAMAyw#slide=id.g94db9f9f78_0_8)\n\n* [Embedded Computer Hardware.](https://docs.google.com/presentation/d/1hDCFcOrZ08kZPhY4DA3gVikGUo47HwNyvqNrLW-t-Tg/edit?resourcekey=0-J6ix5AYvZMGbFFOa7ae4Hw#slide=id.g94db9f9f78_0_8)\n\n* [Embedded I/O.](https://docs.google.com/presentation/d/1rnWh9XC6iCKSx_hQd4xq2iIDlpc-GkBQw_GjzlP5mQc/edit#slide=id.g94db9f9f78_0_8)\n\n* [Embedded systems software.](https://docs.google.com/presentation/d/1TApZn9xxPWCRY-D-soJ8YOSsfysnccR5UjOyspzeTuU/edit?resourcekey=0-BRWIyCKPLNQFnIfG0fJJ9A#slide=id.g94db9f9f78_0_8)\n\n* [Embedded ML software.](https://docs.google.com/presentation/d/17wgAfoF24Rcx7uPrbau0c8FyzXIUWbe48qGGBOXXT-g/edit?resourcekey=0-Uv29DvmF7gYzKdOoRtn0vw#slide=id.g94db9f9f78_0_8)\n\n* [Embedded Inference.](https://docs.google.com/presentation/d/1FOUQ9dbe3l_qTa2AnroSbOz0ykuCz5cbTNO77tvFxEs/edit?usp=drive_link)\n\n* [TinyML on Microcontrollers.](https://docs.google.com/presentation/d/1jwAZz3UOoJTR8PY6Wa34FxijpoDc9gBM/edit?usp=drive_link&ouid=102419556060649178683&rtpof=true&sd=true)\n\n* TinyML as a Service (TinyMLaaS):\n  * [TinyMLaaS: Introduction.](https://docs.google.com/presentation/d/1O7bxb36SnexfDI3iE_p0C8JI_VYXAL8cyAx3JKDfeUo/edit?usp=drive_link)\n\n  * [TinyMLaaS: Design Overview.](https://docs.google.com/presentation/d/1ZUUHtTbKlzeTwVteQMSztscQmdmMxT1A24pBKSys7g0/edit#slide=id.g94db9f9f78_0_2)\n\n:::\n\n:::{.callout-important collapse=\"false\"}\n\n#### Videos\n\n* _Coming soon._\n:::\n\n:::{.callout-caution collapse=\"false\"}\n\n#### Exercises\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\n* _Coming soon._\n:::\n","srcMarkdownNoYaml":"\n\n# ML Systems {#sec-ml_systems}\n\n::: {.content-visible when-format=\"html\"}\nResources: [Slides](#sec-ml-systems-resource), [Videos](#sec-ml-systems-resource), [Exercises](#sec-ml-systems-resource)\n:::\n\n![_DALL·E 3 Prompt: Illustration in a rectangular format depicting the merger of embedded systems with Embedded AI. The left half of the image portrays traditional embedded systems, including microcontrollers and processors, detailed and precise. The right half showcases the world of artificial intelligence, with abstract representations of machine learning models, neurons, and data flow. The two halves are distinctly separated, emphasizing the individual significance of embedded tech and AI, but they come together in harmony at the center._](images/png/cover_ml_systems.png)\n\nMachine learning (ML) systems, built on the foundation of computing systems, hold the potential to transform our world. These systems, with their specialized roles and real-time computational capabilities, represent a critical junction where data and computation meet on a micro-scale. They are specifically tailored to optimize performance, energy usage, and spatial efficiency—key factors essential for the successful implementation of ML systems.\n\nAs this chapter progresses, we will explore ML systems' complex and fascinating world. We'll gain insights into their structural design and operational features and understand their key role in powering ML applications. Starting with the basics of microcontroller units, we will examine the interfaces and peripherals that improve their functionalities. This chapter is designed to be a comprehensive guide that explains the nuanced aspects of different ML systems.\n\n::: {.callout-tip}\n\n## Learning Objectives\n\n* Understand the key characteristics and differences between Cloud ML, Edge ML, and TinyML systems.\n\n* Analyze the benefits and challenges associated with each ML paradigm.\n\n* Explore real-world applications and use cases for Cloud ML, Edge ML, and TinyML.\n\n* Compare the performance aspects of each ML approach, including latency, privacy, and resource utilization.\n\n* Examine the evolving landscape of ML systems and potential future developments.\n\n:::\n\n## Overview\n\nML is rapidly evolving, with new paradigms reshaping how models are developed, trained, and deployed. The field is experiencing significant innovation driven by advancements in hardware, software, and algorithmic techniques. These developments are enabling machine learning to be applied in diverse settings, from large-scale cloud infrastructures to edge devices and even tiny, resource-constrained environments.\n\nModern machine learning systems span a spectrum of deployment options, each with its own set of characteristics and use cases. At one end, we have cloud-based ML, which leverages powerful centralized computing resources for complex, data-intensive tasks. Moving along the spectrum, we encounter edge ML, which brings computation closer to the data source for reduced latency and improved privacy. At the far end, we find TinyML, which enables machine learning on extremely low-power devices with severe memory and processing constraints.\n\nThis chapter explores the landscape of contemporary machine learning systems, covering three key approaches: Cloud ML, Edge ML, and TinyML. @fig-cloud-edge-tinyml-comparison illustrates the spectrum of distributed intelligence across these approaches, providing a visual comparison of their characteristics. We will examine the unique characteristics, advantages, and challenges of each approach, as depicted in the figure. Additionally, we will discuss the emerging trends and technologies that are shaping the future of machine learning deployment, considering how they might influence the balance between these three paradigms.\n\n![Cloud vs. Edge vs. TinyML: The Spectrum of Distributed Intelligence. Source: ABI Research -- TinyML.](images/png/cloud-edge-tiny.png){#fig-cloud-edge-tinyml-comparison}\n\nThe evolution of machine learning systems can be seen as a progression from centralized to distributed computing paradigms:\n\n1. **Cloud ML:** Initially, ML was predominantly cloud-based. Powerful servers in data centers were used to train and run large ML models. This approach leverages vast computational resources and storage capacities, enabling the development of complex models trained on massive datasets. Cloud ML excels at tasks requiring extensive processing power and is ideal for applications where real-time responsiveness isn't critical.\n\n2. **Edge ML:** As the need for real-time, low-latency processing grew, Edge ML emerged. This paradigm brings inference capabilities closer to the data source, typically on edge devices such as smartphones, smart cameras, or IoT gateways. Edge ML reduces latency, enhances privacy by keeping data local, and can operate with intermittent cloud connectivity. It's particularly useful for applications requiring quick responses or handling sensitive data.\n\n3. **TinyML:** The latest development in this progression is TinyML, which enables ML models to run on extremely resource-constrained microcontrollers and small embedded systems. TinyML allows for on-device inference without relying on connectivity to the cloud or edge, opening up new possibilities for intelligent, battery-operated devices. This approach is crucial for applications where size, power consumption, and cost are critical factors.\n\nEach of these paradigms has its own strengths and is suited to different use cases:\n\n- Cloud ML remains essential for tasks requiring massive computational power or large-scale data analysis.\n- Edge ML is ideal for applications needing low-latency responses or local data processing.\n- TinyML enables AI capabilities in small, power-efficient devices, expanding the reach of ML to new domains.\n\nThe progression from Cloud to Edge to TinyML reflects a broader trend in computing towards more distributed, localized processing. This evolution is driven by the need for faster response times, improved privacy, reduced bandwidth usage, and the ability to operate in environments with limited or no connectivity.\n\n@fig-vMLsizes illustrates the key differences between Cloud ML, Edge ML, and TinyML in terms of hardware, latency, connectivity, power requirements, and model complexity. As we move from Cloud to Edge to TinyML, we see a dramatic reduction in available resources, which presents significant challenges for deploying sophisticated machine learning models. This resource disparity becomes particularly apparent when attempting to deploy deep learning models on microcontrollers, the primary hardware platform for TinyML. These tiny devices have severely constrained memory and storage capacities, which are often insufficient for conventional deep learning models. We will learn to put these things into perspective in this chapter. \n\n![From cloud GPUs to microcontrollers: Navigating the memory and storage landscape across computing devices. Source: [@lin2023tiny]](./images/jpg/cloud_mobile_tiny_sizes.jpg){#fig-vMLsizes}\n\n## Cloud ML\n\nCloud ML leverages powerful servers in the cloud for training and running large, complex ML models and relies on internet connectivity. @fig-cloud-ml provides an overview of Cloud ML's capabilities which we will discuss in greater detail throughout this section.\n\n![Section overview for Cloud ML.](images/png/cloudml.png){#fig-cloud-ml}\n\n### Characteristics\n\n**Definition of Cloud ML**\n\nCloud Machine Learning (Cloud ML) is a subfield of machine learning that leverages the power and scalability of cloud computing infrastructure to develop, train, and deploy machine learning models. By utilizing the vast computational resources available in the cloud, Cloud ML enables the efficient handling of large-scale datasets and complex machine learning algorithms.\n\n**Centralized Infrastructure**\n\nOne of the key characteristics of Cloud ML is its centralized infrastructure. @fig-cloudml-example illustrates this concept with an example from Google's Cloud TPU data center. Cloud service providers offer a virtual platform that consists of high-capacity servers, expansive storage solutions, and robust networking architectures, all housed in data centers distributed across the globe. As shown in the figure, these centralized facilities can be massive in scale, housing rows upon rows of specialized hardware. This centralized setup allows for the pooling and efficient management of computational resources, making it easier to scale machine learning projects as needed.\n\n![Cloud TPU data center at Google. Source: [Google.](https://blog.google/technology/ai/google-gemini-ai/#scalable-efficient)](images/png/cloud_ml_tpu.png){#fig-cloudml-example}\n\n**Scalable Data Processing and Model Training**\n\nCloud ML excels in its ability to process and analyze massive volumes of data. The centralized infrastructure is designed to handle complex computations and [model training](../training/training.qmd) tasks that require significant computational power. By leveraging the scalability of the cloud, machine learning models can be trained on vast amounts of data, leading to improved learning capabilities and predictive performance.\n\n**Flexible Deployment and Accessibility**\n\nAnother advantage of Cloud ML is the flexibility it offers in terms of deployment and accessibility. Once a machine learning model is trained and validated, it can be easily deployed and made accessible to users through cloud-based services. This allows for seamless integration of machine learning capabilities into various applications and services, regardless of the user's location or device.\n\n**Collaboration and Resource Sharing**\n\nCloud ML promotes collaboration and resource sharing among teams and organizations. The centralized nature of the cloud infrastructure enables multiple users to access and work on the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, accelerates the development process, and optimizes resource utilization.\n\n**Cost-Effectiveness and Scalability**\n\nBy leveraging the pay-as-you-go pricing model offered by cloud service providers, Cloud ML allows organizations to avoid the upfront costs associated with building and maintaining their own machine learning infrastructure. The ability to scale resources up or down based on demand ensures cost-effectiveness and flexibility in managing machine learning projects.\n\nCloud ML has revolutionized the way machine learning is approached, making it more accessible, scalable, and efficient. It has opened up new possibilities for organizations to harness the power of machine learning without the need for significant investments in hardware and infrastructure.\n\n### Benefits\n\nCloud ML offers several significant benefits that make it a powerful choice for machine learning projects:\n\n**Immense Computational Power**\n\nOne of the key advantages of Cloud ML is its ability to provide vast computational resources. The cloud infrastructure is designed to handle complex algorithms and process large datasets efficiently. This is particularly beneficial for machine learning models that require significant computational power, such as deep learning networks or models trained on massive datasets. By leveraging the cloud's computational capabilities, organizations can overcome the limitations of local hardware setups and scale their machine learning projects to meet demanding requirements.\n\n**Dynamic Scalability**\n\nCloud ML offers dynamic scalability, allowing organizations to easily adapt to changing computational needs. As the volume of data grows or the complexity of machine learning models increases, the cloud infrastructure can seamlessly scale up or down to accommodate these changes. This flexibility ensures consistent performance and enables organizations to handle varying workloads without the need for extensive hardware investments. With Cloud ML, resources can be allocated on-demand, providing a cost-effective and efficient solution for managing machine learning projects.\n\n**Access to Advanced Tools and Algorithms**\n\nCloud ML platforms provide access to a wide range of advanced tools and algorithms specifically designed for machine learning. These tools often include pre-built libraries, frameworks, and APIs that simplify the development and deployment of machine learning models. Developers can leverage these resources to accelerate the building, training, and optimization of sophisticated models. By utilizing the latest advancements in machine learning algorithms and techniques, organizations can stay at the forefront of innovation and achieve better results in their machine learning projects.\n\n**Collaborative Environment**\n\nCloud ML fosters a collaborative environment that enables teams to work together seamlessly. The centralized nature of the cloud infrastructure allows multiple users to access and contribute to the same machine learning projects simultaneously. This collaborative approach facilitates knowledge sharing, promotes cross-functional collaboration, and accelerates the development and iteration of machine learning models. Teams can easily share code, datasets, and results, enabling efficient collaboration and driving innovation across the organization.\n\n**Cost-Effectiveness**\n\nAdopting Cloud ML can be a cost-effective solution for organizations, especially compared to building and maintaining an on-premises machine learning infrastructure. Cloud service providers offer flexible pricing models, such as pay-as-you-go or subscription-based plans, allowing organizations to pay only for the resources they consume. This eliminates the need for upfront capital investments in hardware and infrastructure, reducing the overall cost of implementing machine learning projects. Additionally, the scalability of Cloud ML ensures that organizations can optimize their resource usage and avoid over provisioning, further enhancing cost-efficiency.\n\nThe benefits of Cloud ML, including its immense computational power, dynamic scalability, access to advanced tools and algorithms, collaborative environment, and cost-effectiveness, make it a compelling choice for organizations looking to harness the potential of machine learning. By leveraging the capabilities of the cloud, organizations can accelerate their machine learning initiatives, drive innovation, and gain a competitive edge in today's data-driven landscape.\n\n### Challenges\n\nWhile Cloud ML offers numerous benefits, it also comes with certain challenges that organizations need to consider:\n\n**Latency Issues**\n\nOne of the main challenges of Cloud ML is the potential for latency issues, especially in applications that require real-time responses. Since data needs to be sent from the data source to centralized cloud servers for processing and then back to the application, there can be delays introduced by network transmission. This latency can be a significant drawback in time-sensitive scenarios, such as autonomous vehicles, real-time fraud detection, or industrial control systems, where immediate decision-making is critical. Developers need to carefully design their systems to minimize latency and ensure acceptable response times.\n\n**Data Privacy and Security Concerns**\n\nCentralizing data processing and storage in the cloud can raise concerns about data privacy and security. When sensitive data is transmitted and stored in remote data centers, it becomes vulnerable to potential cyber-attacks and unauthorized access. Cloud data centers can become attractive targets for hackers seeking to exploit vulnerabilities and gain access to valuable information. Organizations need to invest in robust security measures, such as encryption, access controls, and continuous monitoring, to protect their data in the cloud. Compliance with data privacy regulations, such as GDPR or HIPAA, also becomes a critical consideration when handling sensitive data in the cloud.\n\n**Cost Considerations**\n\nAs data processing needs grow, the costs associated with using cloud services can escalate. While Cloud ML offers scalability and flexibility, organizations dealing with large data volumes may face increasing costs as they consume more cloud resources. The pay-as-you-go pricing model of cloud services means that costs can quickly add up, especially for compute-intensive tasks like model training and inference. Organizations need to carefully monitor and optimize their cloud usage to ensure cost-effectiveness. They may need to consider strategies such as data compression, efficient algorithm design, and resource allocation optimization to minimize costs while still achieving desired performance.\n\n**Dependency on Internet Connectivity**\n\nCloud ML relies on stable and reliable internet connectivity to function effectively. Since data needs to be transmitted to and from the cloud, any disruptions or limitations in network connectivity can impact the performance and availability of the machine learning system. This dependency on internet connectivity can be a challenge in scenarios where network access is limited, unreliable, or expensive. Organizations need to ensure robust network infrastructure and consider failover mechanisms or offline capabilities to mitigate the impact of connectivity issues.\n\n**Vendor Lock-In**\n\nWhen adopting Cloud ML, organizations often become dependent on the specific tools, APIs, and services provided by their chosen cloud vendor. This vendor lock-in can make it difficult to switch providers or migrate to different platforms in the future. Organizations may face challenges in terms of portability, interoperability, and cost when considering a change in their cloud ML provider. It is important to carefully evaluate vendor offerings, consider long-term strategic goals, and plan for potential migration scenarios to minimize the risks associated with vendor lock-in.\n\nAddressing these challenges requires careful planning, architectural design, and risk mitigation strategies. Organizations need to weigh the benefits of Cloud ML against the potential challenges and make informed decisions based on their specific requirements, data sensitivity, and business objectives. By proactively addressing these challenges, organizations can effectively leverage the power of Cloud ML while ensuring data privacy, security, cost-effectiveness, and overall system reliability.\n\n### Example Use Cases\n\nCloud ML has found widespread adoption across various domains, revolutionizing the way businesses operate and users interact with technology. Let's explore some notable examples of Cloud ML in action:\n\n**Virtual Assistants**\n\nCloud ML plays a crucial role in powering virtual assistants like Siri and Alexa. These systems leverage the immense computational capabilities of the cloud to process and analyze voice inputs in real-time. By harnessing the power of natural language processing and machine learning algorithms, virtual assistants can understand user queries, extract relevant information, and generate intelligent and personalized responses. The cloud's scalability and processing power enable these assistants to handle a vast number of user interactions simultaneously, providing a seamless and responsive user experience.\n\n**Recommendation Systems**\n\nCloud ML forms the backbone of advanced recommendation systems used by platforms like Netflix and Amazon. These systems use the cloud's ability to process and analyze massive datasets to uncover patterns, preferences, and user behavior. By leveraging collaborative filtering and other machine learning techniques, recommendation systems can offer personalized content or product suggestions tailored to each user's interests. The cloud's scalability allows these systems to continuously update and refine their recommendations based on the ever-growing amount of user data, enhancing user engagement and satisfaction.\n\n**Fraud Detection**\n\nIn the financial industry, Cloud ML has revolutionized fraud detection systems. By leveraging the cloud's computational power, these systems can analyze vast amounts of transactional data in real-time to identify potential fraudulent activities. Machine learning algorithms trained on historical fraud patterns can detect anomalies and suspicious behavior, enabling financial institutions to take proactive measures to prevent fraud and minimize financial losses. The cloud's ability to process and store large volumes of data makes it an ideal platform for implementing robust and scalable fraud detection systems.\n\n**Personalized User Experiences**\n\nCloud ML is deeply integrated into our online experiences, shaping the way we interact with digital platforms. From personalized ads on social media feeds to predictive text features in email services, Cloud ML powers smart algorithms that enhance user engagement and convenience. It enables e-commerce sites to recommend products based on a user's browsing and purchase history, fine-tunes search engines to deliver accurate and relevant results, and automates the tagging and categorization of photos on platforms like Facebook. By leveraging the cloud's computational resources, these systems can continuously learn and adapt to user preferences, providing a more intuitive and personalized user experience.\n\n**Security and Anomaly Detection**\n\nCloud ML plays a role in bolstering user security by powering anomaly detection systems. These systems continuously monitor user activities and system logs to identify unusual patterns or suspicious behavior. By analyzing vast amounts of data in real-time, Cloud ML algorithms can detect potential cyber threats, such as unauthorized access attempts, malware infections, or data breaches. The cloud’s scalability and processing power enable these systems to handle the increasing complexity and volume of security data, providing a proactive approach to protecting users and systems from potential threats. \n\n## Edge ML\n\n### Characteristics\n\n**Definition of Edge ML**\n\nEdge Machine Learning (Edge ML) runs machine learning algorithms directly on endpoint devices or closer to where the data is generated rather than relying on centralized cloud servers. This approach brings computation closer to the data source, reducing the need to send large volumes of data over networks, often resulting in lower latency and improved data privacy.  @fig-edge-ml provides an overview of this section.\n\n![Section overview for Edge ML.](images/png/edgeml.png){#fig-edge-ml}\n\n**Decentralized Data Processing**\n\nIn Edge ML, data processing happens in a decentralized fashion, as illustrated in @fig-edgeml-example. Instead of sending data to remote servers, the data is processed locally on devices like smartphones, tablets, or Internet of Things (IoT) devices. The figure showcases various examples of these edge devices, including wearables, industrial sensors, and smart home appliances. This local processing allows devices to make quick decisions based on the data they collect without relying heavily on a central server's resources. \n\n![Edge ML Examples. Source: Edge Impulse.](images/jpg/edge_ml_iot.jpg){#fig-edgeml-example}\n\n**Local Data Storage and Computation**\n\nLocal data storage and computation are key features of Edge ML. This setup ensures that data can be stored and analyzed directly on the devices, thereby maintaining the privacy of the data and reducing the need for constant internet connectivity. Moreover, this often leads to more efficient computation, as data doesn't have to travel long distances, and computations are performed with a more nuanced understanding of the local context, which can sometimes result in more insightful analyses.\n\n### Benefits\n\n**Reduced Latency**\n\nOne of Edge ML's main advantages is the significant latency reduction compared to Cloud ML. This reduced latency can be a critical benefit in situations where milliseconds count, such as in autonomous vehicles, where quick decision-making can mean the difference between safety and an accident.\n\n**Enhanced Data Privacy**\n\nEdge ML also offers improved data privacy, as data is primarily stored and processed locally. This minimizes the risk of data breaches that are more common in centralized data storage solutions. Sensitive information can be kept more secure, as it's not sent over networks that could be intercepted.\n\n**Lower Bandwidth Usage**\n\nOperating closer to the data source means less data must be sent over networks, reducing bandwidth usage. This can result in cost savings and efficiency gains, especially in environments where bandwidth is limited or costly.\n\n### Challenges\n\n**Limited Computational Resources Compared to Cloud ML**\n\nHowever, Edge ML has its challenges. One of the main concerns is the limited computational resources compared to cloud-based solutions. Endpoint devices may have a different processing power or storage capacity than cloud servers, limiting the complexity of the machine learning models that can be deployed.\n\n**Complexity in Managing Edge Nodes**\n\nManaging a network of edge nodes can introduce complexity, especially regarding coordination, updates, and maintenance. Ensuring all nodes operate seamlessly and are up-to-date with the latest algorithms and security protocols can be a logistical challenge.\n\n**Security Concerns at the Edge Nodes**\n\nWhile Edge ML offers enhanced data privacy, edge nodes can sometimes be more vulnerable to physical and cyber-attacks. Developing robust security protocols that protect data at each node without compromising the system's efficiency remains a significant challenge in deploying Edge ML solutions.\n\n### Example Use Cases\n\nEdge ML has many applications, from autonomous vehicles and smart homes to industrial Internet of Things (IoT). These examples were chosen to highlight scenarios where real-time data processing, reduced latency, and enhanced privacy are not just beneficial but often critical to the operation and success of these technologies. They demonstrate the role that Edge ML can play in driving advancements in various sectors, fostering innovation, and paving the way for more intelligent, responsive, and adaptive systems.\n\n**Autonomous Vehicles**\n\nAutonomous vehicles stand as a prime example of Edge ML's potential. These vehicles rely heavily on real-time data processing to navigate and make decisions. Localized machine learning models assist in quickly analyzing data from various sensors to make immediate driving decisions, ensuring safety and smooth operation.\n\n**Smart Homes and Buildings**\n\nEdge ML plays a crucial role in efficiently managing various systems in smart homes and buildings, from lighting and heating to security. By processing data locally, these systems can operate more responsively and harmoniously with the occupants' habits and preferences, creating a more comfortable living environment.\n\n**Industrial IoT**\n\nThe Industrial IoT leverages Edge ML to monitor and control complex industrial processes. Here, machine learning models can analyze data from numerous sensors in real-time, enabling predictive maintenance, optimizing operations, and enhancing safety measures. This revolution in industrial automation and efficiency is transforming manufacturing and production across various sectors.\n\nThe applicability of Edge ML is vast and not limited to these examples. Various other sectors, including healthcare, agriculture, and urban planning, are exploring and integrating Edge ML to develop innovative solutions responsive to real-world needs and challenges, heralding a new era of smart, interconnected systems.\n\n## Tiny ML\n\n### Characteristics\n\n**Definition of TinyML**\n\nTinyML sits at the crossroads of embedded systems and machine learning, representing a burgeoning field that brings smart algorithms directly to tiny microcontrollers and sensors. These microcontrollers operate under severe resource constraints, particularly regarding memory, storage, and computational power. @fig-tiny-ml encapsulates the key aspects of TinyML discussed in this section.\n\n![Section overview for Tiny ML.](images/png/tinyml.png){#fig-tiny-ml}\n\n**On-Device Machine Learning**\n\nIn TinyML, the focus is on on-device machine learning. This means that machine learning models are deployed and trained on the device, eliminating the need for external servers or cloud infrastructures. This allows TinyML to enable intelligent decision-making right where the data is generated, making real-time insights and actions possible, even in settings where connectivity is limited or unavailable.\n\n**Low Power and Resource-Constrained Environments**\n\nTinyML excels in low-power and resource-constrained settings. These environments require highly optimized solutions that function within the available resources. @fig-tinyml-example showcases an example TinyML device kit, illustrating the compact nature of these systems. These devices can typically fit in the palm of your hand or, in some cases, are even as small as a fingernail. TinyML meets the need for efficiency through specialized algorithms and models designed to deliver decent performance while consuming minimal energy, thus ensuring extended operational periods, even in battery-powered devices like those shown.\n\n![Examples of TinyML device kits. Source: [Widening Access to Applied Machine Learning with TinyML.](https://arxiv.org/pdf/2106.04008.pdf)](images/jpg/tiny_ml.jpg){#fig-tinyml-example}\n\n:::{#exr-tinyml .callout-caution collapse=\"true\"}\n\n### TinyML with Arduino\n\nGet ready to bring machine learning to the smallest of devices! In the embedded machine learning world, TinyML is where resource constraints meet ingenuity. This Colab notebook will walk you through building a gesture recognition model designed on an Arduino board. You'll learn how to train a small but effective neural network, optimize it for minimal memory usage, and deploy it to your microcontroller. If you're excited about making everyday objects smarter, this is where it begins!  \n  \n[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/arduino/ArduinoTensorFlowLiteTutorials/blob/master/GestureToEmoji/arduino_tinyml_workshop.ipynb)\n\n:::\n\n### Benefits\n\n**Extremely Low Latency**\n\nOne of the standout benefits of TinyML is its ability to offer ultra-low latency. Since computation occurs directly on the device, the time required to send data to external servers and receive a response is eliminated. This is crucial in applications requiring immediate decision-making, enabling quick responses to changing conditions.\n\n**High Data Security**\n\nTinyML inherently enhances data security. Because data processing and analysis happen on the device, the risk of data interception during transmission is virtually eliminated. This localized approach to data management ensures that sensitive information stays on the device, strengthening user data security.\n\n**Energy Efficiency**\n\nTinyML operates within an energy-efficient framework, a necessity given its resource-constrained environments. By employing lean algorithms and optimized computational methods, TinyML ensures that devices can execute complex tasks without rapidly depleting battery life, making it a sustainable option for long-term deployments.\n\n### Challenges\n\n**Limited Computational Capabilities**\n\nHowever, the shift to TinyML comes with its set of hurdles. The primary limitation is the devices' constrained computational capabilities. The need to operate within such limits means that deployed models must be simplified, which could affect the accuracy and sophistication of the solutions.\n\n**Complex Development Cycle**\n\nTinyML also introduces a complicated development cycle. Crafting lightweight and effective models demands a deep understanding of machine learning principles and expertise in embedded systems. This complexity calls for a collaborative development approach, where multi-domain expertise is essential for success.\n\n**Model Optimization and Compression**\n\nA central challenge in TinyML is model optimization and compression. Creating machine learning models that can operate effectively within the limited memory and computational power of microcontrollers requires innovative approaches to model design. Developers often face the challenge of striking a delicate balance and optimizing models to maintain effectiveness while fitting within stringent resource constraints.\n\n### Example Use Cases\n\n**Wearable Devices**\n\nIn wearables, TinyML opens the door to smarter, more responsive gadgets. From fitness trackers offering real-time workout feedback to smart glasses processing visual data on the fly, TinyML transforms how we engage with wearable tech, delivering personalized experiences directly from the device.\n\n**Predictive Maintenance**\n\nIn industrial settings, TinyML plays a significant role in predictive maintenance. By deploying TinyML algorithms on sensors that monitor equipment health, companies can preemptively identify potential issues, reducing downtime and preventing costly breakdowns. On-site data analysis ensures quick responses, potentially stopping minor issues from becoming major problems.\n\n**Anomaly Detection**\n\nTinyML can be employed to create anomaly detection models that identify unusual data patterns. For instance, a smart factory could use TinyML to monitor industrial processes and spot anomalies, helping prevent accidents and improve product quality. Similarly, a security company could use TinyML to monitor network traffic for unusual patterns, aiding in detecting and preventing cyber-attacks. TinyML could monitor patient data for anomalies in healthcare, aiding early disease detection and better patient treatment.\n\n**Environmental Monitoring**\n\nIn environmental monitoring, TinyML enables real-time data analysis from various field-deployed sensors. These could range from city air quality monitoring to wildlife tracking in protected areas. Through TinyML, data can be processed locally, allowing for quick responses to changing conditions and providing a nuanced understanding of environmental patterns, crucial for informed decision-making.\n\nIn summary, TinyML serves as a trailblazer in the evolution of machine learning, fostering innovation across various fields by bringing intelligence directly to the edge. Its potential to transform our interaction with technology and the world is immense, promising a future where devices are connected, intelligent, and capable of making real-time decisions and responses. \n\n## Comparison\n\nLet's bring together the different ML variants we've explored individually for a comprehensive view. @fig-venn-diagram illustrates the relationships and overlaps between Cloud ML, Edge ML, and TinyML using a Venn diagram. This visual representation effectively highlights the unique characteristics of each approach while also showing areas of commonality. Each ML paradigm has its own distinct features, but there are also intersections where these approaches share certain attributes or capabilities. This diagram helps us understand how these variants relate to each other in the broader landscape of machine learning implementations.\n\n![ML Venn diagram. Source: [arXiv](https://arxiv.org/html/2403.19076v1)](images/png/venndiagram.png){#fig-venn-diagram}\n\nFor a more detailed comparison of these ML variants, we can refer to @tbl-big_vs_tiny. This table offers a comprehensive analysis of Cloud ML, Edge ML, and TinyML based on various features and aspects. By examining these different characteristics side by side, we gain a clearer perspective on the unique advantages and distinguishing factors of each approach. This detailed comparison, combined with the visual overview provided by the Venn diagram, aids in making informed decisions based on the specific needs and constraints of a given application or project.\n\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Aspect                   | Cloud ML                                                | Edge ML                                                 | TinyML                                                   |\n+:=========================+:========================================================+:========================================================+:=========================================================+\n| Processing Location      | Centralized servers (Data Centers)                      | Local devices (closer to data sources)                  | On-device (microcontrollers, embedded systems)           |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Latency                  | High (Depends on internet connectivity)                 | Moderate (Reduced latency compared to Cloud ML)         | Low (Immediate processing without network delay)         |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Data Privacy             | Moderate (Data transmitted over networks)               | High (Data remains on local networks)                   | Very High (Data processed on-device, not transmitted)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Computational Power      | High (Utilizes powerful data center infrastructure)     | Moderate (Utilizes local device capabilities)           | Low (Limited to the power of the embedded system)        |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Energy Consumption       | High (Data centers consume significant energy)          | Moderate (Less than data centers, more than TinyML)     | Low (Highly energy-efficient, designed for low power)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Scalability              | High (Easy to scale with additional server resources)   | Moderate (Depends on local device capabilities)         | Low (Limited by the hardware resources of the device)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Cost                     | High (Recurring costs for server usage, maintenance)    | Variable (Depends on the complexity of local setup)     | Low (Primarily upfront costs for hardware components)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Connectivity             | High (Requires stable internet connectivity)            | Low (Can operate with intermittent connectivity)        | Very Low (Can operate without any network connectivity)  |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Real-time Processing     | Moderate (Can be affected by network latency)           | High (Capable of real-time processing locally)          | Very High (Immediate processing with minimal latency)    |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Application Examples     | Big Data Analysis, Virtual Assistants                   | Autonomous Vehicles, Smart Homes                        | Wearables, Sensor Networks                               |\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n| Complexity               | Moderate to High (Requires knowledge in cloud computing)| Moderate (Requires knowledge in local network setup)    | Moderate to High (Requires expertise in embedded systems)|\n+--------------------------+---------------------------------------------------------+---------------------------------------------------------+----------------------------------------------------------+\n\n: Comparison of feature aspects across Cloud ML, Edge ML, and TinyML. {#tbl-big_vs_tiny .hover .striped}\n\n## Conclusion\n\nIn this chapter, we've offered a panoramic view of the evolving landscape of machine learning, covering cloud, edge, and tiny ML paradigms. Cloud-based machine learning leverages the immense computational resources of cloud platforms to enable powerful and accurate models but comes with limitations, including latency and privacy concerns. Edge ML mitigates these limitations by bringing inference directly to edge devices, offering lower latency and reduced connectivity needs. TinyML takes this further by miniaturizing ML models to run directly on highly resource-constrained devices, opening up a new category of intelligent applications.\n\nEach approach has its tradeoffs, including model complexity, latency, privacy, and hardware costs. Over time, we anticipate converging these embedded ML approaches, with cloud pre-training facilitating more sophisticated edge and tiny ML implementations. Advances like federated learning and on-device learning will enable embedded devices to refine their models by learning from real-world data.\n\nThe embedded ML landscape is rapidly evolving and poised to enable intelligent applications across a broad spectrum of devices and use cases. This chapter serves as a snapshot of the current state of embedded ML. As algorithms, hardware, and connectivity continue to improve, we can expect embedded devices of all sizes to become increasingly capable, unlocking transformative new applications for artificial intelligence.\n\n## Resources {#sec-ml-systems-resource}\n\nHere is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will be adding new exercises soon.\n\n:::{.callout-note collapse=\"false\"}\n\n#### Slides\n\nThese slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.\n\n* [Embedded Systems Overview.](https://docs.google.com/presentation/d/1Lgrn7bddHYxyrOmk0JfSVmEBimRePqI7WSliUKRPK9E/edit?resourcekey=0-c5JvfDeqHIdV9A5RMAMAyw#slide=id.g94db9f9f78_0_8)\n\n* [Embedded Computer Hardware.](https://docs.google.com/presentation/d/1hDCFcOrZ08kZPhY4DA3gVikGUo47HwNyvqNrLW-t-Tg/edit?resourcekey=0-J6ix5AYvZMGbFFOa7ae4Hw#slide=id.g94db9f9f78_0_8)\n\n* [Embedded I/O.](https://docs.google.com/presentation/d/1rnWh9XC6iCKSx_hQd4xq2iIDlpc-GkBQw_GjzlP5mQc/edit#slide=id.g94db9f9f78_0_8)\n\n* [Embedded systems software.](https://docs.google.com/presentation/d/1TApZn9xxPWCRY-D-soJ8YOSsfysnccR5UjOyspzeTuU/edit?resourcekey=0-BRWIyCKPLNQFnIfG0fJJ9A#slide=id.g94db9f9f78_0_8)\n\n* [Embedded ML software.](https://docs.google.com/presentation/d/17wgAfoF24Rcx7uPrbau0c8FyzXIUWbe48qGGBOXXT-g/edit?resourcekey=0-Uv29DvmF7gYzKdOoRtn0vw#slide=id.g94db9f9f78_0_8)\n\n* [Embedded Inference.](https://docs.google.com/presentation/d/1FOUQ9dbe3l_qTa2AnroSbOz0ykuCz5cbTNO77tvFxEs/edit?usp=drive_link)\n\n* [TinyML on Microcontrollers.](https://docs.google.com/presentation/d/1jwAZz3UOoJTR8PY6Wa34FxijpoDc9gBM/edit?usp=drive_link&ouid=102419556060649178683&rtpof=true&sd=true)\n\n* TinyML as a Service (TinyMLaaS):\n  * [TinyMLaaS: Introduction.](https://docs.google.com/presentation/d/1O7bxb36SnexfDI3iE_p0C8JI_VYXAL8cyAx3JKDfeUo/edit?usp=drive_link)\n\n  * [TinyMLaaS: Design Overview.](https://docs.google.com/presentation/d/1ZUUHtTbKlzeTwVteQMSztscQmdmMxT1A24pBKSys7g0/edit#slide=id.g94db9f9f78_0_2)\n\n:::\n\n:::{.callout-important collapse=\"false\"}\n\n#### Videos\n\n* _Coming soon._\n:::\n\n:::{.callout-caution collapse=\"false\"}\n\n#### Exercises\n\nTo reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.\n\n* _Coming soon._\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":8,"fig-height":6,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../custom_callout.lua"],"reference-location":"margin","highlight-style":"github","toc":true,"toc-depth":4,"include-in-header":{"text":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN\"></script>\n<script type=\"module\"  src=\"/scripts/ai_menu/dist/bundle.js\" defer></script>\n"},"citeproc":true,"output-file":"ml_systems.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../../contents/core/introduction/introduction.bib","../../../contents/core/ai_for_good/ai_for_good.bib","../../../contents/core/benchmarking/benchmarking.bib","../../../contents/core/data_engineering/data_engineering.bib","../../../contents/core/dl_primer/dl_primer.bib","../../../contents/core/efficient_ai/efficient_ai.bib","../../../contents/core/ml_systems/ml_systems.bib","../../../contents/core/frameworks/frameworks.bib","../../../contents/core/generative_ai/generative_ai.bib","../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../contents/core/ops/ops.bib","../../../contents/core/optimizations/optimizations.bib","../../../contents/core/privacy_security/privacy_security.bib","../../../contents/core/responsible_ai/responsible_ai.bib","../../../contents/core/robust_ai/robust_ai.bib","../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../contents/core/training/training.bib","../../../contents/core/workflow/workflow.bib","../../../contents/core/conclusion/conclusion.bib","ml_systems.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"lightbox":true,"theme":{"light":["default","../../../style.scss","../../../style-light.scss"],"dark":["darkly","../../../style.scss","../../../style-dark.scss"]},"code-block-bg":true,"code-block-border-left":"#A51C30","table":{"classes":["table-striped","table-hover"]},"citation-location":"margin","sidenote":true,"linkcolor":"#A51C30","urlcolor":"#A51C30","anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"number-depth":3},"extensions":{"book":{"multiFile":true}}},"titlepage-pdf":{"identifier":{"display-name":"PDF","target-format":"titlepage-pdf","base-format":"pdf","extension-name":"titlepage"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":["../../../_extensions/nmfs-opensci/titlepage/fonts/qualitype/opentype/QTDublinIrish.otf"],"shortcodes":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\titlepage-theme.lua","C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\coverpage-theme.lua","../../../custom_callout.lua"],"toc":true,"top-level-division":"chapter","number-sections":true,"toc-depth":3,"cite-method":"citeproc","reference-location":"margin","include-in-header":[{"file":"../../../tex/header-includes.tex"}],"output-file":"ml_systems.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"template-partials":["../../../_extensions/nmfs-opensci/titlepage/_coverpage.tex","../../../_extensions/nmfs-opensci/titlepage/_author-affiliation-themes.tex","../../../_extensions/nmfs-opensci/titlepage/_header-footer-date-themes.tex","../../../_extensions/nmfs-opensci/titlepage/_title-themes.tex","../../../_extensions/nmfs-opensci/titlepage/_titlepage.tex","../../../_extensions/nmfs-opensci/titlepage/before-body.tex","../../../_extensions/nmfs-opensci/titlepage/pandoc.tex"],"revealjs-plugins":[],"bibliography":["../../../contents/core/introduction/introduction.bib","../../../contents/core/ai_for_good/ai_for_good.bib","../../../contents/core/benchmarking/benchmarking.bib","../../../contents/core/data_engineering/data_engineering.bib","../../../contents/core/dl_primer/dl_primer.bib","../../../contents/core/efficient_ai/efficient_ai.bib","../../../contents/core/ml_systems/ml_systems.bib","../../../contents/core/frameworks/frameworks.bib","../../../contents/core/generative_ai/generative_ai.bib","../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../contents/core/ops/ops.bib","../../../contents/core/optimizations/optimizations.bib","../../../contents/core/privacy_security/privacy_security.bib","../../../contents/core/responsible_ai/responsible_ai.bib","../../../contents/core/robust_ai/robust_ai.bib","../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../contents/core/training/training.bib","../../../contents/core/workflow/workflow.bib","../../../contents/core/conclusion/conclusion.bib","ml_systems.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"documentclass":"scrbook","classoption":["abstract","titlepage"],"coverpage":true,"coverpage-title":"Machine Learning Systems","coverpage-bg-image":"../../../cover-image-transparent.png","coverpage-author":["Vijay","Janapa Reddi"],"coverpage-theme":{"page-text-align":"center","bg-image-left":"0.225\\paperwidth","bg-image-bottom":7,"bg-image-rotate":0,"bg-image-opacity":1,"author-style":"plain","author-sep":"newline","author-fontsize":20,"author-align":"right","author-bottom":"0.15\\paperwidth","author-left":"7in","author-width":"6in","footer-style":"none","header-style":"none","date-style":"none","title-fontsize":57,"title-left":"0.075\\paperwidth","title-bottom":"0.375\\paperwidth","title-width":"0.9\\paperwidth"},"titlepage":true,"titlepage-theme":{"elements":["\\titleblock","Prof. Vijay Janapa Reddi","School of Engineering and Applied Sciences","Harvard University","\\vfill","With heartfelt gratitude to the community for their invaluable contributions and steadfast support.","\\vfill"],"page-align":"left","title-style":"plain","title-fontstyle":["huge","bfseries"],"title-space-after":"4\\baselineskip","title-subtitle-space-between":"0.05\\textheight","subtitle-fontstyle":["large","textit"],"author-style":"superscript-with-and","author-fontstyle":"large","affiliation-style":"numbered-list-with-correspondence","affiliation-fontstyle":"large","affiliation-space-after":"0pt","footer-style":"plain","footer-fontstyle":"large","logo-size":"0.15\\textheight","logo-space-after":"1\\baselineskip","vrule-width":"2pt","vrule-align":"left","vrule-color":"black"},"lof":false,"lot":false,"latex-engine":"xelatex","citation-package":"natbib","link-citations":true,"biblio-title":"References","title-block-style":"none","indent":"0px","fontsize":"10pt","citation-location":"block","fig-caption":true,"cap-location":"margin","fig-cap-location":"margin","tbl-cap-location":"margin","hyperrefoptions":["linktoc=all","pdfwindowui","pdfpagemode=FullScreen","pdfpagelayout=TwoPageRight"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","titlepage-pdf"]}