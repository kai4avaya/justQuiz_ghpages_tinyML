{"title":"DSP Spectral Features","markdown":{"yaml":{"bibliography":"dsp_spectral_features_block.bib"},"headingText":"DSP Spectral Features","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n![*DALLÂ·E 3 Prompt: 1950s style cartoon illustration of a Latin male and female scientist in a vibration research room. The man is using a calculus ruler to examine ancient circuitry. The woman is at a computer with complex vibration graphs. The wooden table has boards with sensors, prominently an accelerometer. A classic, rounded-back computer shows the Arduino IDE with code for LED pin assignments and machine learning algorithms for movement detection. The Serial Monitor displays FFT, classification, wavelets, and DSPs. Vintage lamps, tools, and charts with FFT and Wavelets graphs complete the scene.*](images/jpg/dsp_ini.jpg)\n\n## Overview\n\nTinyML projects related to motion (or vibration) involve data from IMUs (usually **accelerometers** and **Gyroscopes**). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the [Spectral Features Block](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features) for Inertial sensors.\n\nBut how does it work under the hood? Let's dig into it.\n\n## Extracting Features Review\n\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object's motion, such as movement patterns and vibrations. Here's a high-level overview of the process:\n\n**Data collection:** First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It's essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\n\n**Data preprocessing:** Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\n> The Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, [Sensor Data Fusion with Spresense and CommonSense](https://docs.edgeimpulse.com/experts/air-quality-and-environmental-projects/environmental-sensor-fusion-commonsense).\n\n**Segmentation:** Depending on the nature of the data and the application, dividing the data into smaller segments or **windows** may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The **window size** and overlap (**window span**) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of \"data cycles.\"\n\n**Feature extraction:** Once the data is preprocessed and segmented, you can extract features that describe the motion's characteristics. Some typical features extracted from accelerometer data include:\n\n-   **Time-domain** features describe the data's [statistical properties](https://www.mdpi.com/1424-8220/22/5/2012) within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\n-   **Frequency-domain** features are obtained by transforming the data into the frequency domain using techniques like the [Fast Fourier Transform (FFT)](https://en.wikipedia.org/wiki/Fast_Fourier_transform). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\n-   **Time-frequency** domain features combine the time and frequency domain information, such as the [Short-Time Fourier Transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) or the [Discrete Wavelet Transform (DWT)](https://en.wikipedia.org/wiki/Discrete_wavelet_transform). They can provide a more detailed understanding of how the signal's frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\n\nLet's explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.\n\n## A TinyML Motion Classification project\n\n![](images/jpg/spectral_block.jpeg)\n\nIn the hands-on project, *Motion Classification and Anomaly Detection*, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\n-   **Maritime** (pallets in boats)\n-   **Terrestrial** (pallets in a Truck or Train)\n-   **Lift** (pallets being handled by Fork-Lift)\n-   **Idle** (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n\n![](images/png/case_study.png)\n\nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50Hz:\n\n![](images/png/data_sample.png)\n\n> The result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5Hz instead of 50Hz.\n\n## Data Pre-Processing\n\nThe raw data captured by the accelerometer (a \"time series\" data) should be converted to \"tabular data\" using one of the typical Feature Extraction methods described in the last section.\n\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis x 2 seconds x 62.5 samples). The window is slid every 80ms, creating a larger dataset where each instance has 375 \"raw features.\"\n\n![](images/png/v1.png)\n\nOn the Studio, the previous version (V1) of the **Spectral Analysis Block** extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n\n![](images/png/v1_features.png)\n\nThose 33 features were the Input tensor of a Neural Network Classifier.\n\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\n### Edge Impulse - Spectral Analysis Block V.2 under the hood\n\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\n-   RMS\n-   Skewness\n-   Kurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\n-   Spectral Power\n-   Skewness (in the next version)\n-   Kurtosis (in the next version)\n\nIn this [link,](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features) we can have more details about the feature extraction.\n\n> Clone the [public project](https://studio.edgeimpulse.com/public/198358/latest). You can also follow the explanation, playing with the code using my Google CoLab Notebook: [Edge Impulse Spectral Analysis Block Notebook](https://colab.research.google.com/github/Mjrovai/TinyML4D/blob/main/SciTinyM-2023/Edge_Impulse-Spectral_Analysis_Block/Edge_Impulse_Spectral_Analysis_Block_V3.ipynb).\n\nStart importing the libraries:\n\n``` python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\n```\n\nFrom the studied project, let's choose a data sample from accelerometers as below:\n\n-   Window size of 2 seconds: `[2,000]` ms\n-   Sample frequency: `[62.5]` Hz\n-   We will choose the `[None]` filter (for simplicity) and a\n-   FFT length: `[16]`.\n\n``` python\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n```\n\n![](images/png/impulse.png)\n\nSelecting the *Raw Features* on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n\n![](images/png/features.png)\n\nPaste the data points to a new variable *data*:\n\n``` python\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\n```\n\nThe total raw features are 375, but we will work with each axis individually, where N= 125 (number of samples per axis).\n\nWe aim to understand how Edge Impulse gets the processed features.\n\n![](images/png/process_features.png)\n\nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\n\n``` python\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\n```\n\nThe total number of processed features is 39, which means 13 features/axis.\n\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n-   `[rms] [skew] [kurtosis]`\n\nand 10 for the frequency domain (we will return to this later).\n\n-   `[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]`\n\n**Splitting raw data per sensor**\n\nThe data has samples from all axes; let's split and plot them separately:\n\n``` python\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n```\n\n![](images/png/sample.png)\n\n**Subtracting the mean**\n\nNext, we should subtract the mean from the *data*. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\n\nHere are some specific reasons why subtracting the mean can be helpful:\n\n-   It simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\n-   It removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\n-   It can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\n-   It can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\n``` python\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')\n```\n\n![](images/png/sample_no_mean.png)\n\n## Time Domain Statistical features\n\n**RMS Calculation**\n\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the \"value of the direct current that dissipates the same power in a resistor.\"\n\nIn the case of a set of n values {ð¥1, ð¥2, ..., ð¥ð}, the RMS is:\n\n![](images/png/rms.png)\n\n> NOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n``` py\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\n```\n\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n\n``` python\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\n```\n\n`rms_accX=  2.7322`\n\n`rms_accY=  0.7833`\n\n`rms_accZ=  0.1383`\n\nCompared with Edge Impulse result features:\n\n`[2.7322, 0.7833, 0.1383]`\n\n**Skewness and kurtosis calculation**\n\nIn statistics, skewness and kurtosis are two ways to measure the **shape of a distribution**.\n\nHere, we can see the sensor values distribution:\n\n``` python\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n```\n\n![](images/png/skew.png)\n\n[**Skewness**](https://en.wikipedia.org/wiki/Skewness) is a measure of the asymmetry of a distribution. This value can be positive or negative.\n\n![](images/png/skew_2.png)\n\n-   A negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\n-   A positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\n-   A zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\n``` python\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\n```\n\n`skew_accX=  -0.099`\n\n`skew_accY=  0.1756`\n\n`skew_accZ=  6.9463`\n\nCompared with Edge Impulse result features:\n\n`[-0.0978, 0.1735, 6.8629]`\n\n[**Kurtosis**](https://en.wikipedia.org/wiki/Kurtosis) is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n\n![](images/png/kurto.png)\n\n-   The kurtosis of a normal distribution is zero.\n-   If a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\n-   If a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\n``` python\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\n```\n\n`kurt_accX=  -0.3475`\n\n`kurt_accY=  1.2673`\n\n`kurt_accZ=  68.1123`\n\nCompared with Edge Impulse result features:\n\n`[-0.3813, 1.1696, 65.3726]`\n\n## Spectral features\n\nThe filtered signal is passed to the Spectral power section, which computes the **FFT** to generate the spectral features.\n\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or \"sub-windows\"), and the FFT is calculated over each frame.\n\n**FFT length** - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\n-   The total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\n**Spectral Power - Welch's method**\n\nWe should use [Welch's method](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.welch.html) to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n\n``` python\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix <= len(fx):\n        # Slicing truncates if end_idx > len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\n```\n\nApplying the above function to 3 signals:\n\n``` python\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\n```\n\nWe can plot the Power Spectrum P(f):\n\n``` python\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n```\n\n![](images/png/fft.png)\n\nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\n\n``` python\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\n```\n\nLet's now list all Spectral features per axis and compare them with EI:\n\n``` python\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\n```\n\nEI Processed Spectral features (accX):\n\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\n\nCalculated features:\n\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\n\n``` python\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\n```\n\nEI Processed Spectral features (accY):\n\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\n\nCalculated features:\n\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\n\n``` python\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\n```\n\nEI Processed Spectral features (accZ):\n\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\n\nCalculated features:\n\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166\n\n## Time-frequency domain\n\n### Wavelets\n\n[Wavelet](https://en.wikipedia.org/wiki/Wavelet) is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\n\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a **wavelet function**, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\n\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\n\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\n\nLet's select Wavelet on the Spectral Features block in the same project:\n\n-   Type: Wavelet\n-   Wavelet Decomposition Level: 1\n-   Wavelet: bior1.3\n\n![](images/png/fft_result.png)\n\n**The Wavelet Function**\n\n``` python\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n```\n\n![](images/png/wav.png)\n\nAs we did before, let's copy and past the Processed Features:\n\n![](images/png/wav_processed.png)\n\n``` python\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\n```\n\nEdge Impulse computes the [Discrete Wavelet Transform (DWT)](https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\n\nIn the case of **Wavelets**, the extracted features are *basic statistical values*, *crossing values*, and *entropy.* There are, in total, 14 features per layer as below:\n\n-   \\[11\\] Statiscal Features: **n5, n25, n75, n95, mean, median,** standard deviation **(std)**, variance **(var)** root mean square **(rms), kurtosis**, and skewness **(skew)**.\n-   \\[2\\] Crossing Features: Zero crossing rate **(zcross)** and mean crossing rate **(mcross)** are the times that the signal passes through the baseline (y = 0) and the average level (y = u) per unit of time, respectively\n-   \\[1\\] Complexity Feature: **Entropy** is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\n-   The total number of features varies depending on how you set the filter and the number of layers. For example, with \\[None\\] filtering and Level\\[1\\], the number of features per axis will be 14 x 2 (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n### Wavelet Analysis\n\nWavelet analysis decomposes the signal (**accX, accY**, **and accZ**) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as **accX_l1, accY_l1, accZ_l1** and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as **accX_d1, accY_d1, accZ_d1**, permitting the extraction of features for further analysis or classification.\n\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the \"Multilevel 1D Discrete Wavelet Transform\", the result will be a list (for detail, please see: [Discrete Wavelet Transform (DWT)](https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html) )\n\n``` python\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n```\n\n![](images/png/wavelet_input.png)\n\n### Feature Extraction\n\nLet's start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\n\n``` python\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\n```\n\nThe Skelness and Kurtosis:\n\n``` python\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\n```\n\n**Zero crossing (zcross)** is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal's frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\n\n**Mean crossing (mcross)**, on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\n\n``` python\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) < 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\n```\n\nIn wavelet analysis, **entropy** refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal's uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\n\n``` python\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\n```\n\nLet's now list all the wavelet features and create a list by layers.\n\n``` python\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")\n```\n\n![](images/png/wav_result.png)\n\n## Conclusion\n\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\n\nDaniel Situnayake wrote in his [blog:](https://situnayake.com/) \"Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.\" I recommend you read Dan's excellent post in its totality: [nn to cpp: What you need to know about porting deep learning models to the edge](https://situnayake.com/2023/03/21/nn-to-cpp.html).\n","srcMarkdownNoYaml":"\n\n# DSP Spectral Features {.unnumbered}\n\n![*DALLÂ·E 3 Prompt: 1950s style cartoon illustration of a Latin male and female scientist in a vibration research room. The man is using a calculus ruler to examine ancient circuitry. The woman is at a computer with complex vibration graphs. The wooden table has boards with sensors, prominently an accelerometer. A classic, rounded-back computer shows the Arduino IDE with code for LED pin assignments and machine learning algorithms for movement detection. The Serial Monitor displays FFT, classification, wavelets, and DSPs. Vintage lamps, tools, and charts with FFT and Wavelets graphs complete the scene.*](images/jpg/dsp_ini.jpg)\n\n## Overview\n\nTinyML projects related to motion (or vibration) involve data from IMUs (usually **accelerometers** and **Gyroscopes**). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the [Spectral Features Block](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features) for Inertial sensors.\n\nBut how does it work under the hood? Let's dig into it.\n\n## Extracting Features Review\n\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object's motion, such as movement patterns and vibrations. Here's a high-level overview of the process:\n\n**Data collection:** First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It's essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\n\n**Data preprocessing:** Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\n> The Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, [Sensor Data Fusion with Spresense and CommonSense](https://docs.edgeimpulse.com/experts/air-quality-and-environmental-projects/environmental-sensor-fusion-commonsense).\n\n**Segmentation:** Depending on the nature of the data and the application, dividing the data into smaller segments or **windows** may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The **window size** and overlap (**window span**) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of \"data cycles.\"\n\n**Feature extraction:** Once the data is preprocessed and segmented, you can extract features that describe the motion's characteristics. Some typical features extracted from accelerometer data include:\n\n-   **Time-domain** features describe the data's [statistical properties](https://www.mdpi.com/1424-8220/22/5/2012) within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\n-   **Frequency-domain** features are obtained by transforming the data into the frequency domain using techniques like the [Fast Fourier Transform (FFT)](https://en.wikipedia.org/wiki/Fast_Fourier_transform). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\n-   **Time-frequency** domain features combine the time and frequency domain information, such as the [Short-Time Fourier Transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) or the [Discrete Wavelet Transform (DWT)](https://en.wikipedia.org/wiki/Discrete_wavelet_transform). They can provide a more detailed understanding of how the signal's frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\n\nLet's explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons.\n\n## A TinyML Motion Classification project\n\n![](images/jpg/spectral_block.jpeg)\n\nIn the hands-on project, *Motion Classification and Anomaly Detection*, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\n-   **Maritime** (pallets in boats)\n-   **Terrestrial** (pallets in a Truck or Train)\n-   **Lift** (pallets being handled by Fork-Lift)\n-   **Idle** (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n\n![](images/png/case_study.png)\n\nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50Hz:\n\n![](images/png/data_sample.png)\n\n> The result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5Hz instead of 50Hz.\n\n## Data Pre-Processing\n\nThe raw data captured by the accelerometer (a \"time series\" data) should be converted to \"tabular data\" using one of the typical Feature Extraction methods described in the last section.\n\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis x 2 seconds x 62.5 samples). The window is slid every 80ms, creating a larger dataset where each instance has 375 \"raw features.\"\n\n![](images/png/v1.png)\n\nOn the Studio, the previous version (V1) of the **Spectral Analysis Block** extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n\n![](images/png/v1_features.png)\n\nThose 33 features were the Input tensor of a Neural Network Classifier.\n\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\n### Edge Impulse - Spectral Analysis Block V.2 under the hood\n\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\n-   RMS\n-   Skewness\n-   Kurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\n-   Spectral Power\n-   Skewness (in the next version)\n-   Kurtosis (in the next version)\n\nIn this [link,](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features) we can have more details about the feature extraction.\n\n> Clone the [public project](https://studio.edgeimpulse.com/public/198358/latest). You can also follow the explanation, playing with the code using my Google CoLab Notebook: [Edge Impulse Spectral Analysis Block Notebook](https://colab.research.google.com/github/Mjrovai/TinyML4D/blob/main/SciTinyM-2023/Edge_Impulse-Spectral_Analysis_Block/Edge_Impulse_Spectral_Analysis_Block_V3.ipynb).\n\nStart importing the libraries:\n\n``` python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\n```\n\nFrom the studied project, let's choose a data sample from accelerometers as below:\n\n-   Window size of 2 seconds: `[2,000]` ms\n-   Sample frequency: `[62.5]` Hz\n-   We will choose the `[None]` filter (for simplicity) and a\n-   FFT length: `[16]`.\n\n``` python\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n```\n\n![](images/png/impulse.png)\n\nSelecting the *Raw Features* on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n\n![](images/png/features.png)\n\nPaste the data points to a new variable *data*:\n\n``` python\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\n```\n\nThe total raw features are 375, but we will work with each axis individually, where N= 125 (number of samples per axis).\n\nWe aim to understand how Edge Impulse gets the processed features.\n\n![](images/png/process_features.png)\n\nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\n\n``` python\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\n```\n\nThe total number of processed features is 39, which means 13 features/axis.\n\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n-   `[rms] [skew] [kurtosis]`\n\nand 10 for the frequency domain (we will return to this later).\n\n-   `[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]`\n\n**Splitting raw data per sensor**\n\nThe data has samples from all axes; let's split and plot them separately:\n\n``` python\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n```\n\n![](images/png/sample.png)\n\n**Subtracting the mean**\n\nNext, we should subtract the mean from the *data*. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\n\nHere are some specific reasons why subtracting the mean can be helpful:\n\n-   It simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\n-   It removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\n-   It can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\n-   It can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\n``` python\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')\n```\n\n![](images/png/sample_no_mean.png)\n\n## Time Domain Statistical features\n\n**RMS Calculation**\n\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the \"value of the direct current that dissipates the same power in a resistor.\"\n\nIn the case of a set of n values {ð¥1, ð¥2, ..., ð¥ð}, the RMS is:\n\n![](images/png/rms.png)\n\n> NOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n``` py\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\n```\n\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n\n``` python\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\n```\n\n`rms_accX=  2.7322`\n\n`rms_accY=  0.7833`\n\n`rms_accZ=  0.1383`\n\nCompared with Edge Impulse result features:\n\n`[2.7322, 0.7833, 0.1383]`\n\n**Skewness and kurtosis calculation**\n\nIn statistics, skewness and kurtosis are two ways to measure the **shape of a distribution**.\n\nHere, we can see the sensor values distribution:\n\n``` python\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n```\n\n![](images/png/skew.png)\n\n[**Skewness**](https://en.wikipedia.org/wiki/Skewness) is a measure of the asymmetry of a distribution. This value can be positive or negative.\n\n![](images/png/skew_2.png)\n\n-   A negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\n-   A positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\n-   A zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\n``` python\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\n```\n\n`skew_accX=  -0.099`\n\n`skew_accY=  0.1756`\n\n`skew_accZ=  6.9463`\n\nCompared with Edge Impulse result features:\n\n`[-0.0978, 0.1735, 6.8629]`\n\n[**Kurtosis**](https://en.wikipedia.org/wiki/Kurtosis) is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n\n![](images/png/kurto.png)\n\n-   The kurtosis of a normal distribution is zero.\n-   If a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\n-   If a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\n``` python\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\n```\n\n`kurt_accX=  -0.3475`\n\n`kurt_accY=  1.2673`\n\n`kurt_accZ=  68.1123`\n\nCompared with Edge Impulse result features:\n\n`[-0.3813, 1.1696, 65.3726]`\n\n## Spectral features\n\nThe filtered signal is passed to the Spectral power section, which computes the **FFT** to generate the spectral features.\n\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or \"sub-windows\"), and the FFT is calculated over each frame.\n\n**FFT length** - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\n-   The total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\n**Spectral Power - Welch's method**\n\nWe should use [Welch's method](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.welch.html) to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n\n``` python\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix <= len(fx):\n        # Slicing truncates if end_idx > len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\n```\n\nApplying the above function to 3 signals:\n\n``` python\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\n```\n\nWe can plot the Power Spectrum P(f):\n\n``` python\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n```\n\n![](images/png/fft.png)\n\nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\n\n``` python\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\n```\n\nLet's now list all Spectral features per axis and compare them with EI:\n\n``` python\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\n```\n\nEI Processed Spectral features (accX):\n\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\n\nCalculated features:\n\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\n\n``` python\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\n```\n\nEI Processed Spectral features (accY):\n\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\n\nCalculated features:\n\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\n\n``` python\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\n```\n\nEI Processed Spectral features (accZ):\n\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\n\nCalculated features:\n\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166\n\n## Time-frequency domain\n\n### Wavelets\n\n[Wavelet](https://en.wikipedia.org/wiki/Wavelet) is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\n\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a **wavelet function**, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\n\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\n\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\n\nLet's select Wavelet on the Spectral Features block in the same project:\n\n-   Type: Wavelet\n-   Wavelet Decomposition Level: 1\n-   Wavelet: bior1.3\n\n![](images/png/fft_result.png)\n\n**The Wavelet Function**\n\n``` python\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n```\n\n![](images/png/wav.png)\n\nAs we did before, let's copy and past the Processed Features:\n\n![](images/png/wav_processed.png)\n\n``` python\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\n```\n\nEdge Impulse computes the [Discrete Wavelet Transform (DWT)](https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\n\nIn the case of **Wavelets**, the extracted features are *basic statistical values*, *crossing values*, and *entropy.* There are, in total, 14 features per layer as below:\n\n-   \\[11\\] Statiscal Features: **n5, n25, n75, n95, mean, median,** standard deviation **(std)**, variance **(var)** root mean square **(rms), kurtosis**, and skewness **(skew)**.\n-   \\[2\\] Crossing Features: Zero crossing rate **(zcross)** and mean crossing rate **(mcross)** are the times that the signal passes through the baseline (y = 0) and the average level (y = u) per unit of time, respectively\n-   \\[1\\] Complexity Feature: **Entropy** is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\n-   The total number of features varies depending on how you set the filter and the number of layers. For example, with \\[None\\] filtering and Level\\[1\\], the number of features per axis will be 14 x 2 (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n### Wavelet Analysis\n\nWavelet analysis decomposes the signal (**accX, accY**, **and accZ**) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as **accX_l1, accY_l1, accZ_l1** and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as **accX_d1, accY_d1, accZ_d1**, permitting the extraction of features for further analysis or classification.\n\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the \"Multilevel 1D Discrete Wavelet Transform\", the result will be a list (for detail, please see: [Discrete Wavelet Transform (DWT)](https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html) )\n\n``` python\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n```\n\n![](images/png/wavelet_input.png)\n\n### Feature Extraction\n\nLet's start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\n\n``` python\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\n```\n\nThe Skelness and Kurtosis:\n\n``` python\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\n```\n\n**Zero crossing (zcross)** is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal's frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\n\n**Mean crossing (mcross)**, on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\n\n``` python\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) < 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\n```\n\nIn wavelet analysis, **entropy** refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal's uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\n\n``` python\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\n```\n\nLet's now list all the wavelet features and create a list by layers.\n\n``` python\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")\n```\n\n![](images/png/wav_result.png)\n\n## Conclusion\n\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\n\nDaniel Situnayake wrote in his [blog:](https://situnayake.com/) \"Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.\" I recommend you read Dan's excellent post in its totality: [nn to cpp: What you need to know about porting deep learning models to the edge](https://situnayake.com/2023/03/21/nn-to-cpp.html).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":8,"fig-height":6,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../../custom_callout.lua"],"reference-location":"margin","highlight-style":"github","toc":true,"toc-depth":4,"include-in-header":{"text":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN\"></script>\n<script type=\"module\"  src=\"/scripts/ai_menu/dist/bundle.js\" defer></script>\n"},"citeproc":true,"output-file":"dsp_spectral_features_block.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../../../contents/core/introduction/introduction.bib","../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../contents/core/benchmarking/benchmarking.bib","../../../../contents/core/data_engineering/data_engineering.bib","../../../../contents/core/dl_primer/dl_primer.bib","../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../contents/core/ml_systems/ml_systems.bib","../../../../contents/core/frameworks/frameworks.bib","../../../../contents/core/generative_ai/generative_ai.bib","../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../contents/core/ops/ops.bib","../../../../contents/core/optimizations/optimizations.bib","../../../../contents/core/privacy_security/privacy_security.bib","../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../contents/core/robust_ai/robust_ai.bib","../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../contents/core/training/training.bib","../../../../contents/core/workflow/workflow.bib","../../../../contents/core/conclusion/conclusion.bib","dsp_spectral_features_block.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"lightbox":true,"theme":{"light":["default","../../../../style.scss","../../../../style-light.scss"],"dark":["darkly","../../../../style.scss","../../../../style-dark.scss"]},"code-block-bg":true,"code-block-border-left":"#A51C30","table":{"classes":["table-striped","table-hover"]},"citation-location":"margin","sidenote":true,"linkcolor":"#A51C30","urlcolor":"#A51C30","anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"number-depth":3},"extensions":{"book":{"multiFile":true}}},"titlepage-pdf":{"identifier":{"display-name":"PDF","target-format":"titlepage-pdf","base-format":"pdf","extension-name":"titlepage"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":["../../../../_extensions/nmfs-opensci/titlepage/fonts/qualitype/opentype/QTDublinIrish.otf"],"shortcodes":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\titlepage-theme.lua","C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\coverpage-theme.lua","../../../../custom_callout.lua"],"toc":true,"top-level-division":"chapter","number-sections":true,"toc-depth":3,"cite-method":"citeproc","reference-location":"margin","include-in-header":[{"file":"../../../../tex/header-includes.tex"}],"output-file":"dsp_spectral_features_block.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"template-partials":["../../../../_extensions/nmfs-opensci/titlepage/_coverpage.tex","../../../../_extensions/nmfs-opensci/titlepage/_author-affiliation-themes.tex","../../../../_extensions/nmfs-opensci/titlepage/_header-footer-date-themes.tex","../../../../_extensions/nmfs-opensci/titlepage/_title-themes.tex","../../../../_extensions/nmfs-opensci/titlepage/_titlepage.tex","../../../../_extensions/nmfs-opensci/titlepage/before-body.tex","../../../../_extensions/nmfs-opensci/titlepage/pandoc.tex"],"revealjs-plugins":[],"bibliography":["../../../../contents/core/introduction/introduction.bib","../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../contents/core/benchmarking/benchmarking.bib","../../../../contents/core/data_engineering/data_engineering.bib","../../../../contents/core/dl_primer/dl_primer.bib","../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../contents/core/ml_systems/ml_systems.bib","../../../../contents/core/frameworks/frameworks.bib","../../../../contents/core/generative_ai/generative_ai.bib","../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../contents/core/ops/ops.bib","../../../../contents/core/optimizations/optimizations.bib","../../../../contents/core/privacy_security/privacy_security.bib","../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../contents/core/robust_ai/robust_ai.bib","../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../contents/core/training/training.bib","../../../../contents/core/workflow/workflow.bib","../../../../contents/core/conclusion/conclusion.bib","dsp_spectral_features_block.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"documentclass":"scrbook","classoption":["abstract","titlepage"],"coverpage":true,"coverpage-title":"Machine Learning Systems","coverpage-bg-image":"../../../../cover-image-transparent.png","coverpage-author":["Vijay","Janapa Reddi"],"coverpage-theme":{"page-text-align":"center","bg-image-left":"0.225\\paperwidth","bg-image-bottom":7,"bg-image-rotate":0,"bg-image-opacity":1,"author-style":"plain","author-sep":"newline","author-fontsize":20,"author-align":"right","author-bottom":"0.15\\paperwidth","author-left":"7in","author-width":"6in","footer-style":"none","header-style":"none","date-style":"none","title-fontsize":57,"title-left":"0.075\\paperwidth","title-bottom":"0.375\\paperwidth","title-width":"0.9\\paperwidth"},"titlepage":true,"titlepage-theme":{"elements":["\\titleblock","Prof. Vijay Janapa Reddi","School of Engineering and Applied Sciences","Harvard University","\\vfill","With heartfelt gratitude to the community for their invaluable contributions and steadfast support.","\\vfill"],"page-align":"left","title-style":"plain","title-fontstyle":["huge","bfseries"],"title-space-after":"4\\baselineskip","title-subtitle-space-between":"0.05\\textheight","subtitle-fontstyle":["large","textit"],"author-style":"superscript-with-and","author-fontstyle":"large","affiliation-style":"numbered-list-with-correspondence","affiliation-fontstyle":"large","affiliation-space-after":"0pt","footer-style":"plain","footer-fontstyle":"large","logo-size":"0.15\\textheight","logo-space-after":"1\\baselineskip","vrule-width":"2pt","vrule-align":"left","vrule-color":"black"},"lof":false,"lot":false,"latex-engine":"xelatex","citation-package":"natbib","link-citations":true,"biblio-title":"References","title-block-style":"none","indent":"0px","fontsize":"10pt","citation-location":"block","fig-caption":true,"cap-location":"margin","fig-cap-location":"margin","tbl-cap-location":"margin","hyperrefoptions":["linktoc=all","pdfwindowui","pdfpagemode=FullScreen","pdfpagelayout=TwoPageRight"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","titlepage-pdf"]}