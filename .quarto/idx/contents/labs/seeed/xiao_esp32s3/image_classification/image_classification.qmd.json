{"title":"Image Classification","markdown":{"headingText":"Image Classification","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n![*Image by Marcelo Rovai*](./images/png/ini.png)\n\n## Overview\n\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, **Edge AI** has a very high impact potential, and **it is for now**!\n\n![](https://hackster.imgix.net/uploads/attachments/1587506/image_EZKT6sirt5.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\nAt the forefront of the Emerging Technologies Radar is the universal language of Edge Computer Vision. When we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML' Hello World ' that is both simple and profound!\n\nThe Seeed Studio XIAO ESP32S3 Sense is a powerful tool that combines camera and SD card support. With its embedded ML computing power and photography capability, it is an excellent starting point for exploring TinyML vision AI.\n\n## A TinyML Image Classification Project - Fruits versus Veggies\n\n![](./images/png/vegetables.png)\n\nThe whole idea of our project will be to train a model and proceed with inference on the XIAO ESP32S3 Sense. For training, we should find some data **(in fact, tons of data!**).\n\n*But first of all, we need a goal! What do we want to classify?*\n\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We will differentiate **apples** from **bananas** and **potatoes** (you can try other categories)**.**\n\nSo, let's find a specific dataset that includes images from those categories. Kaggle is a good start:\n\n<https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition>\n\nThis dataset contains images of the following food items:\n\n- **Fruits** - *banana, apple*, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango.\n- **Vegetables** - cucumber, carrot, capsicum, onion, *potato,* lemon, tomato, radish, beetroot, cabbage, lettuce, spinach, soybean, cauliflower, bell pepper, chili pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant.\n\nEach category is split into the **train** (100 images), **test** (10 images), and **validation** (10 images).\n\n- Download the dataset from the Kaggle website and put it on your computer.\n\n> Optionally, you can add some fresh photos of bananas, apples, and potatoes from your home kitchen, using, for example, the code discussed in the next setup lab.\n\n## Training the model with Edge Impulse Studio\n\nWe will use the Edge Impulse Studio to train our model. As you may know, [Edge Impulse](https://www.edgeimpulse.com/) is a leading development platform for machine learning on edge devices.\n\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n![](https://hackster.imgix.net/uploads/attachments/1587543/image_MDgkE355g3.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\n### Data Acquisition\n\nNext, on the `UPLOAD DATA` section, upload from your computer the files from chosen categories:\n\n![](https://hackster.imgix.net/uploads/attachments/1587488/image_brdDCN6bc5.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\nIt would be best if you now had your training dataset split into three classes of data:\n\n![](https://hackster.imgix.net/uploads/attachments/1587489/image_QyxusuY3DM.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\n> You can upload extra data for further model testing or split the training data. I will leave it as it is to use the most data possible.\n\n### Impulse Design\n\n> An impulse takes raw data (in this case, images), extracts features (resize pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common use of deep learning, but a lot of data should be used to accomplish this task. We have around 90 images for each category. Is this number enough? Not at all! We will need thousands of images to \"teach or model\" to differentiate an apple from a banana. But, we can solve this issue by re-training a previously trained model with thousands of images. We call this technique \"Transfer Learning\" (TL).\n\n![](https://hackster.imgix.net/uploads/attachments/1587490/tl_fuVIsKd7YV.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\n\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n![](https://hackster.imgix.net/uploads/attachments/1587491/image_QhTt0Av8u3.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\n#### Pre-processing (Feature Generation)\n\nBesides resizing the images, we can change them to Grayscale or keep the actual RGB color depth. Let's start selecting `Grayscale`. Doing that, each one of our data samples will have dimension 9,216 features (96x96x1). Keeping RGB, this dimension would be three times bigger. Working with Grayscale helps to reduce the amount of final memory needed for inference.\n\n![](https://hackster.imgix.net/uploads/attachments/1587492/image_eqGdUoXrMb.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\nRemember to `[Save parameters]`. This will generate the features to be used in training.\n\n#### Model Design\n\n**Transfer Learning**\n\nIn 2007, Google introduced [MobileNetV1,](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html) a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\n\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be smaller and faster. MobileNet introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\n\nEdge Impulse Studio has **MobileNet V1 (96x96 images)** and **V2 (96x96 and 160x160 images)** available, with several different **α** values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.\n\nThe smaller footprint will be obtained at another extreme with **MobileNet V1** and α=0.10 (around 53.2K RAM and 101K ROM).\n\nFor this first pass, we will use **MobileNet V1** and α=0.10.\n\n### Training\n\n**Data Augmentation**\n\nAnother necessary technique to use with deep learning is **data augmentation**. Data augmentation is a method that can help improve the accuracy of machine learning models, creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\n\nUnder the rood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n\n```cpp\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\n```\n\nExposure to these variations during training can help prevent your model from taking shortcuts by \"memorizing\" superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\n\nThe final layer of our model will have 16 neurons with a 10% dropout for overfitting prevention. Here is the Training output:\n\n![](./images/png/train.png)\n\nThe result could be better. The model reached around 77% accuracy, but the amount of RAM expected to be used during the inference is relatively tiny (about 60 KBytes), which is very good.\n\n### Deployment\n\nThe trained model will be deployed as a .zip Arduino library:\n\n![](./images/png/depl.png)\n\nOpen your Arduino IDE, and under **Sketch,** go to **Include Library** and **add.ZIP Library.** Please select the file you download from Edge Impulse Studio, and that's it!\n\n![](./images/png/arduino_zip.png)\n\nUnder the **Examples** tab on Arduino IDE, you should find a sketch code under your project name.\n\n![](./images/png/sketch.png)\n\nOpen the Static Buffer example:\n\n![](./images/png/static_buffer.png)\n\nYou can see that the first line of code is exactly the calling of a library with all the necessary stuff for running inference on your device.\n\n```cpp\n#include <XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h>\n```\n\nOf course, this is a generic code (a \"template\") that only gets one sample of raw data (stored on the variable: features = {} and runs the classifier, doing the inference. The result is shown on the Serial Monitor.\n\nWe should get the sample (image) from the camera and pre-process it (resizing to 96x96, converting to grayscale, and flatting it). This will be the input tensor of our model. The output tensor will be a vector with three values (labels), showing the probabilities of each one of the classes.\n\n![](./images/png/deploy_block.png)\n\nReturning to your project (Tab Image), copy one of the Raw Data Sample:\n\n![](./images/png/get_test_data.png)\n\n9,216 features will be copied to the clipboard. This is the input tensor (a flattened image of 96x96x1), in this case, bananas. Past this Input tensor on`features[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}`\n\n![](./images/png/features.png)\n\nEdge Impulse included the [library ESP NN](https://github.com/espressif/esp-nn) in its SDK, which contains optimized NN (Neural Network) functions for various Espressif chips, including the ESP32S3 (running at Arduino IDE).\n\nWhen running the inference, you should get the highest score for \"banana.\"\n\n![](./images/png/inference1.png)\n\nGreat news! Our device handles an inference, discovering that the input image is a banana. Also, note that the inference time was around 317ms, resulting in a maximum of 3 fps if you tried to classify images from a video.\n\nNow, we should incorporate the camera and classify images in real time.\n\nGo to the Arduino IDE Examples and download from your project the sketch `esp32_camera`:\n\n![](https://hackster.imgix.net/uploads/attachments/1587604/image_hjX5k8gTl8.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n\n```cpp\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\n```\n\nHere you can see the resulting code:\n\n![](./images/png/camera_set.png)\n\nThe modified sketch can be downloaded from GitHub: [xiao_esp32s3_camera](https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/xiao_esp32s3_camera).\n\n> Note that you can optionally keep the pins as a .h file as we did in the Setup Lab.\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start classifying your fruits and vegetables! You can check the result on Serial Monitor.\n\n## Testing the Model (Inference)\n\n![](./images/png/inf_banana.jpg)\n\nGetting a photo with the camera, the classification result will appear on the Serial Monitor:\n\n![](./images/png/inf_banana.png)\n\nOther tests:\n\n![](images/png/inferencia2_apple.png)\n\n![](images/jpeg/inferencia_potato.jpg)\n\n## Testing with a Bigger Model\n\nNow, let's go to the other side of the model size. Let's select a MobilinetV2 96x96 0.35, having as input RGB images.\n\n![](./images/png/train_2.png)\n\nEven with a bigger model, the accuracy could be better, and the amount of memory necessary to run the model increases five times, with latency increasing seven times.\n\n> Note that the performance here is estimated with a smaller device, the ESP-EYE. The actual inference with the ESP32S3 should be better.\n\nTo improve our model, we will need to train more images.\n\nEven though our model did not improve accuracy, let's test whether the XIAO can handle such a bigger model. We will do a simple inference test with the Static Buffer sketch.\n\nLet's redeploy the model. If the EON Compiler is enabled when you generate the library, the total memory needed for inference should be reduced, but it does not influence accuracy.\n\n> ⚠️ **Attention** - The Xiao ESP32S3 with PSRAM enable has enought memory to  run the inference, even in such bigger model. Keep the EON Compiler **NOT ENABLED**.\n\n![](./images/png/deploy_2.png)\n\nDoing an inference with MobilinetV2 96x96 0.35, having as input RGB images, the latency was 219ms, which is great for such a bigger model.\n\n![](./images/png/inf_2.png)\n\nFor the test, we can train the model again, using the smallest version of MobileNet V2, with an alpha of 0.05. Interesting that the result in accuracy was higher.\n\n![](https://hackster.imgix.net/uploads/attachments/1591705/image_lwYLKM696A.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\n> Note that the estimated latency for an Arduino Portenta (or Nicla), running with a clock of 480MHz is 45ms.\n\nDeploying the model, we got an inference of only 135ms, remembering that the XIAO runs with half of the clock used by the Portenta/Nicla (240MHz):\n\n![](https://hackster.imgix.net/uploads/attachments/1591706/image_dAfOl9Tguz.png?auto=compress%2Cformat&w=1280&h=960&fit=max)\n\n## Running inference on the SenseCraft-Web-Toolkit\n\nOne significant limitation of viewing inference on Arduino IDE is that we can not see what the camera focuses on. A good alternative is the **SenseCraft-Web-Toolkit**, a visual model deployment tool provided by [SSCMA](https://sensecraftma.seeed.cc/)(Seeed SenseCraft Model Assistant). This tool allows you to deploy models to various platforms easily through simple operations. The tool offers a user-friendly interface and does not require any coding.\n\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\n1. Open the [SenseCraft-Web-Toolkit website.](https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process)\n2. Connect the XIAO to your computer:\n\n- Having the XIAO connected, select it as below:\n\n![](./images/jpeg/senseCraft-1.jpg)\n\n- Select the device/Port and press `[Connect]`:\n\n    ![](./images/jpeg/senseCraft-2.jpg)\n\n> You can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: `[Upload Custom AI Model]`.\n\nBut first, we must download from Edge Impulse Studio our **quantized.tflite** model.\n\n3. Go to your project at Edge Impulse Studio, or clone this one:\n\n- [XIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN](https://studio.edgeimpulse.com/public/228516/live)\n\n4. On the `Dashboard`, download the model (\"block output\"):  `Transfer learning model - TensorFlow Lite (int8 quantized).`\n\n![](./images/jpeg/senseCraft-4.jpg)\n\n5. On SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: `[Upload Custom AI Model]`. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n![](./images/jpeg/senseCraft-3.jpg)\n\n> Note that you should use the labels trained on EI Studio, entering them in alphabetic order (in our case: apple, banana, potato).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\n![](./images/jpeg/senseCraft-apple.jpg)\n\nThe Classification result will be at the top of the image. You can also select the Confidence of your inference cursor `Confidence`.\n\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, the same that we have done with the Arduino IDE:\n\n![](./images/jpeg/senseCraft-apple-2.jpg)\n\nOn Device Log, you will get information as:\n\n![](./images/jpeg//senseCraft-log.jpg)\n\n- Preprocess time (image capture and Crop): 4ms,\n- Inference time (model latency): 106ms,\n- Postprocess time (display of the image and inclusion of data): 0ms,\n- Output tensor (classes), for example: [[89,0]]; where 0 is Apple (and 1is banana and 2 is potato).\n\nHere are other screenshots:\n\n![](./images/jpeg//inference.jpg)\n\n## Conclusion\n\nThe XIAO ESP32S3 Sense is very flexible, inexpensive, and easy to program. The project proves the potential of TinyML. Memory is not an issue; the device can handle many post-processing tasks, including communication.\n\nYou will find the last version of the codeon the GitHub repository: [XIAO-ESP32S3-Sense.](https://github.com/Mjrovai/XIAO-ESP32S3-Sense)\n\n## Resources\n\n- [XIAO ESP32S3 Codes](https://github.com/Mjrovai/XIAO-ESP32S3-Sense)\n\n- [Dataset](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)\n\n- [Edge Impulse Project](https://studio.edgeimpulse.com/public/228516/live)\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":8,"fig-height":6,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../../../custom_callout.lua"],"reference-location":"margin","highlight-style":"github","toc":true,"toc-depth":4,"include-in-header":{"text":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN\"></script>\n<script type=\"module\"  src=\"/scripts/ai_menu/dist/bundle.js\" defer></script>\n"},"citeproc":true,"output-file":"image_classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"lightbox":true,"theme":{"light":["default","../../../../../style.scss","../../../../../style-light.scss"],"dark":["darkly","../../../../../style.scss","../../../../../style-dark.scss"]},"code-block-bg":true,"code-block-border-left":"#A51C30","table":{"classes":["table-striped","table-hover"]},"citation-location":"margin","sidenote":true,"linkcolor":"#A51C30","urlcolor":"#A51C30","anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"number-depth":3},"extensions":{"book":{"multiFile":true}}},"titlepage-pdf":{"identifier":{"display-name":"PDF","target-format":"titlepage-pdf","base-format":"pdf","extension-name":"titlepage"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":["../../../../../_extensions/nmfs-opensci/titlepage/fonts/qualitype/opentype/QTDublinIrish.otf"],"shortcodes":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\titlepage-theme.lua","C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\coverpage-theme.lua","../../../../../custom_callout.lua"],"toc":true,"top-level-division":"chapter","number-sections":true,"toc-depth":3,"cite-method":"citeproc","reference-location":"margin","include-in-header":[{"file":"../../../../../tex/header-includes.tex"}],"output-file":"image_classification.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"template-partials":["../../../../../_extensions/nmfs-opensci/titlepage/_coverpage.tex","../../../../../_extensions/nmfs-opensci/titlepage/_author-affiliation-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_header-footer-date-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_title-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_titlepage.tex","../../../../../_extensions/nmfs-opensci/titlepage/before-body.tex","../../../../../_extensions/nmfs-opensci/titlepage/pandoc.tex"],"revealjs-plugins":[],"bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"documentclass":"scrbook","classoption":["abstract","titlepage"],"coverpage":true,"coverpage-title":"Machine Learning Systems","coverpage-bg-image":"../../../../../cover-image-transparent.png","coverpage-author":["Vijay","Janapa Reddi"],"coverpage-theme":{"page-text-align":"center","bg-image-left":"0.225\\paperwidth","bg-image-bottom":7,"bg-image-rotate":0,"bg-image-opacity":1,"author-style":"plain","author-sep":"newline","author-fontsize":20,"author-align":"right","author-bottom":"0.15\\paperwidth","author-left":"7in","author-width":"6in","footer-style":"none","header-style":"none","date-style":"none","title-fontsize":57,"title-left":"0.075\\paperwidth","title-bottom":"0.375\\paperwidth","title-width":"0.9\\paperwidth"},"titlepage":true,"titlepage-theme":{"elements":["\\titleblock","Prof. Vijay Janapa Reddi","School of Engineering and Applied Sciences","Harvard University","\\vfill","With heartfelt gratitude to the community for their invaluable contributions and steadfast support.","\\vfill"],"page-align":"left","title-style":"plain","title-fontstyle":["huge","bfseries"],"title-space-after":"4\\baselineskip","title-subtitle-space-between":"0.05\\textheight","subtitle-fontstyle":["large","textit"],"author-style":"superscript-with-and","author-fontstyle":"large","affiliation-style":"numbered-list-with-correspondence","affiliation-fontstyle":"large","affiliation-space-after":"0pt","footer-style":"plain","footer-fontstyle":"large","logo-size":"0.15\\textheight","logo-space-after":"1\\baselineskip","vrule-width":"2pt","vrule-align":"left","vrule-color":"black"},"lof":false,"lot":false,"latex-engine":"xelatex","citation-package":"natbib","link-citations":true,"biblio-title":"References","title-block-style":"none","indent":"0px","fontsize":"10pt","citation-location":"block","fig-caption":true,"cap-location":"margin","fig-cap-location":"margin","tbl-cap-location":"margin","hyperrefoptions":["linktoc=all","pdfwindowui","pdfpagemode=FullScreen","pdfpagelayout=TwoPageRight"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","titlepage-pdf"]}