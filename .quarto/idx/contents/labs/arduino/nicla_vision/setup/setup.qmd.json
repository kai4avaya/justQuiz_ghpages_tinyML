{"title":"Setup","markdown":{"yaml":{"bibliography":"setup.bib"},"headingText":"Setup","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n![*DALL·E 3 Prompt: Illustration reminiscent of a 1950s cartoon where the Arduino NICLA VISION board, equipped with a variety of sensors including a camera, is the focal point on an old-fashioned desk. In the background, a computer screen with rounded edges displays the Arduino IDE. The code seen is related to LED configurations and machine learning voice command detection. Outputs on the Serial Monitor explicitly display the words 'yes' and 'no'.*](images/jpg/nicla_sys_ini.jpg)\n\n## Overview\n\nThe [Arduino Nicla Vision](https://docs.arduino.cc/hardware/nicla-vision) (sometimes called *NiclaV*) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the [Nicla Sense ME](https://www.bosch-sensortec.com/software-tools/tools/arduino-nicla-sense-me/) and the [Nicla Voice](https://store-usa.arduino.cc/products/nicla-voice?_gl=1*l3abc6*_ga*MTQ3NzE4Mjk4Mi4xNjQwMDIwOTk5*_ga_NEXN8H46L5*MTY5NjM0Mzk1My4xMDIuMS4xNjk2MzQ0MjQ1LjAuMC4w). The *Niclas* can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously.\n\n![](images/jpg/image29.jpg)\n\n## Hardware\n\n### Two Parallel Cores\n\nThe central processor is the dual-core [STM32H747,](https://content.arduino.cc/assets/Arduino-Portenta-H7_Datasheet_stm32h747xi.pdf?_gl=1*6quciu*_ga*MTQ3NzE4Mjk4Mi4xNjQwMDIwOTk5*_ga_NEXN8H46L5*MTY0NzQ0NTg1My4xMS4xLjE2NDc0NDYzMzkuMA..) including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\n- Arduino sketches on top of the Arm Mbed OS\n\n- Native Mbed applications\n\n- MicroPython / JavaScript via an interpreter\n\n- TensorFlow Lite\n\n![](images/jpg/image22.jpg)\n\n### Memory\n\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n### Sensors\n\n- **Camera:** A GC2145 2 MP Color CMOS Camera.\n\n- **Microphone:** The `MP34DT05` is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n\n- **6-Axis IMU:** 3D gyroscope and 3D accelerometer data from the `LSM6DSOX` 6-axis IMU.\n\n- **Time of Flight Sensor:** The `VL53L1CBV0FY` Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.\n\n## Arduino IDE Installation\n\nStart connecting the board (*microUSB*) to your computer:\n\n![](images/jpg/image14.jpg)\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to `Tools > Board > Board Manager`, look for Arduino Nicla Vision on the search window, and install the board.\n\n![](images/jpg/image2.jpg)\n\nNext, go to `Tools > Board > Arduino Mbed OS Nicla Boards` and select `Arduino Nicla Vision`. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\n> Open the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n### Testing the Microphone\n\nOn Arduino IDE, go to `Examples > PDM > PDMSerialPlotter`, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n![](images/png/image9.png)\n\n> Vary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n### Testing the IMU\n\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\n![](images/jpg/image19.jpg)\n\nNext, go to `Examples > Arduino_LSM6DSOX > SimpleAccelerometer` and run the accelerometer test (you can also run Gyro and board temperature):\n\n![](images/png/image28.png)\n\n### Testing the ToF (Time of Flight) Sensor\n\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\n![](images/jpg/image15.jpg)\n\nNext, run the sketch [proximity_detection.ino](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Arduino-IDE/proximity_detection/proximity_detection.ino):\n\n![](images/png/image12.png)\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n![](images/jpg/image13.jpg)\n\n### Testing the Camera\n\nWe can also test the camera using, for example, the code provided on `Examples > Camera > CameraCaptureRawBytes`. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\n\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV.\n\n## Installing the OpenMV IDE\n\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\n\nGo to the [OpenMV IDE page](https://openmv.io/pages/download), download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\n![](images/png/image21.png)\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from `Files > Examples > HelloWord > helloword.py`\n\n![](images/png/image7.png)\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the **Serial Terminal** during run time. The image captured by a camera will be displayed in the **Camera Viewer** Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\n> Before connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on `Examples > STM_32H747_System STM32H747_manageBootloader`. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\n![](images/jpg/image23.jpg)\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select `Install the latest release firmware (vX.Y.Z)`. This action will install the latest OpenMV firmware on the Nicla Vision.\n\n![](images/png/image10.png)\n\nYou can leave the option `Erase internal file system` unselected and click `[OK]`.\n\nNicla's green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\n![](images/png/image5.png)\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, \"DFU firmware update complete!\". Press `[OK]`.\n\n![](images/png/image1.png)\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\n![](images/jpg/image18.jpg)\n\nAlso, note that a drive named \"NO NAME\" will appear on your computer.:\n\n![](images/png/image3.png)\n\nEvery time you press the `[RESET]` button on the board, it automatically executes the *main.py* script stored on it. You can load the [main.py](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/main.py) code on the IDE (`File > Open File...`).\n\n![](images/png/image16.png)\n\n> This code is the \"Blink\" code, confirming that the HW is OK.\n\nFor testing the camera, let's run *helloword_1.py*. For that, select the script on `File > Examples > HelloWorld > helloword.py`,\n\nWhen clicking the green play button, the MicroPython script (*hellowolrd.py*) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\n![](images/png/image6.png)\n\nHere is the [helloworld.py](http://helloworld.py/) script:\n\n``` python\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\n```\n\nIn [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision), you can find the Python scripts used here.\n\nThe code can be split into two parts:\n\n- **Setup:** Where the libraries are imported, initialized and the variables are defined and initiated.\n\n- **Loop:** (while loop) part of the code that runs continually. The image (*img* variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red `[X]` button.\n\n> Note: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython), You can find other Python scripts. Try to test the onboard sensors.\n\n## Connecting the Nicla Vision to Edge Impulse Studio\n\nWe will need the Edge Impulse Studio later in other exercises. [Edge Impulse](https://www.edgeimpulse.com/) is a leading development platform for machine learning on edge devices.\n\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\n- Download the most updated [EI Firmware](https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip) and unzip it.\n\n- Open the zip file on your computer and select the uploader corresponding to your OS:\n\n![](images/png/image17.png)\n\n- Put the Nicla-Vision on Boot Mode, pressing the reset button twice.\n\n- Execute the specific batch code for your OS for uploading the binary *arduino-nicla-vision.bin* to your board.\n\nGo to your project on the Studio, and on the `Data Acquisition tab`, select `WebUSB` (1). A window will pop up; choose the option that shows that the `Nicla is paired` (2) and press `[Connect]` (3).\n\n![](images/png/image27.png)\n\nIn the *Collect Data* section on the `Data Acquisition` tab, you can choose which sensor data to pick.\n\n![](images/png/image25.png)\n\nFor example. `IMU data`:\n\n![](images/png/image8.png)\n\nOr Image (`Camera`):\n\n![](images/png/image4.png)\n\nAnd so on. You can also test an external sensor connected to the `ADC` (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF.\n\n## Expanding the Nicla Vision Board (optional)\n\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the [Arduino MKR Connector Carrier (Grove compatible)](https://store-usa.arduino.cc/products/arduino-mkr-connector-carrier-grove-compatible).\n\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\n> Note that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n![](images/jpg/image20.jpg)\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\n![](images/jpg/image26.jpg)\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\n![](images/jpg/image11.jpg)\n\nThe [Grove Light Sensor](https://wiki.seeedstudio.com/Grove-Light_Sensor/) would be connected to one of the single Analog pins (A0/PC4), the [LoRaWAN device](https://wiki.seeedstudio.com/Grove_LoRa_E5_New_Version/) to the UART, and the [OLED](https://arduino.cl/producto/display-oled-grove/) to the I2C connector.\n\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n\n``` python\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\n```\n\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying \"Hello Word\" on the Serial Monitor. Here is the [code](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Arduino-IDE/teste_uart_UNO/teste_uart_UNO.ino).\n\n![](images/jpg/image24.jpg)\n\nBelow is the *Hello World code* to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/ssd1306.py)).\n\n``` python\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\n```\n\nFinally, here is a simple script to read the ADC value on pin \"PC4\" (Nicla pin A0):\n\n``` python\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\n```\n\nThe ADC can be used for other sensor variables, such as [Temperature](https://wiki.seeedstudio.com/Grove-Temperature_Sensor_V1.2/).\n\n> Note that the above scripts ([downloaded from Github](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython)) introduce only how to connect external devices with the Nicla Vision board using MicroPython.\n\n## Conclusion\n\nThe Arduino Nicla Vision is an excellent *tiny device* for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\n> On the [GitHub repository,](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main) you will find the last version of all the codeused or commented on in this hands-on exercise.\n\n## Resources\n\n- [Micropython codes](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython)\n  \n- [Arduino Codes](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Arduino-IDE)\n","srcMarkdownNoYaml":"\n\n# Setup {.unnumbered}\n\n![*DALL·E 3 Prompt: Illustration reminiscent of a 1950s cartoon where the Arduino NICLA VISION board, equipped with a variety of sensors including a camera, is the focal point on an old-fashioned desk. In the background, a computer screen with rounded edges displays the Arduino IDE. The code seen is related to LED configurations and machine learning voice command detection. Outputs on the Serial Monitor explicitly display the words 'yes' and 'no'.*](images/jpg/nicla_sys_ini.jpg)\n\n## Overview\n\nThe [Arduino Nicla Vision](https://docs.arduino.cc/hardware/nicla-vision) (sometimes called *NiclaV*) is a development board that includes two processors that can run tasks in parallel. It is part of a family of development boards with the same form factor but designed for specific tasks, such as the [Nicla Sense ME](https://www.bosch-sensortec.com/software-tools/tools/arduino-nicla-sense-me/) and the [Nicla Voice](https://store-usa.arduino.cc/products/nicla-voice?_gl=1*l3abc6*_ga*MTQ3NzE4Mjk4Mi4xNjQwMDIwOTk5*_ga_NEXN8H46L5*MTY5NjM0Mzk1My4xMDIuMS4xNjk2MzQ0MjQ1LjAuMC4w). The *Niclas* can efficiently run processes created with TensorFlow Lite. For example, one of the cores of the NiclaV runs a computer vision algorithm on the fly (inference), while the other executes low-level operations like controlling a motor and communicating or acting as a user interface. The onboard wireless module allows the management of WiFi and Bluetooth Low Energy (BLE) connectivity simultaneously.\n\n![](images/jpg/image29.jpg)\n\n## Hardware\n\n### Two Parallel Cores\n\nThe central processor is the dual-core [STM32H747,](https://content.arduino.cc/assets/Arduino-Portenta-H7_Datasheet_stm32h747xi.pdf?_gl=1*6quciu*_ga*MTQ3NzE4Mjk4Mi4xNjQwMDIwOTk5*_ga_NEXN8H46L5*MTY0NzQ0NTg1My4xMS4xLjE2NDc0NDYzMzkuMA..) including a Cortex M7 at 480 MHz and a Cortex M4 at 240 MHz. The two cores communicate via a Remote Procedure Call mechanism that seamlessly allows calling functions on the other processor. Both processors share all the on-chip peripherals and can run:\n\n- Arduino sketches on top of the Arm Mbed OS\n\n- Native Mbed applications\n\n- MicroPython / JavaScript via an interpreter\n\n- TensorFlow Lite\n\n![](images/jpg/image22.jpg)\n\n### Memory\n\nMemory is crucial for embedded machine learning projects. The NiclaV board can host up to 16 MB of QSPI Flash for storage. However, it is essential to consider that the MCU SRAM is the one to be used with machine learning inferences; the STM32H747 is only 1MB, shared by both processors. This MCU also has incorporated 2MB of FLASH, mainly for code storage.\n\n### Sensors\n\n- **Camera:** A GC2145 2 MP Color CMOS Camera.\n\n- **Microphone:** The `MP34DT05` is an ultra-compact, low-power, omnidirectional, digital MEMS microphone built with a capacitive sensing element and the IC interface.\n\n- **6-Axis IMU:** 3D gyroscope and 3D accelerometer data from the `LSM6DSOX` 6-axis IMU.\n\n- **Time of Flight Sensor:** The `VL53L1CBV0FY` Time-of-Flight sensor adds accurate and low power-ranging capabilities to the Nicla Vision. The invisible near-infrared VCSEL laser (including the analog driver) is encapsulated with receiving optics in an all-in-one small module below the camera.\n\n## Arduino IDE Installation\n\nStart connecting the board (*microUSB*) to your computer:\n\n![](images/jpg/image14.jpg)\n\nInstall the Mbed OS core for Nicla boards in the Arduino IDE. Having the IDE open, navigate to `Tools > Board > Board Manager`, look for Arduino Nicla Vision on the search window, and install the board.\n\n![](images/jpg/image2.jpg)\n\nNext, go to `Tools > Board > Arduino Mbed OS Nicla Boards` and select `Arduino Nicla Vision`. Having your board connected to the USB, you should see the Nicla on Port and select it.\n\n> Open the Blink sketch on Examples/Basic and run it using the IDE Upload button. You should see the Built-in LED (green RGB) blinking, which means the Nicla board is correctly installed and functional!\n\n### Testing the Microphone\n\nOn Arduino IDE, go to `Examples > PDM > PDMSerialPlotter`, open and run the sketch. Open the Plotter and see the audio representation from the microphone:\n\n![](images/png/image9.png)\n\n> Vary the frequency of the sound you generate and confirm that the mic is working correctly.\n\n### Testing the IMU\n\nBefore testing the IMU, it will be necessary to install the LSM6DSOX library. For that, go to Library Manager and look for LSM6DSOX. Install the library provided by Arduino:\n\n![](images/jpg/image19.jpg)\n\nNext, go to `Examples > Arduino_LSM6DSOX > SimpleAccelerometer` and run the accelerometer test (you can also run Gyro and board temperature):\n\n![](images/png/image28.png)\n\n### Testing the ToF (Time of Flight) Sensor\n\nAs we did with IMU, it is necessary to install the VL53L1X ToF library. For that, go to Library Manager and look for VL53L1X. Install the library provided by Pololu:\n\n![](images/jpg/image15.jpg)\n\nNext, run the sketch [proximity_detection.ino](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Arduino-IDE/proximity_detection/proximity_detection.ino):\n\n![](images/png/image12.png)\n\nOn the Serial Monitor, you will see the distance from the camera to an object in front of it (max of 4m).\n\n![](images/jpg/image13.jpg)\n\n### Testing the Camera\n\nWe can also test the camera using, for example, the code provided on `Examples > Camera > CameraCaptureRawBytes`. We cannot see the image directly, but it is possible to get the raw image data generated by the camera.\n\nAnyway, the best test with the camera is to see a live image. For that, we will use another IDE, the OpenMV.\n\n## Installing the OpenMV IDE\n\nOpenMV IDE is the premier integrated development environment with OpenMV Cameras like the one on the Nicla Vision. It features a powerful text editor, debug terminal, and frame buffer viewer with a histogram display. We will use MicroPython to program the camera.\n\nGo to the [OpenMV IDE page](https://openmv.io/pages/download), download the correct version for your Operating System, and follow the instructions for its installation on your computer.\n\n![](images/png/image21.png)\n\nThe IDE should open, defaulting to the helloworld_1.py code on its Code Area. If not, you can open it from `Files > Examples > HelloWord > helloword.py`\n\n![](images/png/image7.png)\n\nAny messages sent through a serial connection (using print() or error messages) will be displayed on the **Serial Terminal** during run time. The image captured by a camera will be displayed in the **Camera Viewer** Area (or Frame Buffer) and in the Histogram area, immediately below the Camera Viewer.\n\n> Before connecting the Nicla to the OpenMV IDE, ensure you have the latest bootloader version. Go to your Arduino IDE, select the Nicla board, and open the sketch on `Examples > STM_32H747_System STM32H747_manageBootloader`. Upload the code to your board. The Serial Monitor will guide you.\n\nAfter updating the bootloader, put the Nicla Vision in bootloader mode by double-pressing the reset button on the board. The built-in green LED will start fading in and out. Now return to the OpenMV IDE and click on the connect icon (Left ToolBar):\n\n![](images/jpg/image23.jpg)\n\nA pop-up will tell you that a board in DFU mode was detected and ask how you would like to proceed. First, select `Install the latest release firmware (vX.Y.Z)`. This action will install the latest OpenMV firmware on the Nicla Vision.\n\n![](images/png/image10.png)\n\nYou can leave the option `Erase internal file system` unselected and click `[OK]`.\n\nNicla's green LED will start flashing while the OpenMV firmware is uploaded to the board, and a terminal window will then open, showing the flashing progress.\n\n![](images/png/image5.png)\n\nWait until the green LED stops flashing and fading. When the process ends, you will see a message saying, \"DFU firmware update complete!\". Press `[OK]`.\n\n![](images/png/image1.png)\n\nA green play button appears when the Nicla Vison connects to the Tool Bar.\n\n![](images/jpg/image18.jpg)\n\nAlso, note that a drive named \"NO NAME\" will appear on your computer.:\n\n![](images/png/image3.png)\n\nEvery time you press the `[RESET]` button on the board, it automatically executes the *main.py* script stored on it. You can load the [main.py](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/main.py) code on the IDE (`File > Open File...`).\n\n![](images/png/image16.png)\n\n> This code is the \"Blink\" code, confirming that the HW is OK.\n\nFor testing the camera, let's run *helloword_1.py*. For that, select the script on `File > Examples > HelloWorld > helloword.py`,\n\nWhen clicking the green play button, the MicroPython script (*hellowolrd.py*) on the Code Area will be uploaded and run on the Nicla Vision. On-Camera Viewer, you will start to see the video streaming. The Serial Monitor will show us the FPS (Frames per second), which should be around 14fps.\n\n![](images/png/image6.png)\n\nHere is the [helloworld.py](http://helloworld.py/) script:\n\n``` python\n# Hello World Example 2\n#\n# Welcome to the OpenMV IDE! Click on the green run arrow button below to run the script!\n\nimport sensor, image, time\n\nsensor.reset()                      # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565) # Set pixel format to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)   # Set frame size to QVGA (320x240)\nsensor.skip_frames(time = 2000)     # Wait for settings take effect.\nclock = time.clock()                # Create a clock object to track the FPS.\n\nwhile(True):\n    clock.tick()                    # Update the FPS clock.\n    img = sensor.snapshot()         # Take a picture and return the image.\n    print(clock.fps())\n```\n\nIn [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision), you can find the Python scripts used here.\n\nThe code can be split into two parts:\n\n- **Setup:** Where the libraries are imported, initialized and the variables are defined and initiated.\n\n- **Loop:** (while loop) part of the code that runs continually. The image (*img* variable) is captured (one frame). Each of those frames can be used for inference in Machine Learning Applications.\n\nTo interrupt the program execution, press the red `[X]` button.\n\n> Note: OpenMV Cam runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\nIn the [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython), You can find other Python scripts. Try to test the onboard sensors.\n\n## Connecting the Nicla Vision to Edge Impulse Studio\n\nWe will need the Edge Impulse Studio later in other exercises. [Edge Impulse](https://www.edgeimpulse.com/) is a leading development platform for machine learning on edge devices.\n\nEdge Impulse officially supports the Nicla Vision. So, for starting, please create a new project on the Studio and connect the Nicla to it. For that, follow the steps:\n\n- Download the most updated [EI Firmware](https://cdn.edgeimpulse.com/firmware/arduino-nicla-vision.zip) and unzip it.\n\n- Open the zip file on your computer and select the uploader corresponding to your OS:\n\n![](images/png/image17.png)\n\n- Put the Nicla-Vision on Boot Mode, pressing the reset button twice.\n\n- Execute the specific batch code for your OS for uploading the binary *arduino-nicla-vision.bin* to your board.\n\nGo to your project on the Studio, and on the `Data Acquisition tab`, select `WebUSB` (1). A window will pop up; choose the option that shows that the `Nicla is paired` (2) and press `[Connect]` (3).\n\n![](images/png/image27.png)\n\nIn the *Collect Data* section on the `Data Acquisition` tab, you can choose which sensor data to pick.\n\n![](images/png/image25.png)\n\nFor example. `IMU data`:\n\n![](images/png/image8.png)\n\nOr Image (`Camera`):\n\n![](images/png/image4.png)\n\nAnd so on. You can also test an external sensor connected to the `ADC` (Nicla pin 0) and the other onboard sensors, such as the microphone and the ToF.\n\n## Expanding the Nicla Vision Board (optional)\n\nA last item to be explored is that sometimes, during prototyping, it is essential to experiment with external sensors and devices, and an excellent expansion to the Nicla is the [Arduino MKR Connector Carrier (Grove compatible)](https://store-usa.arduino.cc/products/arduino-mkr-connector-carrier-grove-compatible).\n\nThe shield has 14 Grove connectors: five single analog inputs (A0-A5), one double analog input (A5/A6), five single digital I/Os (D0-D4), one double digital I/O (D5/D6), one I2C (TWI), and one UART (Serial). All connectors are 5V compatible.\n\n> Note that all 17 Nicla Vision pins will be connected to the Shield Groves, but some Grove connections remain disconnected.\n\n![](images/jpg/image20.jpg)\n\nThis shield is MKR compatible and can be used with the Nicla Vision and Portenta.\n\n![](images/jpg/image26.jpg)\n\nFor example, suppose that on a TinyML project, you want to send inference results using a LoRaWAN device and add information about local luminosity. Often, with offline operations, a local low-power display such as an OLED is advised. This setup can be seen here:\n\n![](images/jpg/image11.jpg)\n\nThe [Grove Light Sensor](https://wiki.seeedstudio.com/Grove-Light_Sensor/) would be connected to one of the single Analog pins (A0/PC4), the [LoRaWAN device](https://wiki.seeedstudio.com/Grove_LoRa_E5_New_Version/) to the UART, and the [OLED](https://arduino.cl/producto/display-oled-grove/) to the I2C connector.\n\nThe Nicla Pins 3 (Tx) and 4 (Rx) are connected with the Serial Shield connector. The UART communication is used with the LoRaWan device. Here is a simple code to use the UART:\n\n``` python\n# UART Test - By: marcelo_rovai - Sat Sep 23 2023\n\nimport time\nfrom pyb import UART\nfrom pyb import LED\n\nredLED = LED(1) # built-in red LED\n\n# Init UART object.\n# Nicla Vision's UART (TX/RX pins) is on \"LP1\"\nuart = UART(\"LP1\", 9600)\n\nwhile(True):\n    uart.write(\"Hello World!\\r\\n\")\n    redLED.toggle()\n    time.sleep_ms(1000)\n```\n\nTo verify that the UART is working, you should, for example, connect another device as the Arduino UNO, displaying \"Hello Word\" on the Serial Monitor. Here is the [code](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Arduino-IDE/teste_uart_UNO/teste_uart_UNO.ino).\n\n![](images/jpg/image24.jpg)\n\nBelow is the *Hello World code* to be used with the I2C OLED. The MicroPython SSD1306 OLED driver (ssd1306.py), created by Adafruit, should also be uploaded to the Nicla (the ssd1306.py script can be found in [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/ssd1306.py)).\n\n``` python\n# Nicla_OLED_Hello_World - By: marcelo_rovai - Sat Sep 30 2023\n\n#Save on device: MicroPython SSD1306 OLED driver, I2C and SPI interfaces created by Adafruit\nimport ssd1306\n\nfrom machine import I2C\ni2c = I2C(1)\n\noled_width = 128\noled_height = 64\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\noled.text('Hello, World', 10, 10)\noled.show()\n```\n\nFinally, here is a simple script to read the ADC value on pin \"PC4\" (Nicla pin A0):\n\n``` python\n\n# Light Sensor (A0) - By: marcelo_rovai - Wed Oct 4 2023\n\nimport pyb\nfrom time import sleep\n\nadc = pyb.ADC(pyb.Pin(\"PC4\"))     # create an analog object from a pin\nval = adc.read()                  # read an analog value\n\nwhile (True):\n\n    val = adc.read()  \n    print (\"Light={}\".format (val))\n    sleep (1)\n```\n\nThe ADC can be used for other sensor variables, such as [Temperature](https://wiki.seeedstudio.com/Grove-Temperature_Sensor_V1.2/).\n\n> Note that the above scripts ([downloaded from Github](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython)) introduce only how to connect external devices with the Nicla Vision board using MicroPython.\n\n## Conclusion\n\nThe Arduino Nicla Vision is an excellent *tiny device* for industrial and professional uses! However, it is powerful, trustworthy, low power, and has suitable sensors for the most common embedded machine learning applications such as vision, movement, sensor fusion, and sound.\n\n> On the [GitHub repository,](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main) you will find the last version of all the codeused or commented on in this hands-on exercise.\n\n## Resources\n\n- [Micropython codes](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython)\n  \n- [Arduino Codes](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Arduino-IDE)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":8,"fig-height":6,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../../../custom_callout.lua"],"reference-location":"margin","highlight-style":"github","toc":true,"toc-depth":4,"include-in-header":{"text":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN\"></script>\n<script type=\"module\"  src=\"/scripts/ai_menu/dist/bundle.js\" defer></script>\n"},"citeproc":true,"output-file":"setup.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib","setup.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"lightbox":true,"theme":{"light":["default","../../../../../style.scss","../../../../../style-light.scss"],"dark":["darkly","../../../../../style.scss","../../../../../style-dark.scss"]},"code-block-bg":true,"code-block-border-left":"#A51C30","table":{"classes":["table-striped","table-hover"]},"citation-location":"margin","sidenote":true,"linkcolor":"#A51C30","urlcolor":"#A51C30","anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"number-depth":3},"extensions":{"book":{"multiFile":true}}},"titlepage-pdf":{"identifier":{"display-name":"PDF","target-format":"titlepage-pdf","base-format":"pdf","extension-name":"titlepage"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":["../../../../../_extensions/nmfs-opensci/titlepage/fonts/qualitype/opentype/QTDublinIrish.otf"],"shortcodes":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\titlepage-theme.lua","C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\coverpage-theme.lua","../../../../../custom_callout.lua"],"toc":true,"top-level-division":"chapter","number-sections":true,"toc-depth":3,"cite-method":"citeproc","reference-location":"margin","include-in-header":[{"file":"../../../../../tex/header-includes.tex"}],"output-file":"setup.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"template-partials":["../../../../../_extensions/nmfs-opensci/titlepage/_coverpage.tex","../../../../../_extensions/nmfs-opensci/titlepage/_author-affiliation-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_header-footer-date-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_title-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_titlepage.tex","../../../../../_extensions/nmfs-opensci/titlepage/before-body.tex","../../../../../_extensions/nmfs-opensci/titlepage/pandoc.tex"],"revealjs-plugins":[],"bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib","setup.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"documentclass":"scrbook","classoption":["abstract","titlepage"],"coverpage":true,"coverpage-title":"Machine Learning Systems","coverpage-bg-image":"../../../../../cover-image-transparent.png","coverpage-author":["Vijay","Janapa Reddi"],"coverpage-theme":{"page-text-align":"center","bg-image-left":"0.225\\paperwidth","bg-image-bottom":7,"bg-image-rotate":0,"bg-image-opacity":1,"author-style":"plain","author-sep":"newline","author-fontsize":20,"author-align":"right","author-bottom":"0.15\\paperwidth","author-left":"7in","author-width":"6in","footer-style":"none","header-style":"none","date-style":"none","title-fontsize":57,"title-left":"0.075\\paperwidth","title-bottom":"0.375\\paperwidth","title-width":"0.9\\paperwidth"},"titlepage":true,"titlepage-theme":{"elements":["\\titleblock","Prof. Vijay Janapa Reddi","School of Engineering and Applied Sciences","Harvard University","\\vfill","With heartfelt gratitude to the community for their invaluable contributions and steadfast support.","\\vfill"],"page-align":"left","title-style":"plain","title-fontstyle":["huge","bfseries"],"title-space-after":"4\\baselineskip","title-subtitle-space-between":"0.05\\textheight","subtitle-fontstyle":["large","textit"],"author-style":"superscript-with-and","author-fontstyle":"large","affiliation-style":"numbered-list-with-correspondence","affiliation-fontstyle":"large","affiliation-space-after":"0pt","footer-style":"plain","footer-fontstyle":"large","logo-size":"0.15\\textheight","logo-space-after":"1\\baselineskip","vrule-width":"2pt","vrule-align":"left","vrule-color":"black"},"lof":false,"lot":false,"latex-engine":"xelatex","citation-package":"natbib","link-citations":true,"biblio-title":"References","title-block-style":"none","indent":"0px","fontsize":"10pt","citation-location":"block","fig-caption":true,"cap-location":"margin","fig-cap-location":"margin","tbl-cap-location":"margin","hyperrefoptions":["linktoc=all","pdfwindowui","pdfpagemode=FullScreen","pdfpagelayout=TwoPageRight"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","titlepage-pdf"]}