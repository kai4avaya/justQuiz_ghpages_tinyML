{"title":"Motion Classification and Anomaly Detection","markdown":{"yaml":{"bibliography":"motion_classification.bib"},"headingText":"Motion Classification and Anomaly Detection","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n![*DALLÂ·E 3 Prompt: 1950s style cartoon illustration depicting a movement research room. In the center of the room, there's a simulated container used for transporting goods on trucks, boats, and forklifts. The container is detailed with rivets and markings typical of industrial cargo boxes. Around the container, the room is filled with vintage equipment, including an oscilloscope, various sensor arrays, and large paper rolls of recorded data. The walls are adorned with educational posters about transportation safety and logistics. The overall ambiance of the room is nostalgic and scientific, with a hint of industrial flair.*](images/jpg/movement_anomaly_ini.jpg)\n\n\n## Overview\n\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers' safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\n\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\n\n::: callout-tip\n## Learning Objectives\n\n- Setting up the Arduino Nicla Vision Board\n- Data Collection and Preprocessing\n- Building the Motion Classification Model\n- Implementing Anomaly Detection\n- Real-world Testing and Analysis\n:::\n\nBy the end of this tutorial, you'll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.\n\n## IMU Installation and testing\n\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, *Setup Nicla Vision*, the Nicla Vision Board has an onboard **6-axis IMU:** 3D gyroscope and 3D accelerometer, the [LSM6DSOX](https://www.st.com/resource/en/datasheet/lsm6dsox.pdf). Let's verify if the [LSM6DSOX IMU library](https://github.com/arduino-libraries/Arduino_LSM6DSOX) is installed. If not, install it.\n\n![](images/jpg/imu_ide.jpg)\n\nNext, go to `Examples > Arduino_LSM6DSOX > SimpleAccelerometer` and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n\n![](images/jpg/imu_test.jpg)\n\n### Defining the Sampling frequency:\n\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you're interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\n1. **Nature of the Motion:** Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\n\n2. **Hardware Limitations:** The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\n\n3. **Computational Resources:** Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\n\n4. **Battery Life:** A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\n\n5. **Data Storage:** More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, **sampling rates of around 50 Hz to 100 Hz** are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\n\nLet's define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50Hz):\n\n``` cpp\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include <Arduino_LSM6DSOX.h>\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() > last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n  }\n}\n```\n\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n\n![](images/jpg/sampling.jpg)\n\n> Note that with the Nicla board resting on a table (with the camera facing down), the z-axis measures around 9.8m/s$^2$, the expected earth acceleration.\n\n## The Case Study: Simulated Container Transportation\n\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we'll capture motion data by manually simulating the conditions of:\n\n1. **Terrestrial** Transportation (by road or train)\n2. **Maritime**-associated Transportation\n3. Vertical Movement via Fork-**Lift**\n4. Stationary **(Idle**) period in a Warehouse\n\n![](images/jpg/classes.jpg)\n\nFrom the above images, we can define for our simulation that primarily horizontal movements (x or y axis) should be associated with the \"Terrestrial class,\" Vertical movements (z-axis) with the \"Lift Class,\" no activity with the \"Idle class,\" and movement on all three axes to [Maritime class.](https://www.containerhandbuch.de/chb_e/stra/index.html?/chb_e/stra/stra_02_03_03.htm)\n\n![](images/jpg/classes_mov_def.jpg)\n\n## Data Collection\n\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: [Sensor DataLogger](https://www.hackster.io/mjrobot/sensor-datalogger-50e44d)). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the [CSV Wizard tool](https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/csv-wizard).\n\n> In this [video](https://youtu.be/2KBPq_826WM), you can learn alternative ways to send data to the Edge Impulse Studio.\n\n### Connecting the device to Edge Impulse\n\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\n1. Download the latest firmware and connect it directly to the `Data Collection` section.\n2. Use the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the *Setup Nicla Vision* hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let's do it with the last one.\n\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\n1. Install the [Edge Impulse CLI](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-installation) and the [Node.js](https://nodejs.org/en/) into your computer.\n2. Upload a sketch for data capture (the one discussed previously in this tutorial).\n3. Use the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) to capture data from the Nicla's accelerometer and send it to the Studio, as shown in this diagram:\n\n![](images/jpg/data-forw.jpg)\n\nStart the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) on your terminal, entering (if it is the first time) the following command:\n\n```         \n$ edge-impulse-data-forwarder --clean\n```\n\nNext, enter your EI credentials and choose your project, variables (for example, *accX,* *accY*, and *accZ*), and device name (for example, *NiclaV*:\n\n![](images/jpg/term.jpg)\n\nGo to the `Devices` section on your EI Project and verify if the device is connected (the dot should be green):\n\n![](images/jpg/device.jpg)\n\n> You can clone the project developed for this hands-on: [NICLA Vision Movement Classification](https://studio.edgeimpulse.com/public/302078/latest).\n\n### Data Collection\n\nOn the `Data Acquisition` section, you should see that your board `[NiclaV]` is connected. The sensor is available: `[sensor with 3 axes (accX, accY, accZ)]` with a sampling frequency of `[50Hz]`. The Studio suggests a sample length of `[10000]` ms (10s). The last thing left is defining the sample label. Let's start with`[terrestrial]`:\n\n![](images/jpg/collect_data.jpg)\n\n**Terrestrial** (palettes in a Truck or Train), moving horizontally. Press `[Start Sample]`and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n\n![](images/jpg/terrestrial_result.jpg)\n\nAs expected, the movement was captured mainly in the Y-axis (green). In the blue, we see the Z axis, around -10 m/s$^2$ (the Nicla has the camera facing up).\n\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\n\n**Maritime** (pallets in boats into an angry ocean). The movement is captured on all three axes:\n\n![](images/jpg/maritime_result.jpg)\n\n**Lift** (Palettes being handled vertically by a Forklift). Movement captured only in the Z-axis:\n\n![](images/jpg/lift_result.jpg)\n\n**Idle** (Paletts in a warehouse). No movement detected by the accelerometer:\n\n![](images/jpg/idle_result.jpg)\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the `three dots` menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic `Train/Test Split tool` on the `Danger Zone` of `Dashboard` tab. Below, you can see the resulting dataset:\n\n![](images/jpg/dataset.jpg)\n\nOnce you have captured your dataset, you can explore it in more detail using the [Data Explorer](https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/data-explorer), a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) or [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n\n![](images/jpg/data_explorer.jpg)\n\nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only \"vertical\".\n\n## Impulse Design\n\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a *learning block* to classify new data. Go to `Impulse Design` and `Create Impulse`. The Studio will suggest the basic design. Let's also add a second *Learning Block* for `Anomaly Detection`.\n\n![](images/jpg/impulse.jpg)\n\nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n\n![](images/jpg/anomaly_detect.jpg)\n\nThe sampling frequency should be automatically captured, if not, enter it: `[50]`Hz. The Studio suggests a *Window Size* of 2 seconds (`[2000]` ms) with a *sliding window* of `[20]`ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n\n![](images/jpg/impulse-block.jpg)\n\nLet's dig into those steps and parameters to understand better what we are doing here.\n\n### Data Pre-Processing Overview\n\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object's motion, such as movement patterns and vibrations.\n\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or **windows**. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The **window size** and overlap (**window increase**) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of \"cycles of data\".\n\n> With a sampling rate (SR) of 50Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis x 2 seconds x 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n![](images/jpg/pre-process.jpg)\n\nOnce the data is preprocessed and segmented, you can extract features that describe the motion's characteristics. Some typical features extracted from accelerometer data include:\n\n- **Time-domain** features describe the data's statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\n- **Frequency-domain** features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\n- **Time-frequency** domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal's frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n### EI Studio Spectral Features\n\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the [Spectral Features Block](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features).\n\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as [FFT](https://en.wikipedia.org/wiki/Fast_Fourier_transform) or [Wavelets](https://en.wikipedia.org/wiki/Digital_signal_processing#Wavelet).\n\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of `[32]`.\n\nThe per axis/channel **Time Domain Statistical features** are:\n\n- [RMS](https://en.wikipedia.org/wiki/Root_mean_square): 1 feature\n- [Skewness](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSkewness): 1 feature\n- [Kurtosis](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKurtosis): 1 feature\n\nThe per axis/channel **Frequency Domain Spectral features** are:\n\n- [Spectral Power](https://en.wikipedia.org/wiki/Spectral_density): 16 features (FFT Length/2)\n- Skewness: 1 feature\n- Kurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\n> You can learn more about how each feature is calculated by downloading the notebook [Edge Impulse - Spectral Features Block Analysis](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb) [TinyML under the hood: Spectral Analysis](https://www.hackster.io/mjrobot/tinyml-under-the-hood-spectral-analysis-94676c) or [opening it directly on Google CoLab](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb).\n\n### Generating features\n\nOnce we understand what the pre-processing does, it is time to finish the job. So, let's take the raw data (time-series type) and convert it to tabular data. For that, go to the `Spectral Features` section on the `Parameters` tab, define the main parameters as discussed in the previous section (`[FFT]` with `[32]` points), and select`[Save Parameters]`:\n\n![](images/jpg/Parameters_definition.jpg)\n\nAt the top menu, select the `Generate Features` option and the `Generate Features` button. Each 2-second window data will be converted into one data point of 63 features.\n\n> The Feature Explorer will show those data in 2D using [UMAP.](https://umap-learn.readthedocs.io/en/latest/) Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.\n\n![](images/jpg/feature_generation.jpg)\n\n## Models Training\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\n![](images/jpg/model.jpg)\n\nAs hyperparameters, we will use a Learning Rate of `[0.005]`, a Batch size of `[32]`, and `[20]`% of data for validation for `[30]` epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n\n![](images/jpg/train.jpg)\n\nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be `[32]`, as suggested by the Studio:\n\n![](images/jpg/anom_det_train.jpg)\n\n## Testing\n\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\n\nThe default minimum threshold for a considered uncertain result is `[0.6]` for classification and `[0.3]` for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can `Set confidence thresholds` on the `three dots` menu, besides the `Classify all` button.\n\n![](images/jpg/model_testing.jpg)\n\nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\n> Be aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the **model is NOT in your device**).\n\n## Deploy\n\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option `Arduino Library`, and at the bottom, you can choose `Quantized (Int8)` or `Unoptimized (float32)` and `[Build]`. A Zip file will be created and downloaded to your computer.\n\n![](images/jpg/deploy.jpg)\n\nOn your Arduino IDE, go to the `Sketch` tab, select `Add.ZIP Library`, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: `Library installed`.\n\n### Inference\n\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let's change one of the code examples created when you deploy the Arduino Library.\n\nIn your Arduino IDE, go to the `File/Examples` tab and look for your project, and on examples, select `Nicla_vision_fusion`:\n\n![](images/jpg/inference.jpg)\n\nNote that the code created by Edge Impulse considers a *sensor fusion* approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n\n``` cpp\n/* Includes ---------------------------------------------------------------- */\n#include <NICLA_Vision_Movement_Classification_inferencing.h> \n#include <Arduino_LSM6DSOX.h> //IMU\n#include \"VL53L1X.h\" // ToF\n```\n\n> You can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\n\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla `[RESET]` button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\n\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\n- **Idle and lift classes:**\n\n![](images/jpg/inference_1.jpg)\n\n- **Maritime and terrestrial:**\n\n![](images/jpg/inference_2.jpg)\n\nNote that in all situations above, the value of the `anomaly score` was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, \"rolling\" the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\n- **Anomaly detection:**\n\n![](images/jpg/anomaly-boat.jpg)\n\nIn this case, the anomaly is much bigger, over 1.00\n\n### Post-processing\n\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\n\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if *terrestrial* is detected, the Green LED will light; if *maritime*, the Red LED will light, if it is a *lift,* the Blue LED will light; and if no movement is detected *(idle*), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).\n\n## Conclusion\n\n> The notebooks and codeused in this hands-on tutorial will be found on the [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Motion_Classification) repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\n### Case Applications\n\n#### Industrial and Manufacturing\n\n- **Predictive Maintenance:** Detecting anomalies in machinery motion to predict failures before they occur.\n- **Quality Control:** Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\n- **Warehouse Logistics:** Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n#### Healthcare\n\n- **Patient Monitoring:** Detecting falls or abnormal movements in the elderly or those with mobility issues.\n- **Rehabilitation:** Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\n- **Activity Recognition:** Classifying types of physical activity for fitness applications or patient monitoring.\n\n#### Consumer Electronics\n\n- **Gesture Control:** Interpreting specific motions to control devices, such as turning on lights with a hand wave.\n- **Gaming:** Enhancing gaming experiences with motion-controlled inputs.\n\n#### Transportation and Logistics\n\n- **Vehicle Telematics:** Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\n- **Cargo Monitoring:** Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n#### Smart Cities and Infrastructure\n\n- **Structural Health Monitoring:** Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\n- **Traffic Management:** Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n#### Security and Surveillance\n\n- **Intruder Detection:** Detecting motion patterns typical of unauthorized access or other security breaches.\n- **Wildlife Monitoring:** Detecting poachers or abnormal animal movements in protected areas.\n\n#### Agriculture\n\n- **Equipment Monitoring:** Tracking the performance and usage of agricultural machinery.\n- **Animal Behavior Analysis:** Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n#### Environmental Monitoring\n\n- **Seismic Activity:** Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\n- **Oceanography:** Studying wave patterns or marine movements for research and safety purposes.\n\n### Nicla 3D case\n\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: [Arduino Nicla Voice and Vision Wearable Case](https://www.thingiverse.com/thing:5923305).\n\n![](images/jpg/case.jpg)\n\nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.\n\n## Resources\n\n* [Arduino Code](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Motion_Classification/Niclav_Acc_Data_Capture)\n\n* [Edge Impulse Spectral Features Block Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb)\n\n* [Edge Impulse Project](https://studio.edgeimpulse.com/public/302078/latest)\n","srcMarkdownNoYaml":"\n\n# Motion Classification and Anomaly Detection {.unnumbered}\n\n![*DALLÂ·E 3 Prompt: 1950s style cartoon illustration depicting a movement research room. In the center of the room, there's a simulated container used for transporting goods on trucks, boats, and forklifts. The container is detailed with rivets and markings typical of industrial cargo boxes. Around the container, the room is filled with vintage equipment, including an oscilloscope, various sensor arrays, and large paper rolls of recorded data. The walls are adorned with educational posters about transportation safety and logistics. The overall ambiance of the room is nostalgic and scientific, with a hint of industrial flair.*](images/jpg/movement_anomaly_ini.jpg)\n\n\n## Overview\n\nTransportation is the backbone of global commerce. Millions of containers are transported daily via various means, such as ships, trucks, and trains, to destinations worldwide. Ensuring these containers' safe and efficient transit is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly one of them.\n\nIn this hands-on tutorial, we will work to solve real-world problems related to transportation. We will develop a Motion Classification and Anomaly Detection system using the Arduino Nicla Vision board, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, such as terrestrial and maritime transit, vertical movement via forklifts, and stationary periods in warehouses.\n\n::: callout-tip\n## Learning Objectives\n\n- Setting up the Arduino Nicla Vision Board\n- Data Collection and Preprocessing\n- Building the Motion Classification Model\n- Implementing Anomaly Detection\n- Real-world Testing and Analysis\n:::\n\nBy the end of this tutorial, you'll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can be a stepping stone to more advanced projects in the burgeoning field of TinyML involving vibration.\n\n## IMU Installation and testing\n\nFor this project, we will use an accelerometer. As discussed in the Hands-On Tutorial, *Setup Nicla Vision*, the Nicla Vision Board has an onboard **6-axis IMU:** 3D gyroscope and 3D accelerometer, the [LSM6DSOX](https://www.st.com/resource/en/datasheet/lsm6dsox.pdf). Let's verify if the [LSM6DSOX IMU library](https://github.com/arduino-libraries/Arduino_LSM6DSOX) is installed. If not, install it.\n\n![](images/jpg/imu_ide.jpg)\n\nNext, go to `Examples > Arduino_LSM6DSOX > SimpleAccelerometer` and run the accelerometer test. You can check if it works by opening the IDE Serial Monitor or Plotter. The values are in g (earth gravity), with a default range of +/- 4g:\n\n![](images/jpg/imu_test.jpg)\n\n### Defining the Sampling frequency:\n\nChoosing an appropriate sampling frequency is crucial for capturing the motion characteristics you're interested in studying. The Nyquist-Shannon sampling theorem states that the sampling rate should be at least twice the highest frequency component in the signal to reconstruct it properly. In the context of motion classification and anomaly detection for transportation, the choice of sampling frequency would depend on several factors:\n\n1. **Nature of the Motion:** Different types of transportation (terrestrial, maritime, etc.) may involve different ranges of motion frequencies. Faster movements may require higher sampling frequencies.\n\n2. **Hardware Limitations:** The Arduino Nicla Vision board and any associated sensors may have limitations on how fast they can sample data.\n\n3. **Computational Resources:** Higher sampling rates will generate more data, which might be computationally intensive, especially critical in a TinyML environment.\n\n4. **Battery Life:** A higher sampling rate will consume more power. If the system is battery-operated, this is an important consideration.\n\n5. **Data Storage:** More frequent sampling will require more storage space, another crucial consideration for embedded systems with limited memory.\n\nIn many human activity recognition tasks, **sampling rates of around 50 Hz to 100 Hz** are commonly used. Given that we are simulating transportation scenarios, which are generally not high-frequency events, a sampling rate in that range (50-100 Hz) might be a reasonable starting point.\n\nLet's define a sketch that will allow us to capture our data with a defined sampling frequency (for example, 50Hz):\n\n``` cpp\n/*\n * Based on Edge Impulse Data Forwarder Example (Arduino)\n  - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * Developed by M.Rovai @11May23\n */\n\n/* Include ----------------------------------------------------------------- */\n#include <Arduino_LSM6DSOX.h>\n\n/* Constant defines -------------------------------------------------------- */\n#define CONVERT_G_TO_MS2 9.80665f\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nstatic unsigned long last_interval_ms = 0;\nfloat x, y, z;\n\nvoid setup() {\n  Serial.begin(9600);\n  while (!Serial);\n\n  if (!IMU.begin()) {\n    Serial.println(\"Failed to initialize IMU!\");\n    while (1);\n  }\n}\n\nvoid loop() {\n  if (millis() > last_interval_ms + INTERVAL_MS) {\n    last_interval_ms = millis();\n    \n    if (IMU.accelerationAvailable()) {\n      // Read raw acceleration measurements from the device\n      IMU.readAcceleration(x, y, z);\n\n      // converting to m/s2\n      float ax_m_s2 = x * CONVERT_G_TO_MS2;\n      float ay_m_s2 = y * CONVERT_G_TO_MS2;\n      float az_m_s2 = z * CONVERT_G_TO_MS2;\n\n      Serial.print(ax_m_s2); \n      Serial.print(\"\\t\");\n      Serial.print(ay_m_s2); \n      Serial.print(\"\\t\");\n      Serial.println(az_m_s2); \n    }\n  }\n}\n```\n\nUploading the sketch and inspecting the Serial Monitor, we can see that we are capturing 50 samples per second.\n\n![](images/jpg/sampling.jpg)\n\n> Note that with the Nicla board resting on a table (with the camera facing down), the z-axis measures around 9.8m/s$^2$, the expected earth acceleration.\n\n## The Case Study: Simulated Container Transportation\n\nWe will simulate container (or better package) transportation through different scenarios to make this tutorial more relatable and practical. Using the built-in accelerometer of the Arduino Nicla Vision board, we'll capture motion data by manually simulating the conditions of:\n\n1. **Terrestrial** Transportation (by road or train)\n2. **Maritime**-associated Transportation\n3. Vertical Movement via Fork-**Lift**\n4. Stationary **(Idle**) period in a Warehouse\n\n![](images/jpg/classes.jpg)\n\nFrom the above images, we can define for our simulation that primarily horizontal movements (x or y axis) should be associated with the \"Terrestrial class,\" Vertical movements (z-axis) with the \"Lift Class,\" no activity with the \"Idle class,\" and movement on all three axes to [Maritime class.](https://www.containerhandbuch.de/chb_e/stra/index.html?/chb_e/stra/stra_02_03_03.htm)\n\n![](images/jpg/classes_mov_def.jpg)\n\n## Data Collection\n\nFor data collection, we can have several options. In a real case, we can have our device, for example, connected directly to one container, and the data collected on a file (for example .CSV) and stored on an SD card (Via SPI connection) or an offline repo in your computer. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Bluetooth (as done in this project: [Sensor DataLogger](https://www.hackster.io/mjrobot/sensor-datalogger-50e44d)). Once your dataset is collected and stored as a .CSV file, it can be uploaded to the Studio using the [CSV Wizard tool](https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/csv-wizard).\n\n> In this [video](https://youtu.be/2KBPq_826WM), you can learn alternative ways to send data to the Edge Impulse Studio.\n\n### Connecting the device to Edge Impulse\n\nWe will connect the Nicla directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment. For that, you have two options:\n\n1. Download the latest firmware and connect it directly to the `Data Collection` section.\n2. Use the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) tool to capture sensor data from the sensor and send it to the Studio.\n\nOption 1 is more straightforward, as we saw in the *Setup Nicla Vision* hands-on, but option 2 will give you more flexibility regarding capturing your data, such as sampling frequency definition. Let's do it with the last one.\n\nPlease create a new project on the Edge Impulse Studio (EIS) and connect the Nicla to it, following these steps:\n\n1. Install the [Edge Impulse CLI](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-installation) and the [Node.js](https://nodejs.org/en/) into your computer.\n2. Upload a sketch for data capture (the one discussed previously in this tutorial).\n3. Use the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) to capture data from the Nicla's accelerometer and send it to the Studio, as shown in this diagram:\n\n![](images/jpg/data-forw.jpg)\n\nStart the [CLI Data Forwarder](https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder) on your terminal, entering (if it is the first time) the following command:\n\n```         \n$ edge-impulse-data-forwarder --clean\n```\n\nNext, enter your EI credentials and choose your project, variables (for example, *accX,* *accY*, and *accZ*), and device name (for example, *NiclaV*:\n\n![](images/jpg/term.jpg)\n\nGo to the `Devices` section on your EI Project and verify if the device is connected (the dot should be green):\n\n![](images/jpg/device.jpg)\n\n> You can clone the project developed for this hands-on: [NICLA Vision Movement Classification](https://studio.edgeimpulse.com/public/302078/latest).\n\n### Data Collection\n\nOn the `Data Acquisition` section, you should see that your board `[NiclaV]` is connected. The sensor is available: `[sensor with 3 axes (accX, accY, accZ)]` with a sampling frequency of `[50Hz]`. The Studio suggests a sample length of `[10000]` ms (10s). The last thing left is defining the sample label. Let's start with`[terrestrial]`:\n\n![](images/jpg/collect_data.jpg)\n\n**Terrestrial** (palettes in a Truck or Train), moving horizontally. Press `[Start Sample]`and move your device horizontally, keeping one direction over your table. After 10 s, your data will be uploaded to the studio. Here is how the sample was collected:\n\n![](images/jpg/terrestrial_result.jpg)\n\nAs expected, the movement was captured mainly in the Y-axis (green). In the blue, we see the Z axis, around -10 m/s$^2$ (the Nicla has the camera facing up).\n\nAs discussed before, we should capture data from all four Transportation Classes. So, imagine that you have a container with a built-in accelerometer facing the following situations:\n\n**Maritime** (pallets in boats into an angry ocean). The movement is captured on all three axes:\n\n![](images/jpg/maritime_result.jpg)\n\n**Lift** (Palettes being handled vertically by a Forklift). Movement captured only in the Z-axis:\n\n![](images/jpg/lift_result.jpg)\n\n**Idle** (Paletts in a warehouse). No movement detected by the accelerometer:\n\n![](images/jpg/idle_result.jpg)\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds) for each of the four classes (a total of 8 minutes of data). Using the `three dots` menu after each one of the samples, select 2 of them, reserving them for the Test set. Alternatively, you can use the automatic `Train/Test Split tool` on the `Danger Zone` of `Dashboard` tab. Below, you can see the resulting dataset:\n\n![](images/jpg/dataset.jpg)\n\nOnce you have captured your dataset, you can explore it in more detail using the [Data Explorer](https://docs.edgeimpulse.com/docs/edge-impulse-studio/data-acquisition/data-explorer), a visual tool to find outliers or mislabeled data (helping to correct them). The data explorer first tries to extract meaningful features from your data (by applying signal processing and neural network embeddings) and then uses a dimensionality reduction algorithm such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) or [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to map these features to a 2D space. This gives you a one-look overview of your complete dataset.\n\n![](images/jpg/data_explorer.jpg)\n\nIn our case, the dataset seems OK (good separation). But the PCA shows we can have issues between maritime (green) and lift (orange). This is expected, once on a boat, sometimes the movement can be only \"vertical\".\n\n## Impulse Design\n\nThe next step is the definition of our Impulse, which takes the raw data and uses signal processing to extract features, passing them as the input tensor of a *learning block* to classify new data. Go to `Impulse Design` and `Create Impulse`. The Studio will suggest the basic design. Let's also add a second *Learning Block* for `Anomaly Detection`.\n\n![](images/jpg/impulse.jpg)\n\nThis second model uses a K-means model. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly such as a container rolling out of a ship on the ocean or falling from a Forklift.\n\n![](images/jpg/anomaly_detect.jpg)\n\nThe sampling frequency should be automatically captured, if not, enter it: `[50]`Hz. The Studio suggests a *Window Size* of 2 seconds (`[2000]` ms) with a *sliding window* of `[20]`ms. What we are defining in this step is that we will pre-process the captured data (Time-Seres data), creating a tabular dataset features) that will be the input for a Neural Networks Classifier (DNN) and an Anomaly Detection model (K-Means), as shown below:\n\n![](images/jpg/impulse-block.jpg)\n\nLet's dig into those steps and parameters to understand better what we are doing here.\n\n### Data Pre-Processing Overview\n\nData pre-processing is extracting features from the dataset captured with the accelerometer, which involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object's motion, such as movement patterns and vibrations.\n\nRaw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can clean and standardize the data, making it more suitable for feature extraction. In our case, we should divide the data into smaller segments or **windows**. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The **window size** and overlap (**window increase**) choice depend on the application and the frequency of the events of interest. As a thumb rule, we should try to capture a couple of \"cycles of data\".\n\n> With a sampling rate (SR) of 50Hz and a window size of 2 seconds, we will get 100 samples per axis, or 300 in total (3 axis x 2 seconds x 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\n![](images/jpg/pre-process.jpg)\n\nOnce the data is preprocessed and segmented, you can extract features that describe the motion's characteristics. Some typical features extracted from accelerometer data include:\n\n- **Time-domain** features describe the data's statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\n- **Frequency-domain** features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\n- **Time-frequency** domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal's frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature importance calculations.\n\n### EI Studio Spectral Features\n\nData preprocessing is a challenging area for embedded machine learning, still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the [Spectral Features Block](https://docs.edgeimpulse.com/docs/edge-impulse-studio/processing-blocks/spectral-features).\n\nOn the Studio, the collected raw dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as [FFT](https://en.wikipedia.org/wiki/Fast_Fourier_transform) or [Wavelets](https://en.wikipedia.org/wiki/Digital_signal_processing#Wavelet).\n\nFor our project, once the time signal is continuous, we should use FFT with, for example, a length of `[32]`.\n\nThe per axis/channel **Time Domain Statistical features** are:\n\n- [RMS](https://en.wikipedia.org/wiki/Root_mean_square): 1 feature\n- [Skewness](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSkewness): 1 feature\n- [Kurtosis](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKurtosis): 1 feature\n\nThe per axis/channel **Frequency Domain Spectral features** are:\n\n- [Spectral Power](https://en.wikipedia.org/wiki/Spectral_density): 16 features (FFT Length/2)\n- Skewness: 1 feature\n- Kurtosis: 1 feature\n\nSo, for an FFT length of 32 points, the resulting output of the Spectral Analysis Block will be 21 features per axis (a total of 63 features).\n\n> You can learn more about how each feature is calculated by downloading the notebook [Edge Impulse - Spectral Features Block Analysis](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb) [TinyML under the hood: Spectral Analysis](https://www.hackster.io/mjrobot/tinyml-under-the-hood-spectral-analysis-94676c) or [opening it directly on Google CoLab](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb).\n\n### Generating features\n\nOnce we understand what the pre-processing does, it is time to finish the job. So, let's take the raw data (time-series type) and convert it to tabular data. For that, go to the `Spectral Features` section on the `Parameters` tab, define the main parameters as discussed in the previous section (`[FFT]` with `[32]` points), and select`[Save Parameters]`:\n\n![](images/jpg/Parameters_definition.jpg)\n\nAt the top menu, select the `Generate Features` option and the `Generate Features` button. Each 2-second window data will be converted into one data point of 63 features.\n\n> The Feature Explorer will show those data in 2D using [UMAP.](https://umap-learn.readthedocs.io/en/latest/) Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but is also applicable for general non-linear dimension reduction.\n\nThe visualization makes it possible to verify that after the feature generation, the classes present keep their excellent separation, which indicates that the classifier should work well. Optionally, you can analyze how important each one of the features is for one class compared with others.\n\n![](images/jpg/feature_generation.jpg)\n\n## Models Training\n\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\n![](images/jpg/model.jpg)\n\nAs hyperparameters, we will use a Learning Rate of `[0.005]`, a Batch size of `[32]`, and `[20]`% of data for validation for `[30]` epochs. After training, we can see that the accuracy is 98.5%. The cost of memory and latency is meager.\n\n![](images/jpg/train.jpg)\n\nFor Anomaly Detection, we will choose the suggested features that are precisely the most important ones in the Feature Extraction, plus the accZ RMS. The number of clusters will be `[32]`, as suggested by the Studio:\n\n![](images/jpg/anom_det_train.jpg)\n\n## Testing\n\nWe can verify how our model will behave with unknown data using 20% of the data left behind during the data capture phase. The result was almost 95%, which is good. You can always work to improve the results, for example, to understand what went wrong with one of the wrong results. If it is a unique situation, you can add it to the training dataset and then repeat it.\n\nThe default minimum threshold for a considered uncertain result is `[0.6]` for classification and `[0.3]` for anomaly. Once we have four classes (their output sum should be 1.0), you can also set up a lower threshold for a class to be considered valid (for example, 0.4). You can `Set confidence thresholds` on the `three dots` menu, besides the `Classify all` button.\n\n![](images/jpg/model_testing.jpg)\n\nYou can also perform Live Classification with your device (which should still be connected to the Studio).\n\n> Be aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the **model is NOT in your device**).\n\n## Deploy\n\nIt is time to deploy the preprocessing block and the trained model to the Nicla. The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option `Arduino Library`, and at the bottom, you can choose `Quantized (Int8)` or `Unoptimized (float32)` and `[Build]`. A Zip file will be created and downloaded to your computer.\n\n![](images/jpg/deploy.jpg)\n\nOn your Arduino IDE, go to the `Sketch` tab, select `Add.ZIP Library`, and Choose the.zip file downloaded by the Studio. A message will appear in the IDE Terminal: `Library installed`.\n\n### Inference\n\nNow, it is time for a real test. We will make inferences wholly disconnected from the Studio. Let's change one of the code examples created when you deploy the Arduino Library.\n\nIn your Arduino IDE, go to the `File/Examples` tab and look for your project, and on examples, select `Nicla_vision_fusion`:\n\n![](images/jpg/inference.jpg)\n\nNote that the code created by Edge Impulse considers a *sensor fusion* approach where the IMU (Accelerometer and Gyroscope) and the ToF are used. At the beginning of the code, you have the libraries related to our project, IMU and ToF:\n\n``` cpp\n/* Includes ---------------------------------------------------------------- */\n#include <NICLA_Vision_Movement_Classification_inferencing.h> \n#include <Arduino_LSM6DSOX.h> //IMU\n#include \"VL53L1X.h\" // ToF\n```\n\n> You can keep the code this way for testing because the trained model will use only features pre-processed from the accelerometer. But consider that you will write your code only with the needed libraries for a real project.\n\nAnd that is it!\n\nYou can now upload the code to your device and proceed with the inferences. Press the Nicla `[RESET]` button twice to put it on boot mode (disconnect from the Studio if it is still connected), and upload the sketch to your board.\n\nNow you should try different movements with your board (similar to those done during data capture), observing the inference result of each class on the Serial Monitor:\n\n- **Idle and lift classes:**\n\n![](images/jpg/inference_1.jpg)\n\n- **Maritime and terrestrial:**\n\n![](images/jpg/inference_2.jpg)\n\nNote that in all situations above, the value of the `anomaly score` was smaller than 0.0. Try a new movement that was not part of the original dataset, for example, \"rolling\" the Nicla, facing the camera upside-down, as a container falling from a boat or even a boat accident:\n\n- **Anomaly detection:**\n\n![](images/jpg/anomaly-boat.jpg)\n\nIn this case, the anomaly is much bigger, over 1.00\n\n### Post-processing\n\nNow that we know the model is working since it detects the movements, we suggest that you modify the code to see the result with the NiclaV completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\n\nThe idea is to do the same as with the KWS project: if one specific movement is detected, a specific LED could be lit. For example, if *terrestrial* is detected, the Green LED will light; if *maritime*, the Red LED will light, if it is a *lift,* the Blue LED will light; and if no movement is detected *(idle*), the LEDs will be OFF. You can also add a condition when an anomaly is detected, in this case, for example, a white color can be used (all e LEDs light simultaneously).\n\n## Conclusion\n\n> The notebooks and codeused in this hands-on tutorial will be found on the [GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Motion_Classification) repository.\n\nBefore we finish, consider that Movement Classification and Object Detection can be utilized in many applications across various domains. Here are some of the potential applications:\n\n### Case Applications\n\n#### Industrial and Manufacturing\n\n- **Predictive Maintenance:** Detecting anomalies in machinery motion to predict failures before they occur.\n- **Quality Control:** Monitoring the motion of assembly lines or robotic arms for precision assessment and deviation detection from the standard motion pattern.\n- **Warehouse Logistics:** Managing and tracking the movement of goods with automated systems that classify different types of motion and detect anomalies in handling.\n\n#### Healthcare\n\n- **Patient Monitoring:** Detecting falls or abnormal movements in the elderly or those with mobility issues.\n- **Rehabilitation:** Monitoring the progress of patients recovering from injuries by classifying motion patterns during physical therapy sessions.\n- **Activity Recognition:** Classifying types of physical activity for fitness applications or patient monitoring.\n\n#### Consumer Electronics\n\n- **Gesture Control:** Interpreting specific motions to control devices, such as turning on lights with a hand wave.\n- **Gaming:** Enhancing gaming experiences with motion-controlled inputs.\n\n#### Transportation and Logistics\n\n- **Vehicle Telematics:** Monitoring vehicle motion for unusual behavior such as hard braking, sharp turns, or accidents.\n- **Cargo Monitoring:** Ensuring the integrity of goods during transport by detecting unusual movements that could indicate tampering or mishandling.\n\n#### Smart Cities and Infrastructure\n\n- **Structural Health Monitoring:** Detecting vibrations or movements within structures that could indicate potential failures or maintenance needs.\n- **Traffic Management:** Analyzing the flow of pedestrians or vehicles to improve urban mobility and safety.\n\n#### Security and Surveillance\n\n- **Intruder Detection:** Detecting motion patterns typical of unauthorized access or other security breaches.\n- **Wildlife Monitoring:** Detecting poachers or abnormal animal movements in protected areas.\n\n#### Agriculture\n\n- **Equipment Monitoring:** Tracking the performance and usage of agricultural machinery.\n- **Animal Behavior Analysis:** Monitoring livestock movements to detect behaviors indicating health issues or stress.\n\n#### Environmental Monitoring\n\n- **Seismic Activity:** Detecting irregular motion patterns that precede earthquakes or other geologically relevant events.\n- **Oceanography:** Studying wave patterns or marine movements for research and safety purposes.\n\n### Nicla 3D case\n\nFor real applications, as some described before, we can add a case to our device, and Eoin Jordan, from Edge Impulse, developed a great wearable and machine health case for the Nicla range of boards. It works with a 10mm magnet, 2M screws, and a 16mm strap for human and machine health use case scenarios. Here is the link: [Arduino Nicla Voice and Vision Wearable Case](https://www.thingiverse.com/thing:5923305).\n\n![](images/jpg/case.jpg)\n\nThe applications for motion classification and anomaly detection are extensive, and the Arduino Nicla Vision is well-suited for scenarios where low power consumption and edge processing are advantageous. Its small form factor and efficiency in processing make it an ideal choice for deploying portable and remote applications where real-time processing is crucial and connectivity may be limited.\n\n## Resources\n\n* [Arduino Code](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Motion_Classification/Niclav_Acc_Data_Capture)\n\n* [Edge Impulse Spectral Features Block Colab Notebook](https://colab.research.google.com/github/Mjrovai/Arduino_Nicla_Vision/blob/main/Motion_Classification/Edge_Impulse_Spectral_Features_Block.ipynb)\n\n* [Edge Impulse Project](https://studio.edgeimpulse.com/public/302078/latest)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":8,"fig-height":6,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../../../custom_callout.lua"],"reference-location":"margin","highlight-style":"github","toc":true,"toc-depth":4,"include-in-header":{"text":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN\"></script>\n<script type=\"module\"  src=\"/scripts/ai_menu/dist/bundle.js\" defer></script>\n"},"citeproc":true,"output-file":"motion_classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib","motion_classification.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"lightbox":true,"theme":{"light":["default","../../../../../style.scss","../../../../../style-light.scss"],"dark":["darkly","../../../../../style.scss","../../../../../style-dark.scss"]},"code-block-bg":true,"code-block-border-left":"#A51C30","table":{"classes":["table-striped","table-hover"]},"citation-location":"margin","sidenote":true,"linkcolor":"#A51C30","urlcolor":"#A51C30","anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"number-depth":3},"extensions":{"book":{"multiFile":true}}},"titlepage-pdf":{"identifier":{"display-name":"PDF","target-format":"titlepage-pdf","base-format":"pdf","extension-name":"titlepage"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":["../../../../../_extensions/nmfs-opensci/titlepage/fonts/qualitype/opentype/QTDublinIrish.otf"],"shortcodes":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\titlepage-theme.lua","C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\coverpage-theme.lua","../../../../../custom_callout.lua"],"toc":true,"top-level-division":"chapter","number-sections":true,"toc-depth":3,"cite-method":"citeproc","reference-location":"margin","include-in-header":[{"file":"../../../../../tex/header-includes.tex"}],"output-file":"motion_classification.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"template-partials":["../../../../../_extensions/nmfs-opensci/titlepage/_coverpage.tex","../../../../../_extensions/nmfs-opensci/titlepage/_author-affiliation-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_header-footer-date-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_title-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_titlepage.tex","../../../../../_extensions/nmfs-opensci/titlepage/before-body.tex","../../../../../_extensions/nmfs-opensci/titlepage/pandoc.tex"],"revealjs-plugins":[],"bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib","motion_classification.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"documentclass":"scrbook","classoption":["abstract","titlepage"],"coverpage":true,"coverpage-title":"Machine Learning Systems","coverpage-bg-image":"../../../../../cover-image-transparent.png","coverpage-author":["Vijay","Janapa Reddi"],"coverpage-theme":{"page-text-align":"center","bg-image-left":"0.225\\paperwidth","bg-image-bottom":7,"bg-image-rotate":0,"bg-image-opacity":1,"author-style":"plain","author-sep":"newline","author-fontsize":20,"author-align":"right","author-bottom":"0.15\\paperwidth","author-left":"7in","author-width":"6in","footer-style":"none","header-style":"none","date-style":"none","title-fontsize":57,"title-left":"0.075\\paperwidth","title-bottom":"0.375\\paperwidth","title-width":"0.9\\paperwidth"},"titlepage":true,"titlepage-theme":{"elements":["\\titleblock","Prof. Vijay Janapa Reddi","School of Engineering and Applied Sciences","Harvard University","\\vfill","With heartfelt gratitude to the community for their invaluable contributions and steadfast support.","\\vfill"],"page-align":"left","title-style":"plain","title-fontstyle":["huge","bfseries"],"title-space-after":"4\\baselineskip","title-subtitle-space-between":"0.05\\textheight","subtitle-fontstyle":["large","textit"],"author-style":"superscript-with-and","author-fontstyle":"large","affiliation-style":"numbered-list-with-correspondence","affiliation-fontstyle":"large","affiliation-space-after":"0pt","footer-style":"plain","footer-fontstyle":"large","logo-size":"0.15\\textheight","logo-space-after":"1\\baselineskip","vrule-width":"2pt","vrule-align":"left","vrule-color":"black"},"lof":false,"lot":false,"latex-engine":"xelatex","citation-package":"natbib","link-citations":true,"biblio-title":"References","title-block-style":"none","indent":"0px","fontsize":"10pt","citation-location":"block","fig-caption":true,"cap-location":"margin","fig-cap-location":"margin","tbl-cap-location":"margin","hyperrefoptions":["linktoc=all","pdfwindowui","pdfpagemode=FullScreen","pdfpagelayout=TwoPageRight"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","titlepage-pdf"]}