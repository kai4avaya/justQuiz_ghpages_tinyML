{"title":"Image Classification","markdown":{"yaml":{"bibliography":"image_classification.bib"},"headingText":"Image Classification","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n![*DALL·E 3 Prompt: Cartoon in a 1950s style featuring a compact electronic device with a camera module placed on a wooden table. The screen displays blue robots on one side and green periquitos on the other. LED lights on the device indicate classifications, while characters in retro clothing observe with interest.*](images/jpg/img_class_ini.jpg)\n\n## Overview\n\nAs we initiate our studies into embedded machine learning or TinyML, it's impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\n\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, **Edge AI** has a very high impact potential, and **it is for now**!\n\n![](images/jpg/image2.jpg)\n\nIn the \"bullseye\" of the Radar is the *Edge Computer Vision*, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is **Image Classification**, a kind of ML \"Hello World\"!\n\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow's robust ecosystem, we'll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\n\nWe'll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you'll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.\n\n## Computer Vision\n\nAt its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system's ability to understand, interpret, and react to visual stimuli.\n\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are *Image Classification* and *Object Detection*.\n\n![](images/jpg/image15.jpg)\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.\n\n## Image Classification Project Goal\n\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a *robot* and a small Brazilian parrot (named *Periquito*). Also, we will collect images of a *background* where those two objects are absent.\n\n![](images/jpg/image36.jpg)\n\n## Data Collection\n\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\n### Collecting Dataset with OpenMV IDE\n\nFirst, create in your computer a folder where your data will be saved, for example, \"data.\" Next, on the OpenMV IDE, go to `Tools > Dataset Editor` and select `New Dataset` to start the dataset collection:\n\n![](images/png/image29.png)\n\nThe IDE will ask you to open the file where your data will be saved and choose the \"data\" folder that was created. Note that new icons will appear on the Left panel.\n\n![](images/png/image46.png)\n\nUsing the upper icon (1), enter with the first class name, for example, \"periquito\":\n\n![](images/png/image22.png)\n\nRunning the `dataset_capture_script.py` and clicking on the camera icon (2), will start capturing images:\n\n![](images/png/image43.png)\n\nRepeat the same procedure with the other classes\n\n![](images/jpg/image6.jpg)\n\n> We suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the `Tools > Dataset Editor`.\n\nOn your computer, you will end with a dataset that contains three classes: *periquito,* *robot*, and *background*.\n\n![](images/png/image20.png)\n\nYou should return to *Edge Impulse Studio* and upload the dataset to your project.\n\n## Training the model with Edge Impulse Studio\n\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n![](images/png/image45.png)\n\n> Here, you can clone a similar project: [NICLA-Vision_Image_Classification](https://studio.edgeimpulse.com/public/273858/latest).\n\n## Dataset\n\nUsing the EI Studio (or *Studio*), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\n![](images/jpg/image41.jpg)\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into *Training*, *Validation*, and *Test*. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\n![](images/jpg/image7.jpg)\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\n![](images/png/image39.png)\n\nLeave to the Studio the splitting of the original dataset into *train and test* and choose the label about that specific data:\n\n![](images/png/image30.png)\n\nRepeat the procedure for all three classes. At the end, you should see your \"raw data\" in the Studio:\n\n![](images/png/image11.png)\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK.\n\n![](images/png/image44.png)\n\n## The Impulse Design\n\nIn this phase, we should define how to:\n\n- Pre-process our data, which consists of resizing the individual images and determining the `color depth` to use (be it RGB or Grayscale) and\n\n- Specify a Model, in this case, it will be the `Transfer Learning (Images)` to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n![](images/jpg/image23.jpg)\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\n![](images/jpg/image9.jpg)\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\n![](images/jpg/image32.jpg)\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\n\nGo to the Impulse Design Tab and create the *impulse*, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n![](images/png/image16.png)\n\n### Image Pre-Processing\n\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\n![](images/png/image17.png)\n\nPress \\[Save parameters\\] and Generate all features:\n\n![](images/png/image5.png)\n\n### Model Design\n\nIn 2007, Google introduced [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html), a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381).\n\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\n\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is that of thinning a network uniformly at each layer.\n\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different **α** values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\n![](images/jpg/image27.jpg)\n\nWe will use **MobileNetV2 96x96 0.1** for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:\n\n![](images/png/image24.png)\n\n## Model Training\n\nAnother valuable technique to be used with Deep Learning is **Data Augmentation**. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\n\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n\n``` python\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\n```\n\nExposure to these variations during training can help prevent your model from taking shortcuts by \"memorizing\" superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\n\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\n![](images/jpg/image31.jpg)\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference.\n\n## Model Testing\n\n![](images/jpg/image10.jpg)\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\n![](images/png/image34.png)\n\nThe result is, again, excellent.\n\n![](images/png/image12.png)\n\n## Deploying the model\n\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n![](images/jpg/image28.jpg)\n\n### Arduino Library\n\nFirst, Let's deploy it as an Arduino Library:\n\n![](images/png/image48.png)\n\nYou should install the library as.zip on the Arduino IDE and run the sketch *nicla_vision_camera.ino* available in Examples under your library name.\n\n> Note that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (`malloc_addblock((void*)0x30000000, 288 * 1024);`).\n\nThe result is good, with 86ms of measured latency.\n\n![](images/jpg/image25.jpg)\n\nHere is a short video showing the inference results: {{< video https://youtu.be/bZPZZJblU-o >}}\n\n### OpenMV\n\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\n\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\n![](images/png/image26.png)\n\nRunning this model as a *.tflite* directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let's explore this option.\n\nSelect `OpenMV Firmware` on the `Deploy Tab` and press `[Build]`.\n\n![](images/png/image3.png)\n\nOn your computer, you will find a ZIP file. Open it:\n\n![](images/png/image33.png)\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\n![](images/jpg/image35.jpg)\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\n![](images/png/image8.png)\n\nAfter the download is finished, press OK:\n\n![](images/png/image40.png)\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select \\[NO\\].\n\n![](images/png/image42.png)\n\nNow, open the script **ei_image_classification.py** that was downloaded from the Studio and the.bin file for the Nicla.\n\n![](images/png/image14.png)\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n![](images/png/image37.png)\n\n#### Changing the Code to add labels\n\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\n\n[Upload the code from GitHub,](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/nicla_image_classification.py) or modify it as below:\n\n``` python\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val > max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val < 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==> latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\n```\n\nHere you can see the result:\n\n![](images/jpg/image47.jpg)\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\n![](images/jpg/image13.jpg)\n\nThe latency will drop to only 71 ms.\n\n![](images/jpg/image1.jpg)\n\n> The NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n#### Post-Processing with LEDs\n\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\n![](images/jpg/image38.jpg)\n\nTo accomplish that, we should [upload the code from GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/nicla_image_classification_LED.py) or change the last code to include the LEDs:\n\n``` python\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val > max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val < 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==> latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\n```\n\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\n- Led Red 0n: Uncertain (no class is over 0.8)\n\n- Led Green 0n: Periquito \\> 0.8\n\n- Led Blue 0n: Robot \\> 0.8\n\n- All LEDs Off: Background \\> 0.8\n\nHere is the result:\n\n![](images/jpg/image18.jpg)\n\nIn more detail\n\n![](images/jpg/image21.jpg)\n\n## Image Classification (non-official) Benchmark\n\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\n![](images/jpg/image19.jpg)\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino's Library:\n\n![](images/jpg/image4.jpg)\n\n## Conclusion\n\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\n- **Autonomous Vehicles:** Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\n\n- **Healthcare:** Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\n\n- **Retail:** Automated checkout systems that identify products as they pass through a scanner.\n\n- **Security and Surveillance:** Facial recognition, anomaly detection, and object tracking in real-time video feeds.\n\n- **Augmented Reality:** Object detection and classification to overlay digital information in the real world.\n\n- **Industrial Automation:** Visual inspection of products, predictive maintenance, and robot and drone guidance.\n\n- **Agriculture:** Drone-based crop monitoring and automated harvesting.\n\n- **Natural Language Processing:** Image captioning and visual question answering.\n\n- **Gesture Recognition:** For gaming, sign language translation, and human-machine interaction.\n\n- **Content Recommendation:** Image-based recommendation systems in e-commerce.\n\n## Resources\n\n- [Micropython codes](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython)\n\n- [Dataset](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/data)\n\n- [Edge Impulse Project](https://studio.edgeimpulse.com/public/273858/latest)\n","srcMarkdownNoYaml":"\n\n# Image Classification {.unnumbered}\n\n![*DALL·E 3 Prompt: Cartoon in a 1950s style featuring a compact electronic device with a camera module placed on a wooden table. The screen displays blue robots on one side and green periquitos on the other. LED lights on the device indicate classifications, while characters in retro clothing observe with interest.*](images/jpg/img_class_ini.jpg)\n\n## Overview\n\nAs we initiate our studies into embedded machine learning or TinyML, it's impossible to overlook the transformative impact of Computer Vision (CV) and Artificial Intelligence (AI) in our lives. These two intertwined disciplines redefine what machines can perceive and accomplish, from autonomous vehicles and robotics to healthcare and surveillance.\n\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, **Edge AI** has a very high impact potential, and **it is for now**!\n\n![](images/jpg/image2.jpg)\n\nIn the \"bullseye\" of the Radar is the *Edge Computer Vision*, and when we talk about Machine Learning (ML) applied to vision, the first thing that comes to mind is **Image Classification**, a kind of ML \"Hello World\"!\n\nThis exercise will explore a computer vision project utilizing Convolutional Neural Networks (CNNs) for real-time image classification. Leveraging TensorFlow's robust ecosystem, we'll implement a pre-trained MobileNet model and adapt it for edge deployment. The focus will be on optimizing the model to run efficiently on resource-constrained hardware without sacrificing accuracy.\n\nWe'll employ techniques like quantization and pruning to reduce the computational load. By the end of this tutorial, you'll have a working prototype capable of classifying images in real-time, all running on a low-power embedded system based on the Arduino Nicla Vision board.\n\n## Computer Vision\n\nAt its core, computer vision enables machines to interpret and make decisions based on visual data from the world, essentially mimicking the capability of the human optical system. Conversely, AI is a broader field encompassing machine learning, natural language processing, and robotics, among other technologies. When you bring AI algorithms into computer vision projects, you supercharge the system's ability to understand, interpret, and react to visual stimuli.\n\nWhen discussing Computer Vision projects applied to embedded devices, the most common applications that come to mind are *Image Classification* and *Object Detection*.\n\n![](images/jpg/image15.jpg)\n\nBoth models can be implemented on tiny devices like the Arduino Nicla Vision and used on real projects. In this chapter, we will cover Image Classification.\n\n## Image Classification Project Goal\n\nThe first step in any ML project is to define the goal. In this case, it is to detect and classify two specific objects present in one image. For this project, we will use two small toys: a *robot* and a small Brazilian parrot (named *Periquito*). Also, we will collect images of a *background* where those two objects are absent.\n\n![](images/jpg/image36.jpg)\n\n## Data Collection\n\nOnce you have defined your Machine Learning project goal, the next and most crucial step is the dataset collection. You can use the Edge Impulse Studio, the OpenMV IDE we installed, or even your phone for the image capture. Here, we will use the OpenMV IDE for that.\n\n### Collecting Dataset with OpenMV IDE\n\nFirst, create in your computer a folder where your data will be saved, for example, \"data.\" Next, on the OpenMV IDE, go to `Tools > Dataset Editor` and select `New Dataset` to start the dataset collection:\n\n![](images/png/image29.png)\n\nThe IDE will ask you to open the file where your data will be saved and choose the \"data\" folder that was created. Note that new icons will appear on the Left panel.\n\n![](images/png/image46.png)\n\nUsing the upper icon (1), enter with the first class name, for example, \"periquito\":\n\n![](images/png/image22.png)\n\nRunning the `dataset_capture_script.py` and clicking on the camera icon (2), will start capturing images:\n\n![](images/png/image43.png)\n\nRepeat the same procedure with the other classes\n\n![](images/jpg/image6.jpg)\n\n> We suggest around 60 images from each category. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and the RGB565 (color pixel format).\n\nAfter capturing your dataset, close the Dataset Editor Tool on the `Tools > Dataset Editor`.\n\nOn your computer, you will end with a dataset that contains three classes: *periquito,* *robot*, and *background*.\n\n![](images/png/image20.png)\n\nYou should return to *Edge Impulse Studio* and upload the dataset to your project.\n\n## Training the model with Edge Impulse Studio\n\nWe will use the Edge Impulse Studio for training our model. Enter your account credentials and create a new project:\n\n![](images/png/image45.png)\n\n> Here, you can clone a similar project: [NICLA-Vision_Image_Classification](https://studio.edgeimpulse.com/public/273858/latest).\n\n## Dataset\n\nUsing the EI Studio (or *Studio*), we will go over four main steps to have our model ready for use on the Nicla Vision board: Dataset, Impulse, Tests, and Deploy (on the Edge Device, in this case, the NiclaV).\n\n![](images/jpg/image41.jpg)\n\nRegarding the Dataset, it is essential to point out that our Original Dataset, captured with the OpenMV IDE, will be split into *Training*, *Validation*, and *Test*. The Test Set will be divided from the beginning, and a part will reserved to be used only in the Test phase after training. The Validation Set will be used during training.\n\n![](images/jpg/image7.jpg)\n\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload the chosen categories files from your computer:\n\n![](images/png/image39.png)\n\nLeave to the Studio the splitting of the original dataset into *train and test* and choose the label about that specific data:\n\n![](images/png/image30.png)\n\nRepeat the procedure for all three classes. At the end, you should see your \"raw data\" in the Studio:\n\n![](images/png/image11.png)\n\nThe Studio allows you to explore your data, showing a complete view of all the data in your project. You can clear, inspect, or change labels by clicking on individual data items. In our case, a very simple project, the data seems OK.\n\n![](images/png/image44.png)\n\n## The Impulse Design\n\nIn this phase, we should define how to:\n\n- Pre-process our data, which consists of resizing the individual images and determining the `color depth` to use (be it RGB or Grayscale) and\n\n- Specify a Model, in this case, it will be the `Transfer Learning (Images)` to fine-tune a pre-trained MobileNet V2 image classification model on our data. This method performs well even with relatively small image datasets (around 150 images in our case).\n\n![](images/jpg/image23.jpg)\n\nTransfer Learning with MobileNet offers a streamlined approach to model training, which is especially beneficial for resource-constrained environments and projects with limited labeled data. MobileNet, known for its lightweight architecture, is a pre-trained model that has already learned valuable features from a large dataset (ImageNet).\n\n![](images/jpg/image9.jpg)\n\nBy leveraging these learned features, you can train a new model for your specific task with fewer data and computational resources and yet achieve competitive accuracy.\n\n![](images/jpg/image32.jpg)\n\nThis approach significantly reduces training time and computational cost, making it ideal for quick prototyping and deployment on embedded devices where efficiency is paramount.\n\nGo to the Impulse Design Tab and create the *impulse*, defining an image size of 96x96 and squashing them (squared form, without cropping). Select Image and Transfer Learning blocks. Save the Impulse.\n\n![](images/png/image16.png)\n\n### Image Pre-Processing\n\nAll the input QVGA/RGB565 images will be converted to 27,640 features (96x96x3).\n\n![](images/png/image17.png)\n\nPress \\[Save parameters\\] and Generate all features:\n\n![](images/png/image5.png)\n\n### Model Design\n\nIn 2007, Google introduced [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html), a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases. in 2018, Google launched [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381).\n\nMobileNet V1 and MobileNet V2 aim at mobile efficiency and embedded vision applications but differ in architectural complexity and performance. While both use depthwise separable convolutions to reduce the computational cost, MobileNet V2 introduces Inverted Residual Blocks and Linear Bottlenecks to improve performance. These new features allow V2 to capture more complex features using fewer parameters, making it computationally more efficient and generally more accurate than its predecessor. Additionally, V2 employs a non-linear activation in the intermediate expansion layer. It still uses a linear activation for the bottleneck layer, a design choice found to preserve important information through the network. MobileNet V2 offers an optimized architecture for higher accuracy and efficiency and will be used in this project.\n\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be even smaller and faster. MobileNets introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is that of thinning a network uniformly at each layer.\n\nEdge Impulse Studio can use both MobileNetV1 (96x96 images) and V2 (96x96 or 160x160 images), with several different **α** values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3MB RAM and 2.6MB ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at the other extreme with MobileNetV1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\n![](images/jpg/image27.jpg)\n\nWe will use **MobileNetV2 96x96 0.1** for this project, with an estimated memory cost of 265.3 KB in RAM. This model should be OK for the Nicla Vision with 1MB of SRAM. On the Transfer Learning Tab, select this model:\n\n![](images/png/image24.png)\n\n## Model Training\n\nAnother valuable technique to be used with Deep Learning is **Data Augmentation**. Data augmentation is a method to improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\n\nLooking under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n\n``` python\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\n```\n\nExposure to these variations during training can help prevent your model from taking shortcuts by \"memorizing\" superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\n\nThe final layer of our model will have 12 neurons with a 15% dropout for overfitting prevention. Here is the Training result:\n\n![](images/jpg/image31.jpg)\n\nThe result is excellent, with 77ms of latency, which should result in 13fps (frames per second) during inference.\n\n## Model Testing\n\n![](images/jpg/image10.jpg)\n\nNow, you should take the data set aside at the start of the project and run the trained model using it as input:\n\n![](images/png/image34.png)\n\nThe result is, again, excellent.\n\n![](images/png/image12.png)\n\n## Deploying the model\n\nAt this point, we can deploy the trained model as.tflite and use the OpenMV IDE to run it using MicroPython, or we can deploy it as a C/C++ or an Arduino library.\n\n![](images/jpg/image28.jpg)\n\n### Arduino Library\n\nFirst, Let's deploy it as an Arduino Library:\n\n![](images/png/image48.png)\n\nYou should install the library as.zip on the Arduino IDE and run the sketch *nicla_vision_camera.ino* available in Examples under your library name.\n\n> Note that Arduino Nicla Vision has, by default, 512KB of RAM allocated for the M7 core and an additional 244KB on the M4 address space. In the code, this allocation was changed to 288 kB to guarantee that the model will run on the device (`malloc_addblock((void*)0x30000000, 288 * 1024);`).\n\nThe result is good, with 86ms of measured latency.\n\n![](images/jpg/image25.jpg)\n\nHere is a short video showing the inference results: {{< video https://youtu.be/bZPZZJblU-o >}}\n\n### OpenMV\n\nIt is possible to deploy the trained model to be used with OpenMV in two ways: as a library and as a firmware.\n\nThree files are generated as a library: the trained.tflite model, a list with labels, and a simple MicroPython script that can make inferences using the model.\n\n![](images/png/image26.png)\n\nRunning this model as a *.tflite* directly in the Nicla was impossible. So, we can sacrifice the accuracy using a smaller model or deploy the model as an OpenMV Firmware (FW). Choosing FW, the Edge Impulse Studio generates optimized models, libraries, and frameworks needed to make the inference. Let's explore this option.\n\nSelect `OpenMV Firmware` on the `Deploy Tab` and press `[Build]`.\n\n![](images/png/image3.png)\n\nOn your computer, you will find a ZIP file. Open it:\n\n![](images/png/image33.png)\n\nUse the Bootloader tool on the OpenMV IDE to load the FW on your board:\n\n![](images/jpg/image35.jpg)\n\nSelect the appropriate file (.bin for Nicla-Vision):\n\n![](images/png/image8.png)\n\nAfter the download is finished, press OK:\n\n![](images/png/image40.png)\n\nIf a message says that the FW is outdated, DO NOT UPGRADE. Select \\[NO\\].\n\n![](images/png/image42.png)\n\nNow, open the script **ei_image_classification.py** that was downloaded from the Studio and the.bin file for the Nicla.\n\n![](images/png/image14.png)\n\nRun it. Pointing the camera to the objects we want to classify, the inference result will be displayed on the Serial Terminal.\n\n![](images/png/image37.png)\n\n#### Changing the Code to add labels\n\nThe code provided by Edge Impulse can be modified so that we can see, for test reasons, the inference result directly on the image displayed on the OpenMV IDE.\n\n[Upload the code from GitHub,](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/nicla_image_classification.py) or modify it as below:\n\n``` python\n# Marcelo Rovai - NICLA Vision - Image Classification\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pxl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\nwhile(True):\n    clock.tick()  # Starts tracking elapsed time.\n\n    img = sensor.snapshot()\n\n    # default settings just do one detection\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val > max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label with the highest probability\n    if max_val < 0.5:\n        max_lbl = 'uncertain'\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==> latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\n```\n\nHere you can see the result:\n\n![](images/jpg/image47.jpg)\n\nNote that the latency (136 ms) is almost double of what we got directly with the Arduino IDE. This is because we are using the IDE as an interface and also the time to wait for the camera to be ready. If we start the clock just before the inference:\n\n![](images/jpg/image13.jpg)\n\nThe latency will drop to only 71 ms.\n\n![](images/jpg/image1.jpg)\n\n> The NiclaV runs about half as fast when connected to the IDE. The FPS should increase once disconnected.\n\n#### Post-Processing with LEDs\n\nWhen working with embedded machine learning, we are looking for devices that can continually proceed with the inference and result, taking some action directly on the physical world and not displaying the result on a connected computer. To simulate this, we will light up a different LED for each possible inference result.\n\n![](images/jpg/image38.jpg)\n\nTo accomplish that, we should [upload the code from GitHub](https://github.com/Mjrovai/Arduino_Nicla_Vision/blob/main/Micropython/nicla_image_classification_LED.py) or change the last code to include the LEDs:\n\n``` python\n# Marcelo Rovai - NICLA Vision - Image Classification with LEDs\n# Adapted from Edge Impulse - OpenMV Image Classification Example\n# @24Aug23\n\nimport sensor, image, time, os, tf, uos, gc, pyb\n\nledRed = pyb.LED(1)\nledGre = pyb.LED(2)\nledBlu = pyb.LED(3)\n\nsensor.reset()                         # Reset and initialize the sensor.\nsensor.set_pixformat(sensor.RGB565)    # Set pixl fmt to RGB565 (or GRAYSCALE)\nsensor.set_framesize(sensor.QVGA)      # Set frame size to QVGA (320x240)\nsensor.set_windowing((240, 240))       # Set 240x240 window.\nsensor.skip_frames(time=2000)          # Let the camera adjust.\n\nnet = None\nlabels = None\n\nledRed.off()\nledGre.off()\nledBlu.off()\n\ntry:\n    # Load built in model\n    labels, net = tf.load_builtin_model('trained')\nexcept Exception as e:\n    raise Exception(e)\n\nclock = time.clock()\n\n\ndef setLEDs(max_lbl):\n\n    if max_lbl == 'uncertain':\n        ledRed.on()\n        ledGre.off()\n        ledBlu.off()\n\n    if max_lbl == 'periquito':\n        ledRed.off()\n        ledGre.on()\n        ledBlu.off()\n\n    if max_lbl == 'robot':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.on()\n\n    if max_lbl == 'background':\n        ledRed.off()\n        ledGre.off()\n        ledBlu.off()\n\n\nwhile(True):\n    img = sensor.snapshot()\n    clock.tick()  # Starts tracking elapsed time.\n\n    # default settings just do one detection.\n    for obj in net.classify(img, \n                            min_scale=1.0, \n                            scale_mul=0.8, \n                            x_overlap=0.5, \n                            y_overlap=0.5):\n        fps = clock.fps()\n        lat = clock.avg()\n\n        print(\"**********\\nPrediction:\")\n        img.draw_rectangle(obj.rect())\n        # This combines the labels and confidence values into a list of tuples\n        predictions_list = list(zip(labels, obj.output()))\n\n        max_val = predictions_list[0][1]\n        max_lbl = 'background'\n        for i in range(len(predictions_list)):\n            val = predictions_list[i][1]\n            lbl = predictions_list[i][0]\n\n            if val > max_val:\n                max_val = val\n                max_lbl = lbl\n\n    # Print label and turn on LED with the highest probability\n    if max_val < 0.8:\n        max_lbl = 'uncertain'\n\n    setLEDs(max_lbl)\n\n    print(\"{} with a prob of {:.2f}\".format(max_lbl, max_val))\n    print(\"FPS: {:.2f} fps ==> latency: {:.0f} ms\".format(fps, lat))\n\n    # Draw label with highest probability to image viewer\n    img.draw_string(\n        10, 10,\n        max_lbl + \"\\n{:.2f}\".format(max_val),\n        mono_space = False,\n        scale=2\n        )\n```\n\nNow, each time that a class scores a result greater than 0.8, the correspondent LED will be lit:\n\n- Led Red 0n: Uncertain (no class is over 0.8)\n\n- Led Green 0n: Periquito \\> 0.8\n\n- Led Blue 0n: Robot \\> 0.8\n\n- All LEDs Off: Background \\> 0.8\n\nHere is the result:\n\n![](images/jpg/image18.jpg)\n\nIn more detail\n\n![](images/jpg/image21.jpg)\n\n## Image Classification (non-official) Benchmark\n\nSeveral development boards can be used for embedded machine learning (TinyML), and the most common ones for Computer Vision applications (consuming low energy), are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, the Arduino Nicla Vison, and the Arduino Portenta.\n\n![](images/jpg/image19.jpg)\n\nCatching the opportunity, the same trained model was deployed on the ESP-CAM, the XIAO, and the Portenta (in this one, the model was trained again, using grayscaled images to be compatible with its camera). Here is the result, deploying the models as Arduino's Library:\n\n![](images/jpg/image4.jpg)\n\n## Conclusion\n\nBefore we finish, consider that Computer Vision is more than just image classification. For example, you can develop Edge Machine Learning projects around vision in several areas, such as:\n\n- **Autonomous Vehicles:** Use sensor fusion, lidar data, and computer vision algorithms to navigate and make decisions.\n\n- **Healthcare:** Automated diagnosis of diseases through MRI, X-ray, and CT scan image analysis\n\n- **Retail:** Automated checkout systems that identify products as they pass through a scanner.\n\n- **Security and Surveillance:** Facial recognition, anomaly detection, and object tracking in real-time video feeds.\n\n- **Augmented Reality:** Object detection and classification to overlay digital information in the real world.\n\n- **Industrial Automation:** Visual inspection of products, predictive maintenance, and robot and drone guidance.\n\n- **Agriculture:** Drone-based crop monitoring and automated harvesting.\n\n- **Natural Language Processing:** Image captioning and visual question answering.\n\n- **Gesture Recognition:** For gaming, sign language translation, and human-machine interaction.\n\n- **Content Recommendation:** Image-based recommendation systems in e-commerce.\n\n## Resources\n\n- [Micropython codes](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/Micropython)\n\n- [Dataset](https://github.com/Mjrovai/Arduino_Nicla_Vision/tree/main/data)\n\n- [Edge Impulse Project](https://studio.edgeimpulse.com/public/273858/latest)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":8,"fig-height":6,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../../../../custom_callout.lua"],"reference-location":"margin","highlight-style":"github","toc":true,"toc-depth":4,"include-in-header":{"text":"<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN\"></script>\n<script type=\"module\"  src=\"/scripts/ai_menu/dist/bundle.js\" defer></script>\n"},"citeproc":true,"output-file":"image_classification.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author, Editor & Curator","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Last Updated","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.33","bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib","image_classification.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"lightbox":true,"theme":{"light":["default","../../../../../style.scss","../../../../../style-light.scss"],"dark":["darkly","../../../../../style.scss","../../../../../style-dark.scss"]},"code-block-bg":true,"code-block-border-left":"#A51C30","table":{"classes":["table-striped","table-hover"]},"citation-location":"margin","sidenote":true,"linkcolor":"#A51C30","urlcolor":"#A51C30","anchor-sections":true,"smooth-scroll":false,"citations-hover":false,"footnotes-hover":false,"number-depth":3},"extensions":{"book":{"multiFile":true}}},"titlepage-pdf":{"identifier":{"display-name":"PDF","target-format":"titlepage-pdf","base-format":"pdf","extension-name":"titlepage"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":true,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":["../../../../../_extensions/nmfs-opensci/titlepage/fonts/qualitype/opentype/QTDublinIrish.otf"],"shortcodes":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","filters":["C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\titlepage-theme.lua","C:\\Users\\kkleinbard\\Documents\\dev\\kai_projects\\tinyml\\tinyML_repo\\dev_10_26\\cs249r_book\\_extensions\\nmfs-opensci\\titlepage\\coverpage-theme.lua","../../../../../custom_callout.lua"],"toc":true,"top-level-division":"chapter","number-sections":true,"toc-depth":3,"cite-method":"citeproc","reference-location":"margin","include-in-header":[{"file":"../../../../../tex/header-includes.tex"}],"output-file":"image_classification.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"template-partials":["../../../../../_extensions/nmfs-opensci/titlepage/_coverpage.tex","../../../../../_extensions/nmfs-opensci/titlepage/_author-affiliation-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_header-footer-date-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_title-themes.tex","../../../../../_extensions/nmfs-opensci/titlepage/_titlepage.tex","../../../../../_extensions/nmfs-opensci/titlepage/before-body.tex","../../../../../_extensions/nmfs-opensci/titlepage/pandoc.tex"],"revealjs-plugins":[],"bibliography":["../../../../../contents/core/introduction/introduction.bib","../../../../../contents/core/ai_for_good/ai_for_good.bib","../../../../../contents/core/benchmarking/benchmarking.bib","../../../../../contents/core/data_engineering/data_engineering.bib","../../../../../contents/core/dl_primer/dl_primer.bib","../../../../../contents/core/efficient_ai/efficient_ai.bib","../../../../../contents/core/ml_systems/ml_systems.bib","../../../../../contents/core/frameworks/frameworks.bib","../../../../../contents/core/generative_ai/generative_ai.bib","../../../../../contents/core/hw_acceleration/hw_acceleration.bib","../../../../../contents/core/ondevice_learning/ondevice_learning.bib","../../../../../contents/core/ops/ops.bib","../../../../../contents/core/optimizations/optimizations.bib","../../../../../contents/core/privacy_security/privacy_security.bib","../../../../../contents/core/responsible_ai/responsible_ai.bib","../../../../../contents/core/robust_ai/robust_ai.bib","../../../../../contents/core/sustainable_ai/sustainable_ai.bib","../../../../../contents/core/training/training.bib","../../../../../contents/core/workflow/workflow.bib","../../../../../contents/core/conclusion/conclusion.bib","image_classification.bib"],"comments":{"giscus":{"repo":"harvard-edge/cs249r_book"}},"crossref":{"appendix-title":"Appendix","appendix-delim":":","custom":[{"kind":"float","reference-prefix":"Lab","key":"labq","latex-env":"lab"},{"kind":"float","reference-prefix":"Exercise","key":"exr","latex-env":"exr"},{"kind":"float","reference-prefix":"Video","key":"vid","latex-env":"vid"}]},"citation":true,"license":"CC-BY-NC-SA","editor":{"render-on-save":true},"resources":["../../../../../CNAME"],"_quarto-vars":{"email":{"contact":"vj@eecs.harvard.edu","subject":["MLSys Book"],"info":"mailto:vj@eecs.harvard.edu?subject=\"CS249r%20MLSys%20with%20TinyML%20Book%20-%20\""},"title":{"long":"Machine Learning Systems","short":"Machine Learning Systems"}},"documentclass":"scrbook","classoption":["abstract","titlepage"],"coverpage":true,"coverpage-title":"Machine Learning Systems","coverpage-bg-image":"../../../../../cover-image-transparent.png","coverpage-author":["Vijay","Janapa Reddi"],"coverpage-theme":{"page-text-align":"center","bg-image-left":"0.225\\paperwidth","bg-image-bottom":7,"bg-image-rotate":0,"bg-image-opacity":1,"author-style":"plain","author-sep":"newline","author-fontsize":20,"author-align":"right","author-bottom":"0.15\\paperwidth","author-left":"7in","author-width":"6in","footer-style":"none","header-style":"none","date-style":"none","title-fontsize":57,"title-left":"0.075\\paperwidth","title-bottom":"0.375\\paperwidth","title-width":"0.9\\paperwidth"},"titlepage":true,"titlepage-theme":{"elements":["\\titleblock","Prof. Vijay Janapa Reddi","School of Engineering and Applied Sciences","Harvard University","\\vfill","With heartfelt gratitude to the community for their invaluable contributions and steadfast support.","\\vfill"],"page-align":"left","title-style":"plain","title-fontstyle":["huge","bfseries"],"title-space-after":"4\\baselineskip","title-subtitle-space-between":"0.05\\textheight","subtitle-fontstyle":["large","textit"],"author-style":"superscript-with-and","author-fontstyle":"large","affiliation-style":"numbered-list-with-correspondence","affiliation-fontstyle":"large","affiliation-space-after":"0pt","footer-style":"plain","footer-fontstyle":"large","logo-size":"0.15\\textheight","logo-space-after":"1\\baselineskip","vrule-width":"2pt","vrule-align":"left","vrule-color":"black"},"lof":false,"lot":false,"latex-engine":"xelatex","citation-package":"natbib","link-citations":true,"biblio-title":"References","title-block-style":"none","indent":"0px","fontsize":"10pt","citation-location":"block","fig-caption":true,"cap-location":"margin","fig-cap-location":"margin","tbl-cap-location":"margin","hyperrefoptions":["linktoc=all","pdfwindowui","pdfpagemode=FullScreen","pdfpagelayout=TwoPageRight"]},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","titlepage-pdf"]}